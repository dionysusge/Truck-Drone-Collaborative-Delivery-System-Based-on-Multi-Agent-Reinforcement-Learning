# truck_routing.py
import time
import config
from tqdm import tqdm
from config import Config
from generate_model_dataset import (  # ä»æ–‡æ¡£1å¯¼å…¥å¿…è¦çš„å‡½æ•°
    calculate_cp, calculate_di, calculate_sd, calculate_ci,
    calculate_ldp, calculate_ic, calculate_dle, calculate_dii,
    calculate_cci, calculate_cli
)
from typing import List, Tuple, Dict, Any, Optional
from torch.distributions import Categorical, Bernoulli
import csv
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import math
from sklearn.linear_model import LinearRegression
import random
from reward_function import RewardFunction, AdaptiveRewardScheduler
from state_representation import StateRepresentation, TimeWindowConstraints
from action_mask import ActionMaskManager
from demand_model import UncertaintyHandler
from soft_time_window import SoftTimeWindowManager, TimeWindow, TimeWindowOptimizer, PenaltyFunction
from truck_replenishment import (ReplenishmentOptimizer, ReplenishmentStrategy, TruckState, 
                                LockerDemand, ReplenishmentDecision)
from dynamic_drone_scheduler import DynamicDroneScheduler
from dynamic_step_implementation import dynamic_step, get_serviceable_lockers
from dataclasses import dataclass
from enum import Enum

num_lockers = config.num_lockers


# ======================
# è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
# ======================
class DifficultyLevel(Enum):
    """éš¾åº¦ç­‰çº§"""
    BEGINNER = 0
    EASY = 1
    MEDIUM = 2
    HARD = 3
    EXPERT = 4


@dataclass
class CurriculumStage:
    """è¯¾ç¨‹å­¦ä¹ é˜¶æ®µé…ç½®"""
    name: str
    difficulty: DifficultyLevel
    num_lockers: int
    num_trucks: int
    demand_variance: float
    time_pressure: float
    episodes_required: int
    success_threshold: float


class CurriculumManager:
    """
    è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
    å®ç°ä»ç®€å•åˆ°å¤æ‚çš„æ¸è¿›å¼è®­ç»ƒï¼Œç¨³å®šå­¦ä¹ æ›²çº¿ï¼Œæé«˜æ”¶æ•›é€Ÿåº¦å’Œé²æ£’æ€§
    """
    
    def __init__(self, max_lockers: int = 15, max_trucks: int = 4, start_difficulty: str = "expert"):
        """
        åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
        
        Args:
            max_lockers: æœ€å¤§å¿«é€’æŸœæ•°é‡
            max_trucks: æœ€å¤§å¡è½¦æ•°é‡
            start_difficulty: èµ·å§‹éš¾åº¦çº§åˆ« (beginner, easy, medium, hard, expert)
        """
        self.max_lockers = max_lockers
        self.max_trucks = max_trucks
        
        # å®šä¹‰è¯¾ç¨‹é˜¶æ®µ - æ€»è®¡çº¦10ä¸‡è½®è®­ç»ƒï¼Œä¸“å®¶é˜¶æ®µå ä¸»è¦æ¯”é‡
        self.stages = [
            CurriculumStage("åˆå­¦è€…", DifficultyLevel.BEGINNER, 3, 1, 0.1, 0.5, 2000, 0.6),    # 2Kè½®ï¼šåŸºç¡€å…¥é—¨
            CurriculumStage("ç®€å•", DifficultyLevel.EASY, 6, 2, 0.2, 0.6, 5000, 0.65),         # 5Kè½®ï¼šåŒè½¦åè°ƒ
            CurriculumStage("ä¸­ç­‰", DifficultyLevel.MEDIUM, 9, 3, 0.3, 0.7, 8000, 0.7),        # 8Kè½®ï¼šä¸‰è½¦åè°ƒ
            CurriculumStage("å›°éš¾", DifficultyLevel.HARD, 12, 3, 0.4, 0.8, 15000, 0.75),       # 15Kè½®ï¼šå¤æ‚åœºæ™¯
            CurriculumStage("ä¸“å®¶", DifficultyLevel.EXPERT, max_lockers, max_trucks, 0.5, 1.0, 70000, 0.8)  # 70Kè½®ï¼šä¸“å®¶çº§è®­ç»ƒ
        ]
        
        # æ ¹æ®èµ·å§‹éš¾åº¦è®¾ç½®å½“å‰é˜¶æ®µ
        difficulty_map = {
            "beginner": 0,
            "easy": 1, 
            "medium": 2,
            "hard": 3,
            "expert": 4
        }
        self.current_stage_index = difficulty_map.get(start_difficulty.lower(), 4)  # é»˜è®¤ä¸“å®¶çº§
        self.current_stage = self.stages[self.current_stage_index]
        self.episodes_in_stage = 0
        self.performance_history = []
        self.performance_window = 50
        
    def get_current_config(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰é˜¶æ®µé…ç½®
        
        Returns:
            ç¯å¢ƒé…ç½®å­—å…¸
        """
        stage = self.current_stage
        return {
            'num_lockers': stage.num_lockers,
            'num_trucks': stage.num_trucks,
            'demand_variance': stage.demand_variance,
            'time_pressure': stage.time_pressure,
            'difficulty_level': stage.difficulty.value,
            'stage_name': stage.name
        }
    
    def update_performance(self, episode_reward: float, episode_success: bool):
        """
        æ›´æ–°æ€§èƒ½è®°å½•
        
        Args:
            episode_reward: å›åˆå¥–åŠ±
            episode_success: å›åˆæ˜¯å¦æˆåŠŸ
        """
        # æ”¹è¿›çš„æ€§èƒ½è®¡ç®—ï¼šåŸºäºå½“å‰é˜¶æ®µçš„æœŸæœ›å¥–åŠ±èŒƒå›´
        stage_difficulty = self.current_stage.difficulty.value
        
        # æ ¹æ®é˜¶æ®µè°ƒæ•´æœŸæœ›å¥–åŠ±èŒƒå›´ - ä¿®æ­£ä¸ºæ›´ç¬¦åˆå®é™…æƒ…å†µçš„èŒƒå›´
        if stage_difficulty <= 1:  # åˆå­¦è€…å’Œç®€å•é˜¶æ®µ
            expected_min, expected_max = -10, 50
        elif stage_difficulty == 2:  # ä¸­ç­‰é˜¶æ®µ
            expected_min, expected_max = -5, 80
        elif stage_difficulty == 3:  # å›°éš¾é˜¶æ®µ
            expected_min, expected_max = 0, 120
        else:  # ä¸“å®¶é˜¶æ®µ
            expected_min, expected_max = 5, 150
        
        # è®¡ç®—åŸºç¡€æ€§èƒ½åˆ†æ•° (0-1)
        performance_score = max(0, min(1, (episode_reward - expected_min) / (expected_max - expected_min)))
        
        # æˆåŠŸå¥–åŠ±ï¼šæ ¹æ®é˜¶æ®µéš¾åº¦è°ƒæ•´
        if episode_success:
            success_bonus = 0.4 - stage_difficulty * 0.05  # éš¾åº¦è¶Šé«˜ï¼ŒæˆåŠŸå¥–åŠ±è¶Šå°
            performance_score = min(1.0, performance_score + success_bonus)
        
        self.performance_history.append(performance_score)
        self.episodes_in_stage += 1
        
        # æ£€æŸ¥é˜¶æ®µè½¬æ¢
        self._check_stage_transition()
    
    def _check_stage_transition(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é˜¶æ®µè½¬æ¢"""
        # æé«˜æ£€æŸ¥è¦æ±‚ï¼Œç¡®ä¿å……åˆ†è®­ç»ƒ
        min_episodes_for_check = max(100, self.performance_window)  # æœ€å°‘100ä¸ªepisodesè¿›è¡Œè¯„ä¼°
        
        if len(self.performance_history) < min_episodes_for_check:
            return
        
        recent_performance = np.mean(self.performance_history[-min_episodes_for_check:])
        
        # æ›´ä¸¥æ ¼çš„å‰è¿›æ¡ä»¶ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µå……åˆ†è®­ç»ƒ
        min_episodes_required = max(self.current_stage.episodes_required // 2, 500)  # è‡³å°‘500ä¸ªepisodes
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥å‰è¿›
        if (recent_performance >= self.current_stage.success_threshold and 
            self.episodes_in_stage >= min_episodes_required and
            self.current_stage_index < len(self.stages) - 1):
            
            print(f"\nğŸ“ è¯¾ç¨‹å­¦ä¹ ï¼šä» '{self.current_stage.name}' å‰è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ")
            print(f"å½“å‰æ€§èƒ½: {recent_performance:.3f}, ç›®æ ‡: {self.current_stage.success_threshold:.3f}")
            print(f"å·²å®Œæˆepisodes: {self.episodes_in_stage}, æœ€å°‘è¦æ±‚: {min_episodes_required}")
            
            self.current_stage_index += 1
            self.current_stage = self.stages[self.current_stage_index]
            self.episodes_in_stage = 0
            self.performance_history = []
            
            print(f"æ–°é˜¶æ®µ: '{self.current_stage.name}' (éš¾åº¦: {self.current_stage.difficulty.value})")
            return True
        
        # é™é»˜ç­‰å¾…ï¼Œä¸è¾“å‡ºå†—ä½™ä¿¡æ¯
        
        return False
    
    def get_adaptive_hyperparameters(self) -> Dict[str, float]:
        """
        è·å–è‡ªé€‚åº”è¶…å‚æ•°
        
        Returns:
            è¶…å‚æ•°å­—å…¸
        """
        difficulty = self.current_stage.difficulty.value
        
        # æ ¹æ®éš¾åº¦è°ƒæ•´å­¦ä¹ ç‡ - é™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
        learning_rates = [2e-4, 1.5e-4, 1e-4, 8e-5, 5e-5]  # å¤§å¹…é™ä½å­¦ä¹ ç‡
        exploration_rates = [0.25, 0.2, 0.15, 0.1, 0.08]   # é™ä½æ¢ç´¢ç‡
        batch_sizes = [64, 64, 128, 256, 256]               # å¢åŠ æ‰¹æ¬¡å¤§å°
        
        return {
            'learning_rate': learning_rates[difficulty],
            'exploration_rate': exploration_rates[difficulty],
            'batch_size': batch_sizes[difficulty],
            'entropy_coef': exploration_rates[difficulty] * 0.3,  # é™ä½ç†µç³»æ•°
            'value_loss_coef': 0.3,                              # é™ä½ä»·å€¼æŸå¤±æƒé‡
            'max_grad_norm': 0.3                                 # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        }
    
    def get_curriculum_progress(self) -> Dict[str, Any]:
        """
        è·å–è¯¾ç¨‹è¿›åº¦
        
        Returns:
            è¿›åº¦ä¿¡æ¯å­—å…¸
        """
        recent_performance = 0.0
        if len(self.performance_history) >= self.performance_window:
            recent_performance = np.mean(self.performance_history[-self.performance_window:])
        elif len(self.performance_history) > 0:
            recent_performance = np.mean(self.performance_history)
        
        return {
            'current_stage': self.current_stage.name,
            'stage_index': self.current_stage_index,
            'total_stages': len(self.stages),
            'episodes_in_stage': self.episodes_in_stage,
            'required_episodes': self.current_stage.episodes_required,
            'recent_performance': recent_performance,
            'target_performance': self.current_stage.success_threshold,
            'difficulty_level': self.current_stage.difficulty.value
        }
    
    def should_use_reward_shaping(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¥–åŠ±å¡‘å½¢
        
        Returns:
            æ˜¯å¦ä½¿ç”¨å¥–åŠ±å¡‘å½¢
        """
        return self.current_stage.difficulty.value <= 1  # å‰ä¸¤ä¸ªé˜¶æ®µä½¿ç”¨


# ======================
# æ— äººæœºæƒ©ç½šé¢„æµ‹æ¨¡å‹
# ======================
class DronePenaltyPredictor:
    def __init__(self):
        """
        æ— äººæœºè·¯å¾„è§„åˆ’æƒ©ç½šé¢„æµ‹æ¨¡å‹
        """
        # åŠ è½½é¢„è®­ç»ƒçš„çº¿æ€§å›å½’æ¨¡å‹
        self.model = LinearRegression()
        self._load_trained_model()

        # ç‰¹å¾åˆ—é¡ºåºï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        self.feature_order = [
            'CP', 'DI', 'SD', 'CI', 'LDP', 'IC',
            'DLE', 'DII', 'CCI', 'CLI'
        ]

    def _load_trained_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°"""
        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹å‚æ•°
        self.model.coef_ = np.array([
            0.12, -0.08, 0.05, 0.07, 0.15,
            -0.03, 0.09, -0.06, 0.04, 0.11
        ])
        self.model.intercept_ = 2.5

    def predict(self, truck_location, service_area):
        """
        é¢„æµ‹æ— äººæœºè·¯å¾„è§„åˆ’çš„æƒ©ç½š

        å‚æ•°:
        truck_location: å¡è½¦åœé ç‚¹ä½ç½® (x, y)
        service_area: æœåŠ¡åŒºåŸŸå†…çš„å¿«é€’æŸœä¿¡æ¯åˆ—è¡¨
                      [{'location': (x,y), 'delivery_demand': int, 'return_demand': int}, ...]

        è¿”å›:
        penalty: é¢„æµ‹çš„æ— äººæœºè·¯å¾„è§„åˆ’æƒ©ç½š
        """
        # è®¡ç®—ç‰¹å¾
        features = self._calculate_features(truck_location, service_area)

        # é¢„æµ‹æƒ©ç½š
        penalty = self.model.predict([features])[0]

        # ç¡®ä¿æƒ©ç½šå€¼ä¸ºæ­£
        return max(penalty, 0)

    def _calculate_features(self, truck_location, service_area):
        """
        è®¡ç®—æ— äººæœºè·¯å¾„è§„åˆ’ç‰¹å¾

        å‚æ•°:
        truck_location: å¡è½¦åœé ç‚¹ä½ç½® (x, y)
        service_area: æœåŠ¡åŒºåŸŸå†…çš„å¿«é€’æŸœåˆ—è¡¨

        è¿”å›:
        features: ç‰¹å¾å‘é‡
        """
        # å¦‚æœæ²¡æœ‰å¿«é€’æŸœï¼Œè¿”å›é›¶å‘é‡
        if not service_area:
            return np.zeros(len(self.feature_order))

        # å‡†å¤‡å¿«é€’æŸœæ•°æ®
        lockers = []
        for locker in service_area:
            x, y = locker['location']
            delivery = locker['delivery_demand']
            return_d = locker['return_demand']
            lockers.append((x, y, delivery, return_d))

        # è®¾ç½®ä¸­å¿ƒç‚¹
        center = truck_location

        # è®¡ç®—ç‰¹å¾
        features = {
            'CP': calculate_cp(lockers, center),
            'DI': calculate_di(lockers),
            'SD': calculate_sd(lockers),
            'CI': calculate_ci(lockers),
            'LDP': calculate_ldp(lockers, center),
            'IC': calculate_ic(lockers),
            'DLE': calculate_dle(lockers, center),
            'DII': calculate_dii(lockers),
            'CCI': calculate_cci(lockers, center),
            'CLI': calculate_cli(lockers, center)
        }

        # æŒ‰æŒ‡å®šé¡ºåºè¿”å›ç‰¹å¾å‘é‡
        return np.array([features[col] for col in self.feature_order])


class TruckSchedulingEnv:
    def __init__(self, verbose=False):
        """
        å¡è½¦è°ƒåº¦ç¯å¢ƒ - ä¼˜åŒ–ç‰ˆ
        """
        # é…ç½®å‚æ•°
        self.depot = Config.DEPOT  # ä»“åº“ä½ç½®
        self.drone_max_range = Config.DRONE_MAX_RANGE  # æ— äººæœºæœ€å¤§ç»­èˆªè·ç¦»
        self.truck_capacity = Config.TRUCK_CAPACITY  # å¡è½¦å®¹é‡
        self.penalty_weight = Config.PENALTY_WEIGHT  # æ— äººæœºæƒ©ç½šæƒé‡
        self.max_timesteps = Config.MAX_TIMESTEPS  # æœ€å¤§æ—¶é—´æ­¥
        self.num_lockers = config.num_lockers
        self.lockers_info = config.locker_info
        self.verbose = verbose

        # è®¡ç®—æ€»æœŸæœ›éœ€æ±‚
        self.total_lambda_del = sum(self.lockers_info[2])
        self.total_lambda_ret = sum(self.lockers_info[3])

        # è®¡ç®—åˆå§‹è£…è½½æ¯”ä¾‹å’Œå¡è½¦æ•°é‡
        self.initial_load_ratio = self.total_lambda_del / (self.total_lambda_del + self.total_lambda_ret)
        self.initial_delivery_load = int(self.initial_load_ratio * self.truck_capacity)
        self.num_trucks = max(1, math.ceil(self.total_lambda_del / self.initial_delivery_load))

        # åˆå§‹åŒ–æ— äººæœºé¢„æµ‹æ¨¡å‹
        self.drone_predictor = DronePenaltyPredictor()

        # åˆå§‹åŒ–æ–°çš„å¥–åŠ±å‡½æ•°
        self.reward_function = RewardFunction(max_timesteps=self.max_timesteps)
        self.reward_scheduler = AdaptiveRewardScheduler(self.reward_function)

        # åˆå§‹åŒ–å¢å¼ºçŠ¶æ€è¡¨ç¤º
        self.state_representation = StateRepresentation(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity,
            depot_location=self.depot,
            max_timesteps=self.max_timesteps
        )

        # åˆå§‹åŒ–æ—¶é—´çª—çº¦æŸ
        self.time_window_constraints = TimeWindowConstraints(
            num_lockers=self.num_lockers,
            soft_penalty_factor=0.1
        )
        
        # åˆå§‹åŒ–åŠ¨ä½œæ©ç ç®¡ç†å™¨
        self.action_mask_manager = ActionMaskManager(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity,
            depot_location=self.depot,
            max_distance=100.0
        )
        
        # åˆå§‹åŒ–ä¸ç¡®å®šæ€§å¤„ç†å™¨
        self.uncertainty_handler = UncertaintyHandler(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity
        )
        
        # åˆå§‹åŒ–è½¯æ—¶é—´çª—ç®¡ç†å™¨
        self.soft_time_window_manager = SoftTimeWindowManager(
            penalty_function=PenaltyFunction.QUADRATIC,
            early_penalty_weight=0.5,
            late_penalty_weight=2.0,
            max_penalty=50.0
        )
        
        # åˆå§‹åŒ–æ—¶é—´çª—ä¼˜åŒ–å™¨
        self.time_window_optimizer = TimeWindowOptimizer(self.soft_time_window_manager)
        
        # ä¸ºæ¯ä¸ªå¿«é€’æŸœè®¾ç½®æ—¶é—´çª—
        self._initialize_time_windows()
        
        # åˆå§‹åŒ–è¡¥è´§ä¼˜åŒ–å™¨
        self.replenishment_optimizer = ReplenishmentOptimizer(
            truck_capacity=self.truck_capacity,
            depot_location=self.depot,
            strategy=ReplenishmentStrategy.ADAPTIVE
        )
        
        # åˆå§‹åŒ–åŠ¨æ€æ— äººæœºè°ƒåº¦å™¨
        self.drone_scheduler = DynamicDroneScheduler(
            max_service_time=300,  # 300ç§’æ—¶é—´çª—
            drone_speed=Config.DRONE_SPEED,
            service_time_per_item=Config.DRONE_SERVICE_TIME
        )
        
        # åˆå§‹åŒ–å¢å¼ºçš„å¤šå¡è½¦åè°ƒç»„ä»¶
        self.coordination_history = []  # åè°ƒå†å²è®°å½•
        self.truck_performance_metrics = {}  # å¡è½¦æ€§èƒ½æŒ‡æ ‡
        self.global_coordination_info = {}  # å…¨å±€åè°ƒä¿¡æ¯
        self.load_balancing_weights = np.ones(self.num_trucks)  # è´Ÿè½½å‡è¡¡æƒé‡
        self.coordination_update_interval = 5  # åè°ƒæ›´æ–°é—´éš”
        self.last_coordination_update = 0  # ä¸Šæ¬¡åè°ƒæ›´æ–°æ—¶é—´
        
        # åˆå§‹åŒ–å¡è½¦æ€§èƒ½è·Ÿè¸ª
        for truck_id in range(self.num_trucks):
            self.truck_performance_metrics[truck_id] = {
                'total_distance': 0.0,
                'total_service_time': 0.0,
                'items_delivered': 0,
                'items_returned': 0,
                'efficiency_score': 0.0,
                'load_utilization': 0.0,
                'recent_actions': [],
                'predicted_completion_time': 0.0
            }

        # åˆå§‹åŒ–ç»´åº¦ï¼ˆä½¿ç”¨å¢å¼ºçŠ¶æ€è¡¨ç¤ºï¼‰
        self.state_dim = self.state_representation.get_state_dimension()

        if self.verbose:
            self.print_initial_info()

        self.reset()
    
    def update_curriculum_config(self, curriculum_config: Dict[str, Any]):
        """
        æ›´æ–°è¯¾ç¨‹å­¦ä¹ é…ç½®
        
        Args:
            curriculum_config: è¯¾ç¨‹é…ç½®å­—å…¸ï¼ŒåŒ…å«num_trucks, num_lockers, boundaryç­‰
        """
        config_changed = False
        
        # æ›´æ–°è¾¹ç•Œå‚æ•°
        if 'boundary' in curriculum_config:
            new_boundary = curriculum_config['boundary']
            import config
            if new_boundary != config.boundary:
                config.boundary = new_boundary
                config_changed = True
                # é‡æ–°ç”Ÿæˆå¿«é€’æŸœä¿¡æ¯ä»¥åº”ç”¨æ–°è¾¹ç•Œ
                config.generate_locker_info()
                self.lockers_info = config.locker_info
                
                # é‡æ–°è®¡ç®—æ€»éœ€æ±‚
                self.total_lambda_del = sum(self.lockers_info[2])
                self.total_lambda_ret = sum(self.lockers_info[3])
                
                # é‡æ–°è®¡ç®—åˆå§‹è£…è½½æ¯”ä¾‹
                self.initial_load_ratio = self.total_lambda_del / (self.total_lambda_del + self.total_lambda_ret)
                self.initial_delivery_load = int(self.initial_load_ratio * self.truck_capacity)
        
        # æ›´æ–°å¿«é€’æŸœæ•°é‡
        if 'num_lockers' in curriculum_config:
            new_num_lockers = curriculum_config['num_lockers']
            if new_num_lockers != self.num_lockers:
                self.num_lockers = new_num_lockers
                config_changed = True
                # é‡æ–°ç”Ÿæˆå¿«é€’æŸœä¿¡æ¯
                import config
                config.num_lockers = new_num_lockers
                config.generate_locker_info()
                self.lockers_info = config.locker_info
                
                # é‡æ–°è®¡ç®—æ€»éœ€æ±‚
                self.total_lambda_del = sum(self.lockers_info[2])
                self.total_lambda_ret = sum(self.lockers_info[3])
                
                # é‡æ–°è®¡ç®—åˆå§‹è£…è½½æ¯”ä¾‹
                self.initial_load_ratio = self.total_lambda_del / (self.total_lambda_del + self.total_lambda_ret)
                self.initial_delivery_load = int(self.initial_load_ratio * self.truck_capacity)
        
        # æ›´æ–°å¡è½¦æ•°é‡
        if 'num_trucks' in curriculum_config:
            new_num_trucks = curriculum_config['num_trucks']
            if new_num_trucks is None:
                # åŠ¨æ€è®¡ç®—å¡è½¦æ•°é‡
                calculated_trucks = max(1, math.ceil(self.total_lambda_del / self.initial_delivery_load))
                if calculated_trucks != self.num_trucks:
                    self.num_trucks = calculated_trucks
                    config_changed = True
            elif new_num_trucks != self.num_trucks:
                self.num_trucks = new_num_trucks
                config_changed = True
        
        # å¦‚æœé…ç½®å‘ç”Ÿå˜åŒ–ï¼Œé‡æ–°åˆå§‹åŒ–ç›¸å…³ç»„ä»¶
        if config_changed:
            self._reinitialize_components()
            
            if self.verbose:
                print(f"ğŸš› è¯¾ç¨‹å­¦ä¹ é…ç½®æ›´æ–°:")
                print(f"   å¿«é€’æŸœæ•°é‡: {self.num_lockers}")
                print(f"   å¡è½¦æ•°é‡: {self.num_trucks}")
                if 'boundary' in curriculum_config:
                    print(f"   è¾¹ç•ŒèŒƒå›´: Â±{curriculum_config['boundary']}")
                print(f"   çŠ¶æ€ç»´åº¦: {self.state_dim}")
                print(f"   æ€»é…é€éœ€æ±‚: {self.total_lambda_del:.2f}")
                print(f"   æ€»è¿”å›éœ€æ±‚: {self.total_lambda_ret:.2f}")
                print(f"   åˆå§‹è£…è½½é‡: {self.initial_delivery_load}")
    
    def _reinitialize_components(self):
        """
        é‡æ–°åˆå§‹åŒ–ä¾èµ–äºå¡è½¦æ•°é‡çš„ç»„ä»¶
        """
        # é‡æ–°åˆå§‹åŒ–çŠ¶æ€è¡¨ç¤º
        self.state_representation = StateRepresentation(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity,
            depot_location=self.depot,
            max_timesteps=self.max_timesteps
        )
        
        # é‡æ–°åˆå§‹åŒ–åŠ¨ä½œæ©ç ç®¡ç†å™¨
        self.action_mask_manager = ActionMaskManager(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity,
            depot_location=self.depot,
            max_distance=100.0
        )
        
        # é‡æ–°åˆå§‹åŒ–ä¸ç¡®å®šæ€§å¤„ç†å™¨
        self.uncertainty_handler = UncertaintyHandler(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity
        )
        
        # é‡æ–°åˆå§‹åŒ–è´Ÿè½½å‡è¡¡æƒé‡
        self.load_balancing_weights = np.ones(self.num_trucks)
        
        # é‡æ–°åˆå§‹åŒ–å¡è½¦æ€§èƒ½è·Ÿè¸ª
        self.truck_performance_metrics = {}
        for truck_id in range(self.num_trucks):
            self.truck_performance_metrics[truck_id] = {
                'total_distance': 0.0,
                'total_service_time': 0.0,
                'items_delivered': 0,
                'items_returned': 0,
                'efficiency_score': 0.0,
                'load_utilization': 0.0,
                'recent_actions': [],
                'predicted_completion_time': 0.0
            }
        
        # æ›´æ–°çŠ¶æ€ç»´åº¦
        self.state_dim = self.state_representation.get_state_dimension()

    def _euclidean_distance(self, p1, p2):
        """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»"""
        return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)
    
    def _initialize_time_windows(self):
        """
        åˆå§‹åŒ–æ¯ä¸ªå¿«é€’æŸœçš„æ—¶é—´çª—çº¦æŸ
        æ ¹æ®å¿«é€’æŸœçš„ä½ç½®ã€éœ€æ±‚é‡ç­‰å› ç´ è®¾ç½®åˆç†çš„æ—¶é—´çª—
        """
        for locker_id in range(self.num_lockers):
            # è·å–å¿«é€’æŸœä¿¡æ¯
            locker_x, locker_y = self.lockers_info[0][locker_id], self.lockers_info[1][locker_id]
            lambda_del = self.lockers_info[2][locker_id]  # é…é€éœ€æ±‚ç‡
            lambda_ret = self.lockers_info[3][locker_id]  # å–ä»¶éœ€æ±‚ç‡
            
            # è®¡ç®—è·ç¦»ä»“åº“çš„è·ç¦»
            distance_to_depot = self._euclidean_distance((locker_x, locker_y), self.depot)
            
            # æ ¹æ®è·ç¦»å’Œéœ€æ±‚é‡è®¡ç®—åŸºç¡€æ—¶é—´çª—
            # è·ç¦»è¶Šè¿œï¼Œæ—¶é—´çª—è¶Šå®½æ¾ï¼›éœ€æ±‚é‡è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜
            base_travel_time = distance_to_depot / Config.TRUCK_SPEED  # ä½¿ç”¨é…ç½®çš„å¡è½¦é€Ÿåº¦
            demand_priority = (lambda_del + lambda_ret) / (self.total_lambda_del + self.total_lambda_ret)
            
            # è®¾ç½®æ—¶é—´çª—å‚æ•°
            if demand_priority > 0.15:  # é«˜éœ€æ±‚å¿«é€’æŸœ
                early_start = max(0, base_travel_time - 2)
                preferred_start = base_travel_time
                preferred_end = base_travel_time + 3
                late_end = base_travel_time + 8
                priority = 1.5
            elif demand_priority > 0.08:  # ä¸­ç­‰éœ€æ±‚å¿«é€’æŸœ
                early_start = max(0, base_travel_time - 3)
                preferred_start = base_travel_time
                preferred_end = base_travel_time + 5
                late_end = base_travel_time + 12
                priority = 1.0
            else:  # ä½éœ€æ±‚å¿«é€’æŸœ
                early_start = max(0, base_travel_time - 5)
                preferred_start = base_travel_time
                preferred_end = base_travel_time + 8
                late_end = base_travel_time + 20
                priority = 0.7
            
            # åˆ›å»ºæ—¶é—´çª—å¯¹è±¡
            time_window = TimeWindow(
                early_start=early_start,
                preferred_start=preferred_start,
                preferred_end=preferred_end,
                late_end=late_end,
                service_time=1.0,  # å‡è®¾æœåŠ¡æ—¶é—´ä¸º1ä¸ªæ—¶é—´å•ä½
                priority=priority
            )
            
            # è®¾ç½®åˆ°è½¯æ—¶é—´çª—ç®¡ç†å™¨
            self.soft_time_window_manager.set_time_window(locker_id, time_window)



    def print_initial_info(self):
        """æ‰“å°åˆå§‹é…ç½®ä¿¡æ¯"""
        print("\n" + "=" * 50)
        print("åˆå§‹å¡è½¦é…ç½®ä¿¡æ¯:")
        print(f"å¿«é€’æŸœæ•°é‡: {self.num_lockers}")
        print(f"æ€»æœŸæœ›å–è´§éœ€æ±‚: {self.total_lambda_del:.2f}")
        print(f"æ€»æœŸæœ›é€€è´§éœ€æ±‚: {self.total_lambda_ret:.2f}")
        print(f"åˆå§‹è£…è½½æ¯”ä¾‹: {self.initial_load_ratio * 100:.2f}%")
        print(f"å¡è½¦å®¹é‡: {self.truck_capacity}")
        print(f"å¡è½¦æ•°é‡: {self.num_trucks}")
        print(f"æ¯è¾†å¡è½¦åˆå§‹å–è´§è´§ç‰©: {self.initial_delivery_load}")
        print("=" * 50)

    def reset(self):
        """é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€"""
        # åˆå§‹åŒ–æ—¶é—´æ­¥
        self.current_timestep = 0
        
        # æ·»åŠ é‡ç½®è®¡æ•°å™¨ï¼Œç”¨äºæ§åˆ¶ç¯å¢ƒå˜åŒ–é¢‘ç‡
        if not hasattr(self, 'reset_count'):
            self.reset_count = 0
        self.reset_count += 1
        
        # æ¯ä¸ªepisodeéƒ½é‡æ–°ç”Ÿæˆéšæœºéœ€æ±‚ï¼Œç¡®ä¿è®­ç»ƒå¤šæ ·æ€§
        import config
        if self.reset_count == 1:
            # ç¬¬ä¸€æ¬¡é‡ç½®ï¼šç”Ÿæˆå¿«é€’æŸœä½ç½®å’Œéœ€æ±‚
            config.generate_locker_info()
            self.lockers_info = config.locker_info
        else:
            # åç»­é‡ç½®ï¼šä¿æŒå¿«é€’æŸœä½ç½®ä¸å˜ï¼Œä½†æ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆéšæœºéœ€æ±‚
            config.generate_demand_only()
            # æ›´æ–°éœ€æ±‚ä¿¡æ¯ä½†ä¿æŒä½ç½®ä¸å˜
            self.lockers_info[2] = list(config.lambda_del.values())  # lambda_del
            self.lockers_info[3] = list(config.lambda_ret.values())  # lambda_ret
            self.lockers_info[4] = list(config.demand_del.values())  # demand_del
            self.lockers_info[5] = list(config.demand_ret.values())  # demand_ret
        
        # é‡æ–°è®¡ç®—æ€»éœ€æ±‚
        self.total_lambda_del = sum(self.lockers_info[2])
        self.total_lambda_ret = sum(self.lockers_info[3])
        
        # é‡æ–°è®¡ç®—åˆå§‹è£…è½½æ¯”ä¾‹ï¼Œæ·»åŠ éšæœºæ€§
        base_load_ratio = self.total_lambda_del / (self.total_lambda_del + self.total_lambda_ret)
        # åœ¨åŸºç¡€æ¯”ä¾‹ä¸Šæ·»åŠ Â±10%çš„éšæœºå˜åŒ–
        random_factor = random.uniform(0.9, 1.1)
        self.initial_load_ratio = min(0.9, max(0.1, base_load_ratio * random_factor))
        self.initial_delivery_load = int(self.initial_load_ratio * self.truck_capacity)
        
        # é‡æ–°åˆå§‹åŒ–ä¸ç¡®å®šæ€§å¤„ç†å™¨ï¼Œç¡®ä¿éœ€æ±‚ç”Ÿæˆçš„å¤šæ ·æ€§
        self.uncertainty_handler = UncertaintyHandler(
            num_trucks=self.num_trucks,
            num_lockers=self.num_lockers,
            truck_capacity=self.truck_capacity
        )
        
        # åˆå§‹åŒ–æ‰€æœ‰å¿«é€’æŸœä¸ºæœªæœåŠ¡çŠ¶æ€
        self.lockers_state = []
        for i in range(self.num_lockers):
            locker_id = i + 1
            
            # ç›´æ¥ä½¿ç”¨configä¸­é‡æ–°ç”Ÿæˆçš„éœ€æ±‚å€¼ï¼Œç¡®ä¿æ¯æ¬¡é‡ç½®éƒ½æœ‰æ–°çš„éœ€æ±‚åˆ†å¸ƒ
            actual_demand_del = self.lockers_info[4][i]  # ç›´æ¥ä½¿ç”¨configä¸­çš„demand_del
            actual_demand_ret = self.lockers_info[5][i]  # ç›´æ¥ä½¿ç”¨configä¸­çš„demand_ret
            
            # ä½¿ç”¨UncertaintyHandlerç”Ÿæˆä¸ç¡®å®šæ€§ä¿¡æ¯ï¼ˆä½†ä¸è¦†ç›–å®é™…éœ€æ±‚ï¼‰
            delivery_estimate = self.uncertainty_handler.get_robust_demand_estimate(
                locker_id, 'delivery', self.current_timestep
            )
            return_estimate = self.uncertainty_handler.get_robust_demand_estimate(
                locker_id, 'return', self.current_timestep
            )
            
            locker = {
                'id': locker_id,  # å¿«é€’æŸœIDä»1å¼€å§‹
                'location': (self.lockers_info[0][i], self.lockers_info[1][i]),  # ä½ç½®
                'lambda_del': self.lockers_info[2][i],  # é€è´§éœ€æ±‚ç‡ï¼ˆåŸºç¡€ï¼‰
                'lambda_ret': self.lockers_info[3][i],  # é€€è´§éœ€æ±‚ç‡ï¼ˆåŸºç¡€ï¼‰
                'demand_del': actual_demand_del,  # ä½¿ç”¨configä¸­é‡æ–°ç”Ÿæˆçš„å®é™…é€è´§éœ€æ±‚
                'demand_ret': actual_demand_ret,   # ä½¿ç”¨configä¸­é‡æ–°ç”Ÿæˆçš„å®é™…é€€è´§éœ€æ±‚
                'expected_del': delivery_estimate['expected'],  # æœŸæœ›é€è´§éœ€æ±‚
                'expected_ret': return_estimate['expected'],   # æœŸæœ›é€€è´§éœ€æ±‚
                'uncertainty_del': delivery_estimate['uncertainty'],  # é€è´§éœ€æ±‚ä¸ç¡®å®šæ€§
                'uncertainty_ret': return_estimate['uncertainty'],   # é€€è´§éœ€æ±‚ä¸ç¡®å®šæ€§
                'served': False  # åˆå§‹åŒ–ä¸ºæœªæœåŠ¡
            }
            self.lockers_state.append(locker)

        # åˆå§‹åŒ–å¡è½¦çŠ¶æ€
        self.trucks = []
        for i in range(self.num_trucks):
            self.trucks.append({
                'id': i,
                'current_location': 0,  # 0è¡¨ç¤ºä»“åº“ï¼Œå…¶ä»–ä¸ºå¿«é€’æŸœID
                'position': (0, 0),     # å½“å‰ä½ç½®åæ ‡
                'current_delivery_load': self.initial_delivery_load,
                'current_return_load': 0,
                'remaining_space': self.truck_capacity - self.initial_delivery_load,
                'capacity': self.truck_capacity,  # æ·»åŠ å®¹é‡å­—æ®µ
                'visited_stops': [],
                'total_distance': 0.0,
                'returned': False,  # æ·»åŠ è¿”å›çŠ¶æ€æ ‡å¿—
                'service_start_time': None,  # æœåŠ¡å¼€å§‹æ—¶é—´
                'service_end_time': None,    # æœåŠ¡ç»“æŸæ—¶é—´
                'is_servicing': False,       # æ˜¯å¦æ­£åœ¨æœåŠ¡
                'time_at_location': 0,       # åœ¨å½“å‰ä½ç½®åœç•™çš„æ—¶é—´
                'last_position': (0, 0),     # ä¸Šä¸€ä¸ªä½ç½®ï¼Œç”¨äºæ£€æµ‹ä½ç½®å˜åŒ–
                'drone_deployments': []      # æ— äººæœºéƒ¨ç½²è®°å½•
            })

        # ç¯å¢ƒçŠ¶æ€
        self.time_step = 0
        self.total_truck_distance = 0.0
        self.total_drone_cost = 0.0
        self.served_delivery = 0
        self.served_return = 0
        
        # å›åˆçº§åˆ«ç»Ÿè®¡å˜é‡ï¼ˆç”¨äºç»ˆç«¯å¥–åŠ±è®¡ç®—ï¼‰
        self.episode_truck_distance = 0.0      # å›åˆæ€»å¡è½¦è¡Œé©¶è·ç¦»
        self.episode_drone_cost = 0.0          # å›åˆæ€»æ— äººæœºæˆæœ¬
        self.episode_drone_deliveries = 0      # å›åˆæ€»æ— äººæœºé…é€å®Œæˆæ¬¡æ•°
        self.episode_satisfied_lockers = 0     # å›åˆæ€»å®Œå…¨æ»¡è¶³éœ€æ±‚çš„å¿«é€’æŸœæ•°é‡

        # é‡ç½®çŠ¶æ€è¡¨ç¤ºå™¨
        self.state_representation.reset()

        return self._get_state_with_mask()

    def _get_action_mask(self):
        """åˆ›å»ºåŠ¨ä½œå±è”½å‘é‡"""
        # æ„å»ºç¯å¢ƒçŠ¶æ€
        env_state = {
            'trucks': self.trucks,
            'lockers': self.lockers_state,
            'time_step': self.time_step,
            'max_timesteps': self.max_timesteps
        }
        
        # ä½¿ç”¨åŠ¨ä½œæ©ç ç®¡ç†å™¨è·å–æ‰€æœ‰å¡è½¦çš„æ©ç 
        action_masks = self.action_mask_manager.get_action_masks(env_state)
        
        # ä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¡è½¦çš„æ©ç 
        # åœ¨å¤šæ™ºèƒ½ä½“è®­ç»ƒä¸­ï¼Œåº”è¯¥ä½¿ç”¨get_action_masksæ–¹æ³•
        if action_masks:
            return action_masks[0]
        else:
            # è¿”å›é»˜è®¤æ©ç 
            return {
                'stop_mask': torch.ones(self.num_lockers + 1),
                'service_mask': torch.ones(self.num_lockers)
            }
    
    def get_action_masks(self):
        """è·å–æ‰€æœ‰å¡è½¦çš„åŠ¨ä½œæ©ç """
        env_state = {
            'trucks': self.trucks,
            'lockers': self.lockers_state,
            'time_step': self.time_step,
            'max_timesteps': self.max_timesteps
        }
        
        return self.action_mask_manager.get_action_masks(env_state)

    def _get_current_state(self):
        """
        è·å–å½“å‰ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ï¼Œç”¨äºå¥–åŠ±è®¡ç®—
        """
        # è®¡ç®—è·¯å¾„æ•ˆç‡
        path_efficiency = self._calculate_path_efficiency()
        
        # è®¡ç®—å®Œæˆç‡
        completion_rate = self._calculate_completion_rate()
        
        # è®¡ç®—å®¹é‡åˆ©ç”¨ç‡
        capacity_utilization = self._calculate_capacity_utilization()
        
        return {
            'time_step': self.time_step,
            'max_timesteps': self.max_timesteps,
            'trucks': [truck.copy() for truck in self.trucks],
            'lockers': [locker.copy() for locker in self.lockers_state],
            'served_delivery': self.served_delivery,
            'served_return': self.served_return,
            'total_truck_distance': self.total_truck_distance,
            'total_drone_cost': self.total_drone_cost,
            'num_trucks': self.num_trucks,
            'num_lockers': self.num_lockers,
            'truck_capacity': self.truck_capacity,
            'initial_delivery_load': self.initial_delivery_load,
            'path_efficiency': path_efficiency,
            'completion_rate': completion_rate,
            'capacity_utilization': capacity_utilization
        }

    def _get_state_with_mask(self):
        """è·å–å½“å‰ç¯å¢ƒçŠ¶æ€å‘é‡å’ŒåŠ¨ä½œæ©ç """
        # ä½¿ç”¨æ–°çš„çŠ¶æ€è¡¨ç¤ºå™¨
        state_vector = self.state_representation.get_state_vector(
            trucks=self.trucks,
            lockers=self.lockers_state,
            time_step=self.time_step,
            total_distance=self.total_truck_distance,
            total_drone_cost=self.total_drone_cost
        )

        # è·å–åŠ¨ä½œæ©ç 
        action_mask = self._get_action_mask()

        return state_vector, action_mask

    def get_truck_specific_states(self):
        """
        ä¸ºæ¯ä¸ªå¡è½¦ç”Ÿæˆå¢å¼ºçš„ç‰¹å®šçŠ¶æ€è¡¨ç¤ºï¼ŒåŒ…å«å…¨å±€åè°ƒä¿¡æ¯å’ŒåŠ¨æ€è´Ÿè½½å‡è¡¡
        
        Returns:
            List[np.ndarray]: æ¯ä¸ªå¡è½¦çš„å¢å¼ºçŠ¶æ€å‘é‡åˆ—è¡¨
        """
        # æ›´æ–°å…¨å±€åè°ƒä¿¡æ¯
        self._update_global_coordination_info()
        
        # æ›´æ–°å¡è½¦æ€§èƒ½æŒ‡æ ‡
        self._update_truck_performance_metrics()
        
        # åŠ¨æ€è°ƒæ•´è´Ÿè½½å‡è¡¡æƒé‡
        self._update_load_balancing_weights()
        
        states = []
        
        # æ„å»ºå¢å¼ºçš„ç¯å¢ƒçŠ¶æ€å­—å…¸
        env_state = {
            'trucks': self.trucks,
            'lockers': self.lockers_state,
            'time_step': self.time_step,
            'max_timesteps': self.max_timesteps,
            'total_truck_distance': self.total_truck_distance,
            'total_drone_cost': self.total_drone_cost,
            'served_delivery': self.served_delivery,
            'served_return': self.served_return,
            'global_coordination_info': self.global_coordination_info,
            'truck_performance_metrics': self.truck_performance_metrics,
            'load_balancing_weights': self.load_balancing_weights
        }
        
        # ä¸ºæ¯ä¸ªå¡è½¦ç”Ÿæˆå¢å¼ºçš„ç‰¹å®šçŠ¶æ€
        for truck_id in range(self.num_trucks):
            # è·å–åŸºç¡€çŠ¶æ€
            base_state = self.state_representation.get_enhanced_state(env_state, truck_id)
            
            # æ·»åŠ åè°ƒç‰¹å¾
            coordination_features = self._get_coordination_features(truck_id)
            
            # æ·»åŠ é¢„æµ‹ç‰¹å¾
            prediction_features = self._get_prediction_features(truck_id)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            enhanced_state = np.concatenate([
                base_state,
                coordination_features,
                prediction_features
            ])
            
            states.append(enhanced_state)
        
        return states
    
    def _update_global_coordination_info(self):
        """
        æ›´æ–°å…¨å±€åè°ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬å¡è½¦åˆ†å¸ƒã€éœ€æ±‚çƒ­ç‚¹ã€åè°ƒå†²çªç­‰
        """
        # è®¡ç®—å¡è½¦åˆ†å¸ƒå¯†åº¦
        truck_positions = [truck['position'] for truck in self.trucks]
        truck_density = self._calculate_truck_density(truck_positions)
        
        # è¯†åˆ«éœ€æ±‚çƒ­ç‚¹
        demand_hotspots = self._identify_demand_hotspots()
        
        # è®¡ç®—åè°ƒå†²çª
        coordination_conflicts = self._detect_coordination_conflicts()
        
        # è®¡ç®—å…¨å±€è´Ÿè½½åˆ†å¸ƒ
        global_load_distribution = self._calculate_global_load_distribution()
        
        # é¢„æµ‹æœªæ¥éœ€æ±‚è¶‹åŠ¿
        future_demand_trend = self._predict_future_demand_trend()
        
        self.global_coordination_info = {
            'truck_density': truck_density,
            'demand_hotspots': demand_hotspots,
            'coordination_conflicts': coordination_conflicts,
            'global_load_distribution': global_load_distribution,
            'future_demand_trend': future_demand_trend,
            'total_active_trucks': sum(1 for truck in self.trucks if truck['current_location'] != 0),
            'average_truck_distance': np.mean([truck.get('total_distance', 0) for truck in self.trucks]),
            'coordination_efficiency': self._calculate_coordination_efficiency()
        }
    
    def _update_truck_performance_metrics(self):
        """
        æ›´æ–°æ¯ä¸ªå¡è½¦çš„æ€§èƒ½æŒ‡æ ‡
        """
        for truck_id, truck in enumerate(self.trucks):
            metrics = self.truck_performance_metrics[truck_id]
            
            # æ›´æ–°åŸºç¡€æŒ‡æ ‡
            metrics['total_distance'] = truck.get('total_distance', 0)
            metrics['items_delivered'] = truck.get('delivery_items', 0)
            metrics['items_returned'] = truck.get('return_items', 0)
            
            # è®¡ç®—è´Ÿè½½åˆ©ç”¨ç‡
            current_load = truck.get('delivery_items', 0) + truck.get('return_items', 0)
            metrics['load_utilization'] = current_load / self.truck_capacity if self.truck_capacity > 0 else 0
            
            # è®¡ç®—æ•ˆç‡åˆ†æ•°
            if metrics['total_distance'] > 0:
                metrics['efficiency_score'] = (metrics['items_delivered'] + metrics['items_returned']) / metrics['total_distance']
            else:
                metrics['efficiency_score'] = 0
            
            # é¢„æµ‹å®Œæˆæ—¶é—´
            metrics['predicted_completion_time'] = self._predict_truck_completion_time(truck_id)
            
            # æ›´æ–°æœ€è¿‘åŠ¨ä½œå†å²ï¼ˆä¿ç•™æœ€è¿‘10ä¸ªåŠ¨ä½œï¼‰
            if len(metrics['recent_actions']) > 10:
                metrics['recent_actions'] = metrics['recent_actions'][-10:]
    
    def _update_load_balancing_weights(self):
        """
        åŠ¨æ€æ›´æ–°è´Ÿè½½å‡è¡¡æƒé‡
        """
        if self.time_step - self.last_coordination_update >= self.coordination_update_interval:
            # è®¡ç®—æ¯ä¸ªå¡è½¦çš„è´Ÿè½½å’Œæ•ˆç‡
            truck_loads = []
            truck_efficiencies = []
            
            for truck_id in range(self.num_trucks):
                metrics = self.truck_performance_metrics[truck_id]
                truck_loads.append(metrics['load_utilization'])
                truck_efficiencies.append(metrics['efficiency_score'])
            
            # æ ‡å‡†åŒ–è´Ÿè½½å’Œæ•ˆç‡
            if len(truck_loads) > 1:
                load_std = np.std(truck_loads) if np.std(truck_loads) > 0 else 1
                efficiency_mean = np.mean(truck_efficiencies) if np.mean(truck_efficiencies) > 0 else 1
                
                # è®¡ç®—æ–°çš„æƒé‡ï¼šä½è´Ÿè½½é«˜æ•ˆç‡çš„å¡è½¦è·å¾—æ›´é«˜æƒé‡
                for truck_id in range(self.num_trucks):
                    load_factor = 1.0 - truck_loads[truck_id]  # è´Ÿè½½è¶Šä½æƒé‡è¶Šé«˜
                    efficiency_factor = truck_efficiencies[truck_id] / efficiency_mean  # æ•ˆç‡è¶Šé«˜æƒé‡è¶Šé«˜
                    
                    self.load_balancing_weights[truck_id] = 0.6 * load_factor + 0.4 * efficiency_factor
                
                # å½’ä¸€åŒ–æƒé‡
                weight_sum = np.sum(self.load_balancing_weights)
                if weight_sum > 0:
                    self.load_balancing_weights = self.load_balancing_weights / weight_sum * self.num_trucks
            
            self.last_coordination_update = self.time_step
    
    def _get_coordination_features(self, truck_id: int) -> np.ndarray:
        """
        è·å–æŒ‡å®šå¡è½¦çš„åè°ƒç‰¹å¾
        
        Args:
            truck_id: å¡è½¦ID
            
        Returns:
            np.ndarray: åè°ƒç‰¹å¾å‘é‡
        """
        features = []
        
        # å½“å‰å¡è½¦çš„è´Ÿè½½å‡è¡¡æƒé‡
        features.append(self.load_balancing_weights[truck_id])
        
        # ä¸å…¶ä»–å¡è½¦çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯
        current_truck = self.trucks[truck_id]
        current_pos = current_truck['position']
        
        min_distance_to_other = float('inf')
        avg_distance_to_other = 0
        active_trucks_count = 0
        
        for other_id, other_truck in enumerate(self.trucks):
            if other_id != truck_id and other_truck['current_location'] != 0:
                distance = self._euclidean_distance(current_pos, other_truck['position'])
                min_distance_to_other = min(min_distance_to_other, distance)
                avg_distance_to_other += distance
                active_trucks_count += 1
        
        if active_trucks_count > 0:
            avg_distance_to_other /= active_trucks_count
        else:
            min_distance_to_other = 0
            avg_distance_to_other = 0
        
        features.extend([min_distance_to_other / 100.0, avg_distance_to_other / 100.0])  # å½’ä¸€åŒ–
        
        # å…¨å±€åè°ƒä¿¡æ¯
        coord_info = self.global_coordination_info
        features.extend([
            coord_info.get('total_active_trucks', 0) / self.num_trucks,
            coord_info.get('coordination_efficiency', 0),
            len(coord_info.get('coordination_conflicts', [])) / max(1, self.num_trucks),
            len(coord_info.get('demand_hotspots', [])) / max(1, self.num_lockers)
        ])
        
        # ç›¸å¯¹æ€§èƒ½æŒ‡æ ‡
        my_metrics = self.truck_performance_metrics[truck_id]
        all_efficiencies = [self.truck_performance_metrics[i]['efficiency_score'] for i in range(self.num_trucks)]
        all_loads = [self.truck_performance_metrics[i]['load_utilization'] for i in range(self.num_trucks)]
        
        avg_efficiency = np.mean(all_efficiencies) if all_efficiencies else 0
        avg_load = np.mean(all_loads) if all_loads else 0
        
        relative_efficiency = my_metrics['efficiency_score'] - avg_efficiency
        relative_load = my_metrics['load_utilization'] - avg_load
        
        features.extend([relative_efficiency, relative_load])
        
        return np.array(features, dtype=np.float32)
    
    def _get_prediction_features(self, truck_id: int) -> np.ndarray:
        """
        è·å–æŒ‡å®šå¡è½¦çš„é¢„æµ‹ç‰¹å¾
        
        Args:
            truck_id: å¡è½¦ID
            
        Returns:
            np.ndarray: é¢„æµ‹ç‰¹å¾å‘é‡
        """
        features = []
        
        # é¢„æµ‹å®Œæˆæ—¶é—´
        predicted_time = self.truck_performance_metrics[truck_id]['predicted_completion_time']
        normalized_time = predicted_time / self.max_timesteps if self.max_timesteps > 0 else 0
        features.append(normalized_time)
        
        # æœªæ¥éœ€æ±‚è¶‹åŠ¿
        future_trend = self.global_coordination_info.get('future_demand_trend', {})
        features.extend([
            future_trend.get('delivery_trend', 0),
            future_trend.get('return_trend', 0),
            future_trend.get('hotspot_shift', 0)
        ])
        
        # è·¯å¾„ä¼˜åŒ–æ½œåŠ›
        path_optimization_potential = self._calculate_path_optimization_potential(truck_id)
        features.append(path_optimization_potential)
        
        # åè°ƒæœºä¼šè¯„åˆ†
        coordination_opportunity = self._calculate_coordination_opportunity(truck_id)
        features.append(coordination_opportunity)
        
        return np.array(features, dtype=np.float32)

    def get_locker(self, locker_id):
        """æ ¹æ®IDè·å–å¿«é€’æŸœ"""
        for locker in self.lockers_state:
            if locker['id'] == locker_id:
                return locker
        return None

    def _calculate_path_efficiency(self) -> float:
        """
        è®¡ç®—è·¯å¾„æ•ˆç‡ - åŸºäºæœåŠ¡å¯†åº¦çš„åŠ¨æ€ç†æƒ³æ­¥æ•°è¯„ä¼°
        
        æ ¸å¿ƒåŸåˆ™: åŸºäºæœåŠ¡å¯†åº¦åŠ¨æ€è®¡ç®—ç†æƒ³æ­¥æ•°ï¼Œè€ƒè™‘æ— äººæœºå¹¶è¡ŒæœåŠ¡èƒ½åŠ›
        
        è€ƒè™‘å› ç´ :
        - æœåŠ¡å¯†åº¦åŠ¨æ€ç†æƒ³æ­¥æ•°ï¼ˆåŸºäºå¿«é€’æŸœåˆ†å¸ƒå’Œæ— äººæœºè¦†ç›–èŒƒå›´ï¼‰
        - æ— äººæœºå¹¶è¡ŒæœåŠ¡æ•ˆç‡ï¼ˆå¤šæ— äººæœºåŒæ—¶å·¥ä½œçš„æ—¶é—´ä¼˜åŠ¿ï¼‰
        - æœåŠ¡å®Œæˆåº¦å’Œå¤šå¡è½¦åè°ƒå¤æ‚åº¦
        
        è¿”å›:
        - path_efficiency: è·¯å¾„æ•ˆç‡ï¼ˆ0-1ï¼ŒæœŸæœ›å€¼0.6-0.9ä¸ºè‰¯å¥½ï¼‰
        """
        # è·å–å½“å‰æ­¥æ•°
        current_steps = getattr(self, 'time_step', 0)
        if current_steps <= 0:
            return 0.0
        
        # è·å–å·²æœåŠ¡çš„å¿«é€’æŸœæ•°é‡
        served_lockers_count = sum(1 for locker in self.lockers_state if locker.get('served', False))
        if served_lockers_count == 0:
            return 0.0
        
        # 1. åŸºäºæœåŠ¡å¯†åº¦è®¡ç®—åŠ¨æ€ç†æƒ³æ­¥æ•°
        ideal_steps = self._calculate_service_density_based_ideal_steps()
        
        if ideal_steps <= 0:
            return 0.0
        
        # 2. åŸºç¡€æ­¥æ•°æ•ˆç‡
        step_efficiency = min(ideal_steps / current_steps, 1.0)
        
        # 3. æœåŠ¡å®Œæˆåº¦å¥–åŠ±
        total_lockers = len(self.lockers_state)
        completion_rate = served_lockers_count / total_lockers
        completion_bonus = completion_rate * 0.3  # æœ€å¤š30%çš„å¥–åŠ±
        
        # 4. å¤šå¡è½¦åè°ƒå¤æ‚åº¦è°ƒæ•´
        coordination_factor = 1.0 + (self.num_trucks - 1) * 0.1  # æ¯å¢åŠ ä¸€è¾†å¡è½¦ï¼ŒæœŸæœ›æé«˜10%
        
        # 5. ç»¼åˆæ•ˆç‡è®¡ç®—
        base_efficiency = step_efficiency + completion_bonus
        final_efficiency = base_efficiency * coordination_factor
        
        return min(final_efficiency, 1.0)
    
    def _calculate_service_density_based_ideal_steps(self) -> float:
        """
        åŸºäºæœåŠ¡å¯†åº¦è®¡ç®—åŠ¨æ€ç†æƒ³æ­¥æ•°
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - åˆ†ææ¯ä¸ªåœé ç‚¹çš„æœåŠ¡å¯†åº¦ï¼ˆå¯æœåŠ¡å¿«é€’æŸœæ•°é‡å’Œéœ€æ±‚å¯†åº¦ï¼‰
        - è€ƒè™‘æ— äººæœºå¹¶è¡ŒæœåŠ¡èƒ½åŠ›å’Œé£è¡Œæ—¶é—´
        - æ ¹æ®å¿«é€’æŸœåˆ†å¸ƒä¼˜åŒ–åœé ç‚¹é€‰æ‹©ï¼Œæœ€å°åŒ–æ€»æ­¥æ•°
        
        è¿”å›:
        - ideal_steps: åŸºäºæœåŠ¡å¯†åº¦çš„ç†æƒ³æ­¥æ•°
        """
        # è·å–å·²æœåŠ¡çš„å¿«é€’æŸœ
        served_lockers = [locker for locker in self.lockers_state if locker.get('served', False)]
        if not served_lockers:
            return 0.0
        
        # 1. åˆ†ææœåŠ¡å¯†åº¦åˆ†å¸ƒ
        service_density_analysis = self._analyze_service_density_distribution(served_lockers)
        
        # 2. è®¡ç®—æœ€ä¼˜åœé ç‚¹æ•°é‡
        optimal_stops = self._calculate_optimal_stop_count(service_density_analysis)
        
        # 3. è€ƒè™‘æ— äººæœºå¹¶è¡ŒæœåŠ¡æ—¶é—´
        drone_service_time = self._calculate_drone_parallel_service_time(service_density_analysis)
        
        # 4. è®¡ç®—ç†æƒ³æ­¥æ•°
        # åŸºç¡€æ­¥æ•° = åœé ç‚¹æ•°é‡ï¼ˆç§»åŠ¨æ­¥æ•°ï¼‰
        base_steps = optimal_stops
        
        # æœåŠ¡æ—¶é—´æ­¥æ•° = æ— äººæœºå¹¶è¡ŒæœåŠ¡æ—¶é—´ / æ¯æ­¥æ—¶é—´
        # å‡è®¾æ¯æ­¥ä»£è¡¨Config.TRUCK_SERVICE_TIMEçš„æ—¶é—´
        time_per_step = Config.TRUCK_SERVICE_TIME
        service_steps = max(1, int(drone_service_time / time_per_step))
        
        # æ€»ç†æƒ³æ­¥æ•° = ç§»åŠ¨æ­¥æ•° + æœåŠ¡æ­¥æ•°
        ideal_steps = base_steps + service_steps
        
        # 5. å¤šå¡è½¦å¹¶è¡Œä¼˜åŒ–
        if self.num_trucks > 1:
            # å¤šå¡è½¦å¯ä»¥å¹¶è¡Œå·¥ä½œï¼Œå‡å°‘æ€»æ­¥æ•°
            parallel_factor = min(self.num_trucks, optimal_stops)
            if parallel_factor > 1:
                ideal_steps = max(service_steps, ideal_steps / parallel_factor)
        
        return ideal_steps
    
    def _analyze_service_density_distribution(self, served_lockers: List[Dict]) -> Dict[str, Any]:
        """
        åˆ†æå·²æœåŠ¡å¿«é€’æŸœçš„æœåŠ¡å¯†åº¦åˆ†å¸ƒ
        
        å‚æ•°:
        - served_lockers: å·²æœåŠ¡çš„å¿«é€’æŸœåˆ—è¡¨
        
        è¿”å›:
        - æœåŠ¡å¯†åº¦åˆ†æç»“æœ
        """
        if not served_lockers:
            return {'clusters': [], 'total_demand': 0, 'coverage_efficiency': 0.0}
        
        # è®¡ç®—æ€»éœ€æ±‚é‡
        total_demand = sum(
            locker.get('demand_del', 0) + locker.get('demand_ret', 0) 
            for locker in served_lockers
        )
        
        # ä½¿ç”¨èšç±»åˆ†ææ‰¾åˆ°æœåŠ¡å¯†åº¦é›†ä¸­åŒºåŸŸ
        clusters = self._identify_service_clusters(served_lockers)
        
        # è®¡ç®—è¦†ç›–æ•ˆç‡
        coverage_efficiency = self._calculate_coverage_efficiency(clusters)
        
        return {
            'clusters': clusters,
            'total_demand': total_demand,
            'coverage_efficiency': coverage_efficiency,
            'served_count': len(served_lockers)
        }
    
    def _identify_service_clusters(self, served_lockers: List[Dict]) -> List[Dict]:
        """
        è¯†åˆ«æœåŠ¡å¯†åº¦é›†ä¸­çš„åŒºåŸŸï¼ˆèšç±»ï¼‰
        
        å‚æ•°:
        - served_lockers: å·²æœåŠ¡çš„å¿«é€’æŸœåˆ—è¡¨
        
        è¿”å›:
        - æœåŠ¡èšç±»åˆ—è¡¨
        """
        if not served_lockers:
            return []
        
        clusters = []
        drone_range = Config.DRONE_MAX_RANGE
        
        # ç®€å•çš„åŸºäºè·ç¦»çš„èšç±»ç®—æ³•
        unprocessed = served_lockers.copy()
        
        while unprocessed:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªå¿«é€’æŸœä½œä¸ºèšç±»ä¸­å¿ƒ
            center_locker = unprocessed.pop(0)
            cluster = {
                'center': center_locker['location'],
                'lockers': [center_locker],
                'total_demand': center_locker.get('demand_del', 0) + center_locker.get('demand_ret', 0),
                'coverage_radius': 0.0
            }
            
            # æ‰¾åˆ°åœ¨æ— äººæœºèŒƒå›´å†…çš„å…¶ä»–å¿«é€’æŸœ
            remaining = []
            for locker in unprocessed:
                distance = np.sqrt(
                    (center_locker['location'][0] - locker['location'][0])**2 + 
                    (center_locker['location'][1] - locker['location'][1])**2
                )
                
                if distance <= drone_range:
                    cluster['lockers'].append(locker)
                    cluster['total_demand'] += locker.get('demand_del', 0) + locker.get('demand_ret', 0)
                    cluster['coverage_radius'] = max(cluster['coverage_radius'], distance)
                else:
                    remaining.append(locker)
            
            unprocessed = remaining
            clusters.append(cluster)
        
        return clusters
    
    def _calculate_coverage_efficiency(self, clusters: List[Dict]) -> float:
        """
        è®¡ç®—æœåŠ¡è¦†ç›–æ•ˆç‡
        
        å‚æ•°:
        - clusters: æœåŠ¡èšç±»åˆ—è¡¨
        
        è¿”å›:
        - è¦†ç›–æ•ˆç‡ï¼ˆ0-1ï¼‰
        """
        if not clusters:
            return 0.0
        
        total_efficiency = 0.0
        total_weight = 0.0
        
        for cluster in clusters:
            # èšç±»æ•ˆç‡ = æœåŠ¡çš„å¿«é€’æŸœæ•°é‡ / ç†è®ºæœ€å¤§è¦†ç›–æ•°é‡
            lockers_in_cluster = len(cluster['lockers'])
            
            # ç†è®ºæœ€å¤§è¦†ç›–ï¼šåŸºäºæ— äººæœºæ•°é‡å’ŒæœåŠ¡æ—¶é—´
            max_drones = Config.DRONE_NUM
            max_coverage = min(lockers_in_cluster, max_drones * 2)  # å‡è®¾æ¯ä¸ªæ— äººæœºå¯ä»¥æœåŠ¡2ä¸ªå¿«é€’æŸœ
            
            if max_coverage > 0:
                cluster_efficiency = lockers_in_cluster / max_coverage
                weight = cluster['total_demand']
                
                total_efficiency += cluster_efficiency * weight
                total_weight += weight
        
        return total_efficiency / total_weight if total_weight > 0 else 0.0
    
    def _calculate_optimal_stop_count(self, service_density_analysis: Dict[str, Any]) -> int:
        """
        è®¡ç®—æœ€ä¼˜åœé ç‚¹æ•°é‡
        
        å‚æ•°:
        - service_density_analysis: æœåŠ¡å¯†åº¦åˆ†æç»“æœ
        
        è¿”å›:
        - æœ€ä¼˜åœé ç‚¹æ•°é‡
        """
        clusters = service_density_analysis.get('clusters', [])
        if not clusters:
            return 1
        
        # åŸºç¡€åœé ç‚¹æ•°é‡ = èšç±»æ•°é‡
        base_stops = len(clusters)
        
        # æ ¹æ®è¦†ç›–æ•ˆç‡è°ƒæ•´
        coverage_efficiency = service_density_analysis.get('coverage_efficiency', 0.0)
        
        # å¦‚æœè¦†ç›–æ•ˆç‡ä½ï¼Œå¯èƒ½éœ€è¦æ›´å¤šåœé ç‚¹
        if coverage_efficiency < 0.6:
            # æ•ˆç‡ä½æ—¶ï¼Œå¢åŠ åœé ç‚¹
            adjustment_factor = 1.5
        elif coverage_efficiency > 0.8:
            # æ•ˆç‡é«˜æ—¶ï¼Œå¯ä»¥å‡å°‘åœé ç‚¹
            adjustment_factor = 0.8
        else:
            adjustment_factor = 1.0
        
        optimal_stops = max(1, int(base_stops * adjustment_factor))
        
        return optimal_stops
    
    def _calculate_drone_parallel_service_time(self, service_density_analysis: Dict[str, Any]) -> float:
        """
        è®¡ç®—æ— äººæœºå¹¶è¡ŒæœåŠ¡æ—¶é—´
        
        å‚æ•°:
        - service_density_analysis: æœåŠ¡å¯†åº¦åˆ†æç»“æœ
        
        è¿”å›:
        - å¹¶è¡ŒæœåŠ¡æ—¶é—´ï¼ˆç§’ï¼‰
        """
        clusters = service_density_analysis.get('clusters', [])
        if not clusters:
            return Config.DRONE_SERVICE_TIME
        
        max_service_time = 0.0
        
        for cluster in clusters:
            lockers_count = len(cluster['lockers'])
            total_demand = cluster['total_demand']
            
            # è®¡ç®—è¯¥èšç±»çš„æœåŠ¡æ—¶é—´
            # è€ƒè™‘æ— äººæœºæ•°é‡å’Œå¹¶è¡Œèƒ½åŠ›
            available_drones = Config.DRONE_NUM
            
            # æ¯ä¸ªæ— äººæœºçš„æœåŠ¡æ—¶é—´ = é£è¡Œæ—¶é—´ + æœåŠ¡æ—¶é—´
            avg_flight_distance = cluster['coverage_radius']
            flight_time = (avg_flight_distance * 2) / Config.DRONE_SPEED  # å¾€è¿”æ—¶é—´
            service_time_per_demand = Config.DRONE_SERVICE_TIME
            
            # æ€»æœåŠ¡æ—¶é—´ = é£è¡Œæ—¶é—´ + éœ€æ±‚æœåŠ¡æ—¶é—´
            total_service_time_per_drone = flight_time + (total_demand * service_time_per_demand / available_drones)
            
            # å¹¶è¡ŒæœåŠ¡æ—¶é—´ = æœ€é•¿çš„å•ä¸ªæ— äººæœºæœåŠ¡æ—¶é—´
            cluster_service_time = total_service_time_per_drone
            
            max_service_time = max(max_service_time, cluster_service_time)
        
        return max_service_time
    
    def _calculate_demand_weighted_optimal_distance(self, served_lockers: List[Dict]) -> float:
        """
        è®¡ç®—éœ€æ±‚é‡åŠ æƒçš„ç†æƒ³è·¯å¾„è·ç¦»
        
        æ ¸å¿ƒæ€æƒ³:
        - é«˜éœ€æ±‚é‡çš„å¿«é€’æŸœåº”è¯¥ä¼˜å…ˆè®¿é—®ï¼ˆè·ç¦»æƒé‡æ›´ä½ï¼‰
        - è·¯å¾„è§„åˆ’åº”è¯¥è€ƒè™‘éœ€æ±‚å¯†åº¦ï¼Œè€Œä¸ä»…ä»…æ˜¯åœ°ç†è·ç¦»
        - ä½¿ç”¨éœ€æ±‚é‡å¯¹è·ç¦»è¿›è¡ŒåŠ æƒï¼Œåæ˜ å®é™…çš„æœåŠ¡ä»·å€¼
        
        å‚æ•°:
        - served_lockers: å·²æœåŠ¡çš„å¿«é€’æŸœåˆ—è¡¨ï¼ŒåŒ…å«ä½ç½®å’Œéœ€æ±‚é‡ä¿¡æ¯
        
        è¿”å›:
        - weighted_optimal_distance: éœ€æ±‚é‡åŠ æƒçš„ç†æƒ³è·ç¦»
        """
        if not served_lockers:
            return 0.0
        
        if len(served_lockers) == 1:
            # å•ä¸ªå¿«é€’æŸœï¼šä»“åº“å¾€è¿”è·ç¦»ï¼ŒæŒ‰éœ€æ±‚é‡è°ƒæ•´
            locker = served_lockers[0]
            base_distance = self._euclidean_distance(self.depot, locker['location']) * 2
            # éœ€æ±‚é‡è¶Šé«˜ï¼Œç†æƒ³è·ç¦»ç›¸å¯¹è¶ŠçŸ­ï¼ˆæ•ˆç‡è¶Šé«˜ï¼‰
            demand_factor = 1.0 / (1.0 + locker['demand'] * 0.1)  # éœ€æ±‚é‡è¶Šé«˜ï¼Œå› å­è¶Šå°
            return base_distance * demand_factor
        
        # å¤šä¸ªå¿«é€’æŸœï¼šä½¿ç”¨éœ€æ±‚é‡åŠ æƒçš„TSPç®—æ³•
        weighted_distance = self._estimate_demand_weighted_tsp(served_lockers)
        
        return weighted_distance
    
    def _estimate_demand_weighted_tsp(self, lockers_with_demand: List[Dict]) -> float:
        """
        ä½¿ç”¨éœ€æ±‚é‡åŠ æƒçš„æœ€è¿‘é‚»ç®—æ³•ä¼°ç®—TSPè·ç¦»
        
        ç®—æ³•æ€è·¯:
        1. è®¡ç®—æ‰€æœ‰å¿«é€’æŸœä¹‹é—´çš„éœ€æ±‚é‡åŠ æƒè·ç¦»
        2. ä¼˜å…ˆé€‰æ‹©é«˜éœ€æ±‚é‡/è·ç¦»æ¯”çš„å¿«é€’æŸœ
        3. ä»ä»“åº“å‡ºå‘å¹¶è¿”å›ä»“åº“
        
        å‚æ•°:
        - lockers_with_demand: åŒ…å«ä½ç½®å’Œéœ€æ±‚é‡çš„å¿«é€’æŸœåˆ—è¡¨
        
        è¿”å›:
        - estimated_weighted_distance: ä¼°ç®—çš„åŠ æƒè·ç¦»
        """
        if not lockers_with_demand:
            return 0.0
        
        # ä»ä»“åº“å¼€å§‹
        current_pos = self.depot
        unvisited = lockers_with_demand.copy()
        total_weighted_distance = 0.0
        
        # éœ€æ±‚é‡åŠ æƒçš„æœ€è¿‘é‚»ç®—æ³•
        while unvisited:
            best_idx = 0
            best_score = float('inf')
            
            for i, locker in enumerate(unvisited):
                # è®¡ç®—åœ°ç†è·ç¦»
                geo_distance = self._euclidean_distance(current_pos, locker['location'])
                
                # è®¡ç®—éœ€æ±‚é‡åŠ æƒåˆ†æ•°ï¼šè·ç¦» / éœ€æ±‚é‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                # é«˜éœ€æ±‚é‡çš„å¿«é€’æŸœä¼šæœ‰æ›´ä½çš„åˆ†æ•°ï¼Œä¼˜å…ˆè¢«é€‰æ‹©
                demand_weight = max(locker['demand'], 1)  # é¿å…é™¤é›¶
                weighted_score = geo_distance / demand_weight
                
                if weighted_score < best_score:
                    best_score = weighted_score
                    best_idx = i
            
            # ç§»åŠ¨åˆ°æœ€ä¼˜å¿«é€’æŸœ
            selected_locker = unvisited.pop(best_idx)
            move_distance = self._euclidean_distance(current_pos, selected_locker['location'])
            
            # æ ¹æ®éœ€æ±‚é‡è°ƒæ•´å®é™…è·ç¦»æƒé‡
            demand_factor = 1.0 / (1.0 + selected_locker['demand'] * 0.05)
            weighted_move_distance = move_distance * demand_factor
            
            total_weighted_distance += weighted_move_distance
            current_pos = selected_locker['location']
        
        # è¿”å›ä»“åº“çš„è·ç¦»
        return_distance = self._euclidean_distance(current_pos, self.depot)
        total_weighted_distance += return_distance
        
        return total_weighted_distance
    
    def _calculate_realistic_optimal_distance(self, served_lockers: List[Dict]) -> float:
        """
        è®¡ç®—æ›´ç°å®çš„ç†æƒ³è·¯å¾„è·ç¦»
        
        ç›¸æ¯”éœ€æ±‚é‡åŠ æƒTSPï¼Œè¿™ä¸ªæ–¹æ³•ï¼š
        - å‡å°‘éœ€æ±‚é‡æƒé‡çš„è¿‡åº¦å½±å“
        - è€ƒè™‘å®é™…çº¦æŸæ¡ä»¶
        - æä¾›æ›´åˆç†çš„åŸºå‡†
        
        å‚æ•°:
        - served_lockers: å·²æœåŠ¡çš„å¿«é€’æŸœåˆ—è¡¨
        
        è¿”å›:
        - realistic_optimal_distance: ç°å®ç†æƒ³è·ç¦»
        """
        if not served_lockers:
            return 0.0
        
        if len(served_lockers) == 1:
            # å•ä¸ªå¿«é€’æŸœï¼šä»“åº“å¾€è¿”è·ç¦»ï¼Œè½»å¾®éœ€æ±‚é‡è°ƒæ•´
            locker = served_lockers[0]
            base_distance = self._euclidean_distance(self.depot, locker['location']) * 2
            # éœ€æ±‚é‡è°ƒæ•´å› å­æ›´ä¿å®ˆ
            demand_factor = 0.9 + 0.1 / (1.0 + locker['demand'] * 0.02)  # æœ€å¤š10%çš„è°ƒæ•´
            return base_distance * demand_factor
        
        # å¤šä¸ªå¿«é€’æŸœï¼šåŸºç¡€TSP + è½»å¾®éœ€æ±‚é‡è°ƒæ•´
        positions = [locker['location'] for locker in served_lockers]
        base_tsp_distance = self._estimate_tsp_distance(positions)
        
        # éœ€æ±‚é‡è°ƒæ•´ï¼šè®¡ç®—å¹³å‡éœ€æ±‚å¯†åº¦
        total_demand = sum(locker['demand'] for locker in served_lockers)
        avg_demand = total_demand / len(served_lockers)
        
        # éœ€æ±‚å¯†åº¦è°ƒæ•´å› å­ï¼ˆæ›´ä¿å®ˆï¼‰
        demand_adjustment = 0.95 + 0.05 / (1.0 + avg_demand * 0.01)  # æœ€å¤š5%çš„è°ƒæ•´
        
        return base_tsp_distance * demand_adjustment
    
    def _calculate_optimal_route_distance(self, served_lockers: List[Dict]) -> float:
        """
        è®¡ç®—æœåŠ¡å·²å®Œæˆå¿«é€’æŸœçš„ç†è®ºæœ€ä¼˜è·¯å¾„è·ç¦»ï¼ˆä¿ç•™åŸæ–¹æ³•ç”¨äºå…¼å®¹æ€§ï¼‰
        
        å‚æ•°:
        - served_lockers: å·²æœåŠ¡çš„å¿«é€’æŸœåˆ—è¡¨
        
        è¿”å›:
        - optimal_distance: ç†è®ºæœ€ä¼˜è·ç¦»
        """
        if not served_lockers:
            return 0.0
        
        # è·å–å¿«é€’æŸœä½ç½®
        locker_positions = [locker['location'] if isinstance(locker, dict) and 'location' in locker 
                          else locker for locker in served_lockers]
        
        if len(locker_positions) == 1:
            # å•ä¸ªå¿«é€’æŸœï¼šä»“åº“å¾€è¿”è·ç¦»
            distance_to_locker = self._euclidean_distance(self.depot, locker_positions[0])
            return distance_to_locker * 2  # å¾€è¿”
        
        # å¤šä¸ªå¿«é€’æŸœï¼šä½¿ç”¨æœ€è¿‘é‚»ç®—æ³•ä¼°ç®—æœ€ä¼˜è·¯å¾„
        optimal_distance = self._estimate_tsp_distance(locker_positions)
        
        return optimal_distance
    
    def _estimate_tsp_distance(self, positions: List[Tuple[float, float]]) -> float:
        """
        ä½¿ç”¨æœ€è¿‘é‚»ç®—æ³•ä¼°ç®—TSPè·ç¦»ï¼ˆä»ä»“åº“å‡ºå‘å¹¶è¿”å›ï¼‰
        
        å‚æ•°:
        - positions: å¿«é€’æŸœä½ç½®åˆ—è¡¨
        
        è¿”å›:
        - estimated_distance: ä¼°ç®—çš„æ€»è·ç¦»
        """
        if not positions:
            return 0.0
        
        # ä»ä»“åº“å¼€å§‹
        current_pos = self.depot
        unvisited = positions.copy()
        total_distance = 0.0
        
        # æœ€è¿‘é‚»ç®—æ³•
        while unvisited:
            # æ‰¾åˆ°æœ€è¿‘çš„æœªè®¿é—®å¿«é€’æŸœ
            nearest_idx = 0
            min_distance = self._euclidean_distance(current_pos, unvisited[0])
            
            for i, pos in enumerate(unvisited[1:], 1):
                distance = self._euclidean_distance(current_pos, pos)
                if distance < min_distance:
                    min_distance = distance
                    nearest_idx = i
            
            # ç§»åŠ¨åˆ°æœ€è¿‘çš„å¿«é€’æŸœ
            total_distance += min_distance
            current_pos = unvisited.pop(nearest_idx)
        
        # è¿”å›ä»“åº“
        total_distance += self._euclidean_distance(current_pos, self.depot)
        
        return total_distance
    
    def _calculate_completion_rate(self) -> float:
        """
        è®¡ç®—å®Œæˆç‡
        
        è¿”å›:
        - completion_rate: å®Œæˆç‡ï¼ˆ0-1ï¼‰
        """
        if not self.lockers_state:
            return 0.0
        
        served_count = sum(1 for locker in self.lockers_state if locker.get('served', False))
        total_count = len(self.lockers_state)
        
        return served_count / total_count
    
    def _calculate_capacity_utilization(self) -> float:
        """
        è®¡ç®—å®¹é‡åˆ©ç”¨ç‡
        
        è¿”å›:
        - capacity_utilization: å®¹é‡åˆ©ç”¨ç‡ï¼ˆ0-1ï¼‰
        """
        if not self.trucks:
            return 0.0
        
        total_utilization = 0.0
        for truck in self.trucks:
            current_load = truck.get('current_delivery_load', 0) + truck.get('current_return_load', 0)
            capacity = truck.get('capacity', self.truck_capacity)
            utilization = current_load / capacity if capacity > 0 else 0
            total_utilization += utilization
        
        return total_utilization / len(self.trucks)

    def get_locker_location(self, locker_id):
        """æ ¹æ®IDè·å–å¿«é€’æŸœä½ç½®"""
        locker = self.get_locker(locker_id)
        return locker['location'] if locker else self.depot

    def step(self, actions):
        """
        æ–°çš„åŠ¨æ€stepæ–¹æ³•ï¼šä½¿ç”¨å¤–éƒ¨å®ç°çš„åŠ¨æ€è°ƒåº¦é€»è¾‘
        """
        return dynamic_step(self, actions)
    
    def _update_demand_and_handle_uncertainty(self):
        """
        æ›´æ–°éœ€æ±‚æ¨¡å‹å’Œå¤„ç†ä¸ç¡®å®šæ€§
        """
        # æ”¶é›†å½“å‰è§‚å¯Ÿåˆ°çš„éœ€æ±‚æ•°æ®
        observed_demands = {}
        for locker in self.lockers_state:
            observed_demands[locker['id']] = {
                'delivery': locker['demand_del'],
                'return': locker['demand_ret']
            }
        
        # æ›´æ–°éœ€æ±‚æ¨¡å‹
        self.uncertainty_handler.update_demand_model(self.time_step, observed_demands)
        
        # æ£€æµ‹å’Œå¤„ç†éœ€æ±‚å†²å‡»
        shock_response = self.uncertainty_handler.handle_demand_shock(
            self.trucks, self.lockers_state
        )
        
        # å¦‚æœæ£€æµ‹åˆ°å®¹é‡çŸ­ç¼ºï¼Œè®°å½•ç›¸å…³ä¿¡æ¯ï¼ˆç§»é™¤é¢‘ç¹çš„æ—¶é—´æ­¥è¾“å‡ºï¼‰
        if shock_response['shortage_analysis']['shortage_detected']:
            # å®¹é‡çŸ­ç¼ºä¿¡æ¯å·²è®°å½•ï¼Œä½†ä¸åœ¨æ¯ä¸ªæ—¶é—´æ­¥è¾“å‡º
            pass
        
        # ä¸ºæœªæœåŠ¡çš„å¿«é€’æŸœæ›´æ–°éœ€æ±‚ï¼ˆæ¨¡æ‹Ÿéœ€æ±‚å˜åŒ–ï¼‰
        for locker in self.lockers_state:
            if not locker['served']:
                # è·å–æ›´æ–°çš„éœ€æ±‚ä¼°è®¡
                delivery_estimate = self.uncertainty_handler.get_robust_demand_estimate(
                    locker['id'], 'delivery', self.time_step
                )
                return_estimate = self.uncertainty_handler.get_robust_demand_estimate(
                    locker['id'], 'return', self.time_step
                )
                
                # æ›´æ–°éœ€æ±‚ï¼ˆæ·»åŠ å°å¹…éšæœºå˜åŒ–ï¼‰
                demand_change_factor = 0.05  # 5%çš„å˜åŒ–å¹…åº¦
                delivery_change = (delivery_estimate['actual'] - locker['demand_del']) * demand_change_factor
                return_change = (return_estimate['actual'] - locker['demand_ret']) * demand_change_factor
                
                locker['demand_del'] = max(0, locker['demand_del'] + delivery_change)
                locker['demand_ret'] = max(0, locker['demand_ret'] + return_change)
                
                # æ›´æ–°ä¸ç¡®å®šæ€§ä¿¡æ¯
                locker['uncertainty_del'] = delivery_estimate['uncertainty']
                locker['uncertainty_ret'] = return_estimate['uncertainty']

    def optimize_time_windows(self):
        """
        ä¼˜åŒ–æ—¶é—´çª—å‚æ•°ï¼ŒåŸºäºå†å²æ€§èƒ½æ•°æ®è°ƒæ•´æ—¶é—´çª—è®¾ç½®
        """
        # æ”¶é›†æ€§èƒ½æ•°æ®
        performance_data = {}
        for locker_id in range(self.num_lockers):
            locker = self.get_locker(locker_id)
            performance_data[locker_id] = {
                'served': locker['served'],
                'demand_del': locker['demand_del'],
                'demand_ret': locker['demand_ret'],
                'service_time': self.time_step if locker['served'] else None
            }
        
        # ä½¿ç”¨æ—¶é—´çª—ä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–
        self.time_window_optimizer.optimize_time_windows(performance_data)
        
        # æ—¶é—´çª—ä¼˜åŒ–å®Œæˆï¼ˆç§»é™¤é¢‘ç¹çš„æ—¶é—´æ­¥è¾“å‡ºï¼‰
    
    def get_time_window_statistics(self):
        """
        è·å–æ—¶é—´çª—çº¦æŸçš„ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total_violations': 0,
            'early_violations': 0,
            'late_violations': 0,
            'total_penalty': 0.0,
            'average_penalty': 0.0
        }
        
        try:
            violations = self.soft_time_window_manager.get_violation_statistics()
            for violation in violations:
                stats['total_violations'] += 1
                # å¦‚æœviolationæ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡penaltyè®¡ç®—
                if hasattr(violation, 'penalty'):
                    stats['total_penalty'] += violation.penalty
                    
                    if hasattr(violation, 'violation_type') and hasattr(violation.violation_type, 'name'):
                        if violation.violation_type.name == 'EARLY':
                            stats['early_violations'] += 1
                        elif violation.violation_type.name == 'LATE':
                            stats['late_violations'] += 1
        except Exception as e:
            # å¦‚æœè·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            pass
        
        if stats['total_violations'] > 0:
            stats['average_penalty'] = stats['total_penalty'] / stats['total_violations']
        
        return stats

    def _check_replenishment_need(self, truck: Dict, truck_id: int) -> ReplenishmentDecision:
        """
        æ£€æŸ¥å¡è½¦æ˜¯å¦éœ€è¦è¡¥è´§
        
        Args:
            truck: å¡è½¦çŠ¶æ€å­—å…¸
            truck_id: å¡è½¦ID
            
        Returns:
            ReplenishmentDecision: è¡¥è´§å†³ç­–ç»“æœ
        """
        # è½¬æ¢å¡è½¦çŠ¶æ€ä¸ºè¡¥è´§æ¨¡å—æ ¼å¼
        truck_state = TruckState(
            truck_id=truck_id,
            current_location=truck['current_location'],
            current_delivery_load=truck['current_delivery_load'],
            current_return_load=truck['current_return_load'],
            remaining_capacity=truck['remaining_space'],
            total_distance=truck['total_distance'],
            visited_stops=truck['visited_stops'].copy(),
            returned=truck['current_location'] == 0  # æ ¹æ®ä½ç½®åˆ¤æ–­æ˜¯å¦åœ¨ä»“åº“
        )
        
        # è·å–å‰©ä½™æœªæœåŠ¡çš„å¿«é€’æŸœ
        remaining_lockers = []
        for locker in self.lockers_state:
            if not locker['served']:
                # è·å–æ—¶é—´çª—ä¿¡æ¯
                time_window = self.soft_time_window_manager.get_time_window(locker['id'])
                
                locker_demand = LockerDemand(
                    locker_id=locker['id'],
                    location=locker['location'],
                    delivery_demand=locker['demand_del'],
                    return_demand=locker['demand_ret'],
                    served=locker['served'],
                    priority=self._calculate_locker_priority(locker),
                    time_window_start=time_window.preferred_start if time_window else 0,
                    time_window_end=time_window.preferred_end if time_window else 100
                )
                remaining_lockers.append(locker_demand)
        
        # ä½¿ç”¨è¡¥è´§ä¼˜åŒ–å™¨è¿›è¡Œå†³ç­–
        return self.replenishment_optimizer.should_replenish(
            truck_state, remaining_lockers, self.time_step
        )
    
    def _calculate_locker_priority(self, locker: Dict) -> float:
        """
        è®¡ç®—å¿«é€’æŸœä¼˜å…ˆçº§
        
        Args:
            locker: å¿«é€’æŸœçŠ¶æ€å­—å…¸
            
        Returns:
            float: ä¼˜å…ˆçº§åˆ†æ•° (0-1)
        """
        # åŸºäºéœ€æ±‚é‡è®¡ç®—åŸºç¡€ä¼˜å…ˆçº§
        total_demand = locker['demand_del'] + locker['demand_ret']
        demand_priority = min(1.0, total_demand / 30.0)  # å‡è®¾æœ€å¤§éœ€æ±‚ä¸º30
        
        # åŸºäºè·ç¦»è®¡ç®—ä¼˜å…ˆçº§ï¼ˆè·ç¦»è¶Šè¿‘ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        locker_location = locker['location']
        distance_to_depot = self._euclidean_distance(locker_location, self.depot)
        distance_priority = 1.0 / (1.0 + distance_to_depot / 50.0)
        
        # åŸºäºä¸ç¡®å®šæ€§è®¡ç®—ä¼˜å…ˆçº§ï¼ˆä¸ç¡®å®šæ€§è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        uncertainty_del = locker.get('uncertainty_del', 0.0)
        uncertainty_ret = locker.get('uncertainty_ret', 0.0)
        uncertainty_priority = (uncertainty_del + uncertainty_ret) / 2.0
        
        # ç»¼åˆä¼˜å…ˆçº§
        return (demand_priority * 0.5 + distance_priority * 0.3 + uncertainty_priority * 0.2)
    
    def _execute_replenishment(self, truck: Dict, decision: ReplenishmentDecision, truck_id: int):
        """
        æ‰§è¡Œè¡¥è´§å†³ç­–
        
        Args:
            truck: å¡è½¦çŠ¶æ€å­—å…¸
            decision: è¡¥è´§å†³ç­–
            truck_id: å¡è½¦ID
        """
        if self.verbose:
            print(f"å¡è½¦ {truck_id} æ‰§è¡Œè¡¥è´§å†³ç­–:")
            print(f"  è§¦å‘åŸå› : {decision.trigger_reason.value}")
            print(f"  ç´§æ€¥ç¨‹åº¦: {decision.urgency_level:.2f}")
            print(f"  é¢„æœŸæ”¶ç›Š: {decision.expected_benefit:.2f}")
            print(f"  é£é™©è¯„ä¼°: {decision.risk_assessment:.2f}")
            print(f"  ç½®ä¿¡åº¦: {decision.confidence:.2f}")
        
        # è®°å½•è¡¥è´§å‰çŠ¶æ€
        old_location_id = truck['current_location']
        old_location = self.depot if old_location_id == 0 else self.get_locker_location(old_location_id)
        
        # è®¡ç®—è¿”å›ä»“åº“çš„è·ç¦»
        depot_distance = self._euclidean_distance(old_location, self.depot)
        truck['total_distance'] += depot_distance
        self.total_truck_distance += depot_distance
        
        # æ›´æ–°å¡è½¦çŠ¶æ€ - è¿”å›ä»“åº“è¡¥è´§
        truck['current_location'] = 0
        truck['current_delivery_load'] = self.initial_delivery_load  # é‡æ–°è£…è½½é…é€è´§ç‰©
        truck['current_return_load'] = 0  # å¸è½½å–ä»¶è´§ç‰©
        truck['remaining_space'] = self.truck_capacity - truck['current_delivery_load']
        
        # è®°å½•è¡¥è´§äº‹ä»¶
        self._record_replenishment_event(truck_id, decision, depot_distance)
        
        if self.verbose:
            print(f"  è¡¥è´§å®Œæˆï¼Œè¡Œé©¶è·ç¦»: {depot_distance:.2f}")
            print(f"  æ–°çš„é…é€è½½é‡: {truck['current_delivery_load']}")
            print(f"  å‰©ä½™å®¹é‡: {truck['remaining_space']}")
    
    def _record_replenishment_event(self, truck_id: int, decision: ReplenishmentDecision, distance: float):
        """
        è®°å½•è¡¥è´§äº‹ä»¶ç”¨äºæ€§èƒ½åˆ†æ
        
        Args:
            truck_id: å¡è½¦ID
            decision: è¡¥è´§å†³ç­–
            distance: è¡¥è´§è¡Œé©¶è·ç¦»
        """
        # è¿™é‡Œå¯ä»¥è®°å½•è¡¥è´§äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºåç»­æ€§èƒ½åˆ†æ
        # æš‚æ—¶åªåœ¨verboseæ¨¡å¼ä¸‹è¾“å‡º
        pass
    
    def get_replenishment_statistics(self) -> Dict[str, Any]:
        """
        è·å–è¡¥è´§ç­–ç•¥ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: è¡¥è´§ç»Ÿè®¡ä¿¡æ¯
        """
        return self.replenishment_optimizer.get_strategy_statistics()
    
    def get_truck_capacity_status(self) -> Dict[str, Any]:
        """
        è·å–è¯¦ç»†çš„å¡è½¦å®¹é‡çŠ¶æ€ä¿¡æ¯
        
        Returns:
            Dict: åŒ…å«æ‰€æœ‰å¡è½¦å®¹é‡çŠ¶æ€çš„è¯¦ç»†ä¿¡æ¯
        """
        truck_status = []
        total_delivery_load = 0
        total_return_load = 0
        total_remaining_space = 0
        
        for i, truck in enumerate(self.trucks):
            delivery_load = truck['current_delivery_load']
            return_load = truck['current_return_load']
            remaining_space = truck['remaining_space']
            
            # è®¡ç®—åˆ©ç”¨ç‡
            used_capacity = delivery_load + return_load
            utilization_rate = used_capacity / self.truck_capacity if self.truck_capacity > 0 else 0
            
            # è®¡ç®—å®¹é‡çŠ¶æ€
            capacity_status = "æ­£å¸¸"
            if utilization_rate > 0.9:
                capacity_status = "æ¥è¿‘æ»¡è½½"
            elif utilization_rate > 0.7:
                capacity_status = "é«˜è´Ÿè½½"
            elif utilization_rate < 0.3:
                capacity_status = "ä½è´Ÿè½½"
            
            truck_info = {
                'truck_id': i,
                'current_location': truck['current_location'],
                'delivery_load': delivery_load,
                'return_load': return_load,
                'total_load': used_capacity,
                'remaining_space': remaining_space,
                'capacity': self.truck_capacity,
                'utilization_rate': utilization_rate,
                'capacity_status': capacity_status,
                'returned': truck['returned'],
                'visited_stops': len(truck['visited_stops']),
                'total_distance': truck['total_distance']
            }
            truck_status.append(truck_info)
            
            # ç´¯è®¡ç»Ÿè®¡
            total_delivery_load += delivery_load
            total_return_load += return_load
            total_remaining_space += remaining_space
        
        # è®¡ç®—è½¦é˜Ÿçº§åˆ«ç»Ÿè®¡
        fleet_capacity = self.num_trucks * self.truck_capacity
        fleet_used = total_delivery_load + total_return_load
        fleet_utilization = fleet_used / fleet_capacity if fleet_capacity > 0 else 0
        
        return {
            'individual_trucks': truck_status,
            'fleet_summary': {
                'total_trucks': self.num_trucks,
                'total_capacity': fleet_capacity,
                'total_delivery_load': total_delivery_load,
                'total_return_load': total_return_load,
                'total_used_capacity': fleet_used,
                'total_remaining_space': total_remaining_space,
                'fleet_utilization_rate': fleet_utilization,
                'average_utilization': np.mean([truck['utilization_rate'] for truck in truck_status]),
                'max_utilization': max([truck['utilization_rate'] for truck in truck_status]) if truck_status else 0,
                'min_utilization': min([truck['utilization_rate'] for truck in truck_status]) if truck_status else 0
            }
        }
    
    def validate_truck_capacity_consistency(self) -> Dict[str, Any]:
        """
        éªŒè¯å¡è½¦å®¹é‡çŠ¶æ€çš„ä¸€è‡´æ€§
        
        Returns:
            Dict: å®¹é‡ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
        """
        issues = []
        warnings = []
        
        for i, truck in enumerate(self.trucks):
            delivery_load = truck['current_delivery_load']
            return_load = truck['current_return_load']
            remaining_space = truck['remaining_space']
            
            # æ£€æŸ¥å®¹é‡è®¡ç®—æ˜¯å¦æ­£ç¡®
            calculated_remaining = self.truck_capacity - delivery_load - return_load
            if abs(calculated_remaining - remaining_space) > 0.01:  # å…è®¸å°çš„æµ®ç‚¹è¯¯å·®
                issues.append(f"å¡è½¦{i}å®¹é‡è®¡ç®—ä¸ä¸€è‡´: è®¡ç®—å€¼={calculated_remaining}, è®°å½•å€¼={remaining_space}")
            
            # æ£€æŸ¥æ˜¯å¦è¶…è½½
            total_load = delivery_load + return_load
            if total_load > self.truck_capacity:
                issues.append(f"å¡è½¦{i}è¶…è½½: æ€»è´Ÿè½½={total_load}, å®¹é‡={self.truck_capacity}")
            
            # æ£€æŸ¥è´Ÿå€¼
            if delivery_load < 0:
                issues.append(f"å¡è½¦{i}é€è´§è´Ÿè½½ä¸ºè´Ÿå€¼: {delivery_load}")
            if return_load < 0:
                issues.append(f"å¡è½¦{i}é€€è´§è´Ÿè½½ä¸ºè´Ÿå€¼: {return_load}")
            if remaining_space < 0:
                issues.append(f"å¡è½¦{i}å‰©ä½™ç©ºé—´ä¸ºè´Ÿå€¼: {remaining_space}")
            
            # æ£€æŸ¥è­¦å‘Šæƒ…å†µ
            if total_load == 0 and not truck['returned']:
                warnings.append(f"å¡è½¦{i}è´Ÿè½½ä¸º0ä½†æœªè¿”å›")
            
            utilization = total_load / self.truck_capacity
            if utilization > 0.95:
                warnings.append(f"å¡è½¦{i}åˆ©ç”¨ç‡è¿‡é«˜: {utilization:.2%}")
        
        return {
            'is_consistent': len(issues) == 0,
            'issues': issues,
            'warnings': warnings,
            'total_issues': len(issues),
            'total_warnings': len(warnings)
        }
    
    def _calculate_regional_density(self, center_point: Tuple[float, float], radius: float = 20.0) -> Dict[str, float]:
        """
        è®¡ç®—æŒ‡å®šåŒºåŸŸçš„å¿«é€’æŸœå¯†é›†åº¦å’Œéœ€æ±‚èšåˆä¿¡æ¯
        
        Args:
            center_point: ä¸­å¿ƒç‚¹åæ ‡ (x, y)
            radius: è¦†ç›–åŠå¾„
            
        Returns:
            Dict[str, float]: åŒºåŸŸç‰¹å¾ä¿¡æ¯
        """
        lockers_in_range = []
        total_pickup_demand = 0.0
        total_return_demand = 0.0
        
        for i, locker in enumerate(self.lockers_state):
            locker_pos = locker['location']
            distance = self._euclidean_distance(center_point, locker_pos)
            
            if distance <= radius:
                lockers_in_range.append(i)
                total_pickup_demand += locker.get('demand_del', 0)
                total_return_demand += locker.get('demand_ret', 0)
        
        # è®¡ç®—å¯†é›†åº¦æŒ‡æ ‡
        area = math.pi * radius * radius
        density = len(lockers_in_range) / area if area > 0 else 0
        
        # è®¡ç®—éœ€æ±‚å¯†åº¦
        demand_density = (total_pickup_demand + total_return_demand) / area if area > 0 else 0
        
        # è®¡ç®—æœåŠ¡æ•ˆç‡æ½œåŠ›ï¼ˆæœªæœåŠ¡çš„éœ€æ±‚æ¯”ä¾‹ï¼‰
        unserved_lockers = sum(1 for i in lockers_in_range 
                              if not self.lockers_state[i].get('served', False))
        
        service_potential = unserved_lockers / max(len(lockers_in_range), 1)
        
        return {
            'locker_count': len(lockers_in_range),
            'locker_density': density,
            'total_demand': total_pickup_demand + total_return_demand,
            'demand_density': demand_density,
            'service_potential': service_potential,
            'coverage_efficiency': len(lockers_in_range) / max(self.num_lockers, 1)
        }
    
    def _get_global_features(self) -> Dict[str, Any]:
        """
        è®¡ç®—å…¨å±€ç‰¹å¾ï¼Œç”¨äºç­–ç•¥ç½‘ç»œçš„å…¨å±€ä¿¡æ¯æ„ŸçŸ¥
        
        Returns:
            Dict[str, Any]: å…¨å±€ç‰¹å¾ä¿¡æ¯
        """
        # è®¡ç®—æ‰€æœ‰å¿«é€’æŸœçš„ä¸­å¿ƒç‚¹
        if not self.lockers_state:
            return {}
            
        center_x = sum(locker['location'][0] for locker in self.lockers_state) / len(self.lockers_state)
        center_y = sum(locker['location'][1] for locker in self.lockers_state) / len(self.lockers_state)
        
        # åˆ†æä¸åŒåŠå¾„ä¸‹çš„åŒºåŸŸç‰¹å¾
        regional_features = {}
        for radius in [15, 25, 35]:  # ä¸åŒè¦†ç›–åŠå¾„
            features = self._calculate_regional_density((center_x, center_y), radius)
            regional_features[f'radius_{radius}'] = features
        
        # è®¡ç®—å¿«é€’æŸœåˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦
        distances_from_center = [
            self._euclidean_distance((center_x, center_y), locker['location'])
            for locker in self.lockers_state
        ]
        spread = np.std(distances_from_center) if distances_from_center else 0
        
        # è®¡ç®—éœ€æ±‚çƒ­ç‚¹åŒºåŸŸ
        demand_hotspots = self._identify_demand_hotspots()
        
        # è®¡ç®—å¡è½¦å½“å‰ä½ç½®çš„æˆ˜ç•¥ä»·å€¼
        truck_strategic_values = []
        for truck in self.trucks:
            truck_pos = truck['position']
            strategic_value = self._calculate_strategic_value(truck_pos)
            truck_strategic_values.append(strategic_value)
        
        return {
            'center_point': (center_x, center_y),
            'distribution_spread': spread,
            'regional_features': regional_features,
            'demand_hotspots': demand_hotspots,
            'truck_strategic_values': truck_strategic_values,
            'total_unserved_demand': self._calculate_total_unserved_demand()
        }
    
    def _identify_demand_hotspots(self, grid_size: int = 5) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«éœ€æ±‚çƒ­ç‚¹åŒºåŸŸ
        
        Args:
            grid_size: ç½‘æ ¼åˆ’åˆ†å¤§å°
            
        Returns:
            List[Dict[str, Any]]: çƒ­ç‚¹åŒºåŸŸä¿¡æ¯
        """
        # è®¡ç®—è¾¹ç•Œ
        min_x = min(locker['location'][0] for locker in self.lockers_state)
        max_x = max(locker['location'][0] for locker in self.lockers_state)
        min_y = min(locker['location'][1] for locker in self.lockers_state)
        max_y = max(locker['location'][1] for locker in self.lockers_state)
        
        # åˆ›å»ºç½‘æ ¼
        x_step = (max_x - min_x) / grid_size
        y_step = (max_y - min_y) / grid_size
        
        hotspots = []
        for i in range(grid_size):
            for j in range(grid_size):
                grid_x = min_x + (i + 0.5) * x_step
                grid_y = min_y + (j + 0.5) * y_step
                
                # è®¡ç®—è¯¥ç½‘æ ¼çš„éœ€æ±‚å¯†åº¦
                grid_features = self._calculate_regional_density((grid_x, grid_y), radius=15.0)
                
                if grid_features['locker_count'] > 0:
                    hotspots.append({
                        'center': (grid_x, grid_y),
                        'locker_count': grid_features['locker_count'],
                        'demand_density': grid_features['demand_density'],
                        'service_potential': grid_features['service_potential']
                    })
        
        # æŒ‰éœ€æ±‚å¯†åº¦æ’åº
        hotspots.sort(key=lambda x: x['demand_density'], reverse=True)
        return hotspots[:3]  # è¿”å›å‰3ä¸ªçƒ­ç‚¹
    
    def _calculate_strategic_value(self, position: Tuple[float, float]) -> float:
        """
        è®¡ç®—ä½ç½®çš„æˆ˜ç•¥ä»·å€¼
        
        Args:
            position: ä½ç½®åæ ‡
            
        Returns:
            float: æˆ˜ç•¥ä»·å€¼åˆ†æ•°
        """
        # è®¡ç®—è¯¥ä½ç½®çš„åŒºåŸŸç‰¹å¾
        regional_features = self._calculate_regional_density(position, radius=25.0)
        
        # ç»¼åˆè¯„åˆ†ï¼šå¯†é›†åº¦ + éœ€æ±‚å¯†åº¦ + æœåŠ¡æ½œåŠ›
        strategic_value = (
            regional_features['locker_density'] * 0.3 +
            regional_features['demand_density'] * 0.4 +
            regional_features['service_potential'] * 0.3
        )
        
        return strategic_value
    
    def _calculate_total_unserved_demand(self) -> float:
        """
        è®¡ç®—æ€»çš„æœªæœåŠ¡éœ€æ±‚
        
        Returns:
            float: æœªæœåŠ¡éœ€æ±‚æ€»é‡
        """
        total_unserved = 0.0
        for locker in self.lockers_state:
            if not locker.get('pickup_served', False):
                total_unserved += locker.get('pickup_demand', 0)
            if not locker.get('return_served', False):
                total_unserved += locker.get('return_demand', 0)
        return total_unserved
    
    def _calculate_truck_density(self, truck_positions: List[Tuple[float, float]]) -> float:
        """
        è®¡ç®—å¡è½¦åˆ†å¸ƒå¯†åº¦
        
        Args:
            truck_positions: å¡è½¦ä½ç½®åˆ—è¡¨
            
        Returns:
            float: å¯†åº¦å€¼
        """
        if len(truck_positions) < 2:
            return 0.0
        
        total_distance = 0
        count = 0
        
        for i in range(len(truck_positions)):
            for j in range(i + 1, len(truck_positions)):
                total_distance += self._euclidean_distance(truck_positions[i], truck_positions[j])
                count += 1
        
        if count == 0:
            return 0.0
        
        avg_distance = total_distance / count
        # å¯†åº¦ä¸å¹³å‡è·ç¦»æˆåæ¯”
        return 1.0 / (1.0 + avg_distance / 100.0)
    
    def _detect_coordination_conflicts(self) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹å¡è½¦é—´çš„åè°ƒå†²çª
        
        Returns:
            List[Dict]: å†²çªåˆ—è¡¨
        """
        conflicts = []
        
        # æ£€æµ‹ç›®æ ‡å†²çªï¼šå¤šä¸ªå¡è½¦å‰å¾€åŒä¸€ä¸ªå¿«é€’æŸœ
        target_conflicts = {}
        for truck_id, truck in enumerate(self.trucks):
            if truck['current_location'] != 0:  # 0è¡¨ç¤ºä»“åº“
                # ç®€åŒ–ï¼šå‡è®¾å¡è½¦æ­£åœ¨å‰å¾€æœ€è¿‘çš„æœ‰éœ€æ±‚çš„å¿«é€’æŸœ
                nearest_locker = self._find_nearest_locker_with_demand(truck['position'])
                if nearest_locker is not None:
                    if nearest_locker not in target_conflicts:
                        target_conflicts[nearest_locker] = []
                    target_conflicts[nearest_locker].append(truck_id)
        
        for locker_id, truck_ids in target_conflicts.items():
            if len(truck_ids) > 1:
                conflicts.append({
                    'type': 'target_conflict',
                    'locker_id': locker_id,
                    'truck_ids': truck_ids,
                    'severity': len(truck_ids) / self.num_trucks
                })
        
        # æ£€æµ‹è·¯å¾„å†²çªï¼šå¡è½¦è¿‡äºæ¥è¿‘
        for i in range(len(self.trucks)):
            for j in range(i + 1, len(self.trucks)):
                truck1, truck2 = self.trucks[i], self.trucks[j]
                if truck1['current_location'] != 0 and truck2['current_location'] != 0:  # 0è¡¨ç¤ºä»“åº“
                    distance = self._euclidean_distance(truck1['position'], truck2['position'])
                    if distance < 10.0:  # é˜ˆå€¼ï¼š10å•ä½è·ç¦»
                        conflicts.append({
                            'type': 'proximity_conflict',
                            'truck_ids': [i, j],
                            'distance': distance,
                            'severity': max(0, (10.0 - distance) / 10.0)
                        })
        
        return conflicts
    
    def _calculate_global_load_distribution(self) -> Dict[str, float]:
        """
        è®¡ç®—å…¨å±€è´Ÿè½½åˆ†å¸ƒ
        
        Returns:
            Dict: è´Ÿè½½åˆ†å¸ƒç»Ÿè®¡
        """
        loads = []
        for truck in self.trucks:
            total_load = truck.get('delivery_items', 0) + truck.get('return_items', 0)
            load_ratio = total_load / self.truck_capacity if self.truck_capacity > 0 else 0
            loads.append(load_ratio)
        
        if not loads:
            return {'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'balance_score': 1.0}
        
        mean_load = np.mean(loads)
        std_load = np.std(loads)
        min_load = np.min(loads)
        max_load = np.max(loads)
        
        # å¹³è¡¡åˆ†æ•°ï¼šæ ‡å‡†å·®è¶Šå°ï¼Œå¹³è¡¡æ€§è¶Šå¥½
        balance_score = 1.0 / (1.0 + std_load)
        
        return {
            'mean': mean_load,
            'std': std_load,
            'min': min_load,
            'max': max_load,
            'balance_score': balance_score
        }
    
    def _predict_future_demand_trend(self) -> Dict[str, float]:
        """
        é¢„æµ‹æœªæ¥éœ€æ±‚è¶‹åŠ¿
        
        Returns:
            Dict: éœ€æ±‚è¶‹åŠ¿é¢„æµ‹
        """
        # ç®€åŒ–çš„è¶‹åŠ¿é¢„æµ‹ï¼šåŸºäºå½“å‰éœ€æ±‚åˆ†å¸ƒå’Œæ—¶é—´
        current_delivery_demand = sum(locker.get('delivery_demand', 0) for locker in self.lockers_state)
        current_return_demand = sum(locker.get('return_demand', 0) for locker in self.lockers_state)
        
        # åŸºäºæ—¶é—´æ­¥çš„è¶‹åŠ¿é¢„æµ‹
        time_factor = self.time_step / self.max_timesteps if self.max_timesteps > 0 else 0
        
        # å‡è®¾é€è´§éœ€æ±‚åœ¨å‰æœŸè¾ƒé«˜ï¼Œé€€è´§éœ€æ±‚åœ¨åæœŸè¾ƒé«˜
        delivery_trend = max(0, 1.0 - time_factor * 1.5)  # é€’å‡è¶‹åŠ¿
        return_trend = min(1.0, time_factor * 1.2)  # é€’å¢è¶‹åŠ¿
        
        # çƒ­ç‚¹è½¬ç§»ï¼šåŸºäºå½“å‰éœ€æ±‚åˆ†å¸ƒçš„å˜åŒ–
        hotspot_shift = abs(current_delivery_demand - current_return_demand) / max(1, current_delivery_demand + current_return_demand)
        
        return {
            'delivery_trend': delivery_trend,
            'return_trend': return_trend,
            'hotspot_shift': hotspot_shift
        }
    
    def _calculate_coordination_efficiency(self) -> float:
        """
        è®¡ç®—åè°ƒæ•ˆç‡
        
        Returns:
            float: åè°ƒæ•ˆç‡åˆ†æ•°
        """
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—åè°ƒæ•ˆç‡
        
        # 1. è´Ÿè½½å¹³è¡¡æ€§
        load_distribution = self._calculate_global_load_distribution()
        load_balance_score = load_distribution['balance_score']
        
        # 2. å†²çªç¨‹åº¦
        conflicts = self._detect_coordination_conflicts()
        conflict_penalty = sum(conflict['severity'] for conflict in conflicts) / max(1, len(conflicts))
        conflict_score = max(0, 1.0 - conflict_penalty)
        
        # 3. è¦†ç›–æ•ˆç‡
        active_trucks = sum(1 for truck in self.trucks if truck['current_location'] != 0)
        coverage_score = active_trucks / self.num_trucks if self.num_trucks > 0 else 0
        
        # ç»¼åˆè¯„åˆ†
        efficiency = 0.4 * load_balance_score + 0.3 * conflict_score + 0.3 * coverage_score
        return min(1.0, max(0.0, efficiency))
    
    def _predict_truck_completion_time(self, truck_id: int) -> float:
        """
        é¢„æµ‹å¡è½¦å®Œæˆå½“å‰ä»»åŠ¡çš„æ—¶é—´
        
        Args:
            truck_id: å¡è½¦ID
            
        Returns:
            float: é¢„æµ‹å®Œæˆæ—¶é—´
        """
        truck = self.trucks[truck_id]
        
        # å¦‚æœå¡è½¦å·²è¿”å›ä»“åº“ï¼Œå®Œæˆæ—¶é—´ä¸º0
        if truck['current_location'] == 0:
            return 0.0
        
        # è®¡ç®—è¿”å›ä»“åº“çš„è·ç¦»
        distance_to_depot = self._euclidean_distance(truck['position'], (0, 0))
        
        # ä¼°ç®—å‰©ä½™æœåŠ¡æ—¶é—´ï¼ˆåŸºäºå½“å‰è´Ÿè½½ï¼‰
        current_load = truck.get('delivery_items', 0) + truck.get('return_items', 0)
        service_time = current_load * 2  # å‡è®¾æ¯ä¸ªç‰©å“éœ€è¦2ä¸ªæ—¶é—´å•ä½
        
        # ä¼°ç®—ç§»åŠ¨æ—¶é—´ï¼ˆå‡è®¾é€Ÿåº¦ä¸º1å•ä½/æ—¶é—´æ­¥ï¼‰
        travel_time = distance_to_depot
        
        return service_time + travel_time
    
    def _calculate_path_optimization_potential(self, truck_id: int) -> float:
        """
        è®¡ç®—è·¯å¾„ä¼˜åŒ–æ½œåŠ›
        
        Args:
            truck_id: å¡è½¦ID
            
        Returns:
            float: ä¼˜åŒ–æ½œåŠ›åˆ†æ•°
        """
        truck = self.trucks[truck_id]
        current_pos = truck['position']
        
        # æ‰¾åˆ°é™„è¿‘çš„éœ€æ±‚ç‚¹
        nearby_demands = []
        for i, locker in enumerate(self.lockers_state):
            if locker.get('delivery_demand', 0) > 0 or locker.get('return_demand', 0) > 0:
                distance = self._euclidean_distance(current_pos, locker['location'])
                if distance <= 50.0:  # 50å•ä½èŒƒå›´å†…
                    nearby_demands.append((i, distance, locker.get('delivery_demand', 0) + locker.get('return_demand', 0)))
        
        if not nearby_demands:
            return 0.0
        
        # è®¡ç®—å½“å‰è·¯å¾„æ•ˆç‡
        total_demand = sum(demand for _, _, demand in nearby_demands)
        total_distance = sum(distance for _, distance, _ in nearby_demands)
        
        if total_distance == 0:
            return 0.0
        
        current_efficiency = total_demand / total_distance
        
        # è®¡ç®—ç†è®ºæœ€ä¼˜æ•ˆç‡ï¼ˆæŒ‰è·ç¦»æ’åºï¼‰
        nearby_demands.sort(key=lambda x: x[1])  # æŒ‰è·ç¦»æ’åº
        optimal_distance = sum(nearby_demands[i][1] for i in range(min(3, len(nearby_demands))))  # æœ€è¿‘3ä¸ª
        optimal_demand = sum(nearby_demands[i][2] for i in range(min(3, len(nearby_demands))))
        
        if optimal_distance == 0:
            return 0.0
        
        optimal_efficiency = optimal_demand / optimal_distance
        
        # ä¼˜åŒ–æ½œåŠ› = (æœ€ä¼˜æ•ˆç‡ - å½“å‰æ•ˆç‡) / æœ€ä¼˜æ•ˆç‡
        if optimal_efficiency == 0:
            return 0.0
        
        potential = max(0, (optimal_efficiency - current_efficiency) / optimal_efficiency)
        return min(1.0, potential)
    
    def _calculate_coordination_opportunity(self, truck_id: int) -> float:
        """
        è®¡ç®—åè°ƒæœºä¼šè¯„åˆ†
        
        Args:
            truck_id: å¡è½¦ID
            
        Returns:
            float: åè°ƒæœºä¼šåˆ†æ•°
        """
        truck = self.trucks[truck_id]
        current_pos = truck['position']
        
        # è®¡ç®—ä¸å…¶ä»–å¡è½¦çš„åè°ƒæœºä¼š
        coordination_score = 0.0
        active_trucks = 0
        
        for other_id, other_truck in enumerate(self.trucks):
            if other_id != truck_id and other_truck['position'] != 0:
                distance = self._euclidean_distance(current_pos, other_truck['position'])
                
                # å¦‚æœè·ç¦»é€‚ä¸­ï¼ˆä¸å¤ªè¿‘ä¹Ÿä¸å¤ªè¿œï¼‰ï¼Œæœ‰åè°ƒæœºä¼š
                if 20.0 <= distance <= 80.0:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒçš„æœåŠ¡åŒºåŸŸ
                    common_area_score = self._calculate_common_service_area(truck_id, other_id)
                    
                    # æ£€æŸ¥è´Ÿè½½äº’è¡¥æ€§
                    load_complementarity = self._calculate_load_complementarity(truck_id, other_id)
                    
                    truck_coordination = 0.5 * common_area_score + 0.5 * load_complementarity
                    coordination_score += truck_coordination
                    active_trucks += 1
        
        if active_trucks == 0:
            return 0.0
        
        return coordination_score / active_trucks
    
    def _find_nearest_locker_with_demand(self, position: Tuple[float, float]) -> Optional[int]:
        """
        æ‰¾åˆ°æœ€è¿‘çš„æœ‰éœ€æ±‚çš„å¿«é€’æŸœ
        
        Args:
            position: å½“å‰ä½ç½®
            
        Returns:
            Optional[int]: å¿«é€’æŸœIDï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        min_distance = float('inf')
        nearest_locker = None
        
        for i, locker in enumerate(self.lockers_state):
            if locker.get('delivery_demand', 0) > 0 or locker.get('return_demand', 0) > 0:
                distance = self._euclidean_distance(position, locker['location'])
                if distance < min_distance:
                    min_distance = distance
                    nearest_locker = i
        
        return nearest_locker
    
    def _calculate_common_service_area(self, truck1_id: int, truck2_id: int) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå¡è½¦çš„å…±åŒæœåŠ¡åŒºåŸŸé‡å åº¦
        
        Args:
            truck1_id: å¡è½¦1 ID
            truck2_id: å¡è½¦2 ID
            
        Returns:
            float: é‡å åº¦åˆ†æ•°
        """
        truck1_pos = self.trucks[truck1_id]['position']
        truck2_pos = self.trucks[truck2_id]['position']
        
        # è®¡ç®—ä¸¤ä¸ªå¡è½¦æœåŠ¡èŒƒå›´çš„é‡å 
        service_radius = 30.0  # å‡è®¾æœåŠ¡åŠå¾„ä¸º30å•ä½
        distance_between = self._euclidean_distance(truck1_pos, truck2_pos)
        
        if distance_between >= 2 * service_radius:
            return 0.0  # æ²¡æœ‰é‡å 
        
        if distance_between == 0:
            return 1.0  # å®Œå…¨é‡å 
        
        # è®¡ç®—åœ†å½¢åŒºåŸŸé‡å æ¯”ä¾‹ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        overlap_ratio = max(0, (2 * service_radius - distance_between) / (2 * service_radius))
        return overlap_ratio
    
    def _calculate_load_complementarity(self, truck1_id: int, truck2_id: int) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå¡è½¦çš„è´Ÿè½½äº’è¡¥æ€§
        
        Args:
            truck1_id: å¡è½¦1 ID
            truck2_id: å¡è½¦2 ID
            
        Returns:
            float: äº’è¡¥æ€§åˆ†æ•°
        """
        truck1 = self.trucks[truck1_id]
        truck2 = self.trucks[truck2_id]
        
        # è®¡ç®—è´Ÿè½½å·®å¼‚
        load1 = (truck1.get('delivery_items', 0) + truck1.get('return_items', 0)) / self.truck_capacity
        load2 = (truck2.get('delivery_items', 0) + truck2.get('return_items', 0)) / self.truck_capacity
        
        # äº’è¡¥æ€§ï¼šè´Ÿè½½å·®å¼‚è¶Šå¤§ï¼Œäº’è¡¥æ€§è¶Šå¼º
        load_difference = abs(load1 - load2)
        
        # è®¡ç®—ç±»å‹äº’è¡¥æ€§ï¼ˆé€è´§vsé€€è´§ï¼‰
        delivery1 = truck1.get('delivery_items', 0) / max(1, truck1.get('delivery_items', 0) + truck1.get('return_items', 0))
        delivery2 = truck2.get('delivery_items', 0) / max(1, truck2.get('delivery_items', 0) + truck2.get('return_items', 0))
        
        type_complementarity = abs(delivery1 - delivery2)
        
        # ç»¼åˆäº’è¡¥æ€§åˆ†æ•°
        complementarity = 0.6 * load_difference + 0.4 * type_complementarity
        return min(1.0, complementarity)


class RouteAwareValueNetwork(nn.Module):
    """
    è·¯çº¿è§„åˆ’æ„ŸçŸ¥çš„ä»·å€¼ç½‘ç»œ
    
    ä¸“é—¨é’ˆå¯¹è·¯çº¿è§„åˆ’ä»»åŠ¡è®¾è®¡çš„ä»·å€¼ç½‘ç»œï¼Œèƒ½å¤Ÿç†è§£ï¼š
    - è·¯å¾„æ•ˆç‡å’Œä¼˜åŒ–æ½œåŠ›
    - éœ€æ±‚æƒé‡å’Œæ—¶åºå†³ç­–
    - é•¿æœŸè·¯çº¿è§„åˆ’ä»·å€¼
    
    Author: Dionysus
    Contact: wechat:gzw1546484791
    """
    
    def __init__(self, state_dim: int):
        super(RouteAwareValueNetwork, self).__init__()
        
        # è¾“å…¥ç‰¹å¾ç¼–ç å™¨
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # è·¯å¾„æ•ˆç‡æ„ŸçŸ¥åˆ†æ”¯
        self.path_efficiency_branch = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # éœ€æ±‚æƒé‡æ„ŸçŸ¥åˆ†æ”¯
        self.demand_weight_branch = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # æ—¶åºå†³ç­–æ„ŸçŸ¥åˆ†æ”¯
        self.temporal_decision_branch = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # å…¨å±€è·¯çº¿è§„åˆ’åˆ†æ”¯
        self.global_route_branch = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Sequential(
            nn.Linear(64 * 4 + 256, 256),  # 4ä¸ªåˆ†æ”¯ + åŸå§‹ç‰¹å¾
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.05)
        )
        
        # ä»·å€¼é¢„æµ‹å¤´
        self.value_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # è·¯çº¿è§„åˆ’ä»·å€¼é¢„æµ‹å¤´ï¼ˆè¾…åŠ©ä»»åŠ¡ï¼‰
        self.route_value_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            state: è¾“å…¥çŠ¶æ€å¼ é‡
            
        Returns:
            Tuple[torch.Tensor, torch.Tensor]: (ä¸»ä»·å€¼, è·¯çº¿ä»·å€¼)
        """
        # ç¼–ç è¾“å…¥çŠ¶æ€
        encoded_state = self.state_encoder(state)
        
        # å¤šåˆ†æ”¯ç‰¹å¾æå–
        path_features = self.path_efficiency_branch(encoded_state)
        demand_features = self.demand_weight_branch(encoded_state)
        temporal_features = self.temporal_decision_branch(encoded_state)
        global_features = self.global_route_branch(encoded_state)
        
        # ç‰¹å¾èåˆ
        fused_features = torch.cat([
            encoded_state,
            path_features,
            demand_features,
            temporal_features,
            global_features
        ], dim=-1)
        
        # èåˆå¤„ç†
        processed_features = self.feature_fusion(fused_features)
        
        # ä»·å€¼é¢„æµ‹
        main_value = self.value_head(processed_features)
        route_value = self.route_value_head(processed_features)
        
        return main_value, route_value


# å¢å¼ºçš„ç­–ç•¥ç½‘ç»œ
class TruckPolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(TruckPolicyNetwork, self).__init__()
        
        # å¢å¼ºçš„çŠ¶æ€ç¼–ç å™¨ - 6å±‚æ·±åº¦ç½‘ç»œï¼Œæå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
        self.state_encoder = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šè¾“å…¥å¤„ç† - å¢åŠ å®½åº¦
            nn.Linear(state_dim, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # ç¬¬äºŒå±‚ï¼šæ·±å±‚ç‰¹å¾æå– - æ–°å¢å±‚
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # ç¬¬ä¸‰å±‚ï¼šç‰¹å¾èåˆ
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # éœ€æ±‚æ„ŸçŸ¥ç‰¹å¾æå–å™¨ï¼ˆæ–°å¢ï¼‰
        self.demand_encoder = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(96, 64),
            nn.ReLU()
        )
        
        # åœ°å›¾å…¨å±€ä¿¡æ¯ç¼–ç å™¨ï¼ˆæ–°å¢ï¼‰
        self.global_encoder = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.05),
            nn.Linear(96, 64),
            nn.ReLU()
        )
        
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ - åˆ†åˆ«å¤„ç†éœ€æ±‚å’Œåœ°å›¾ä¿¡æ¯
        self.demand_attention = nn.MultiheadAttention(
            embed_dim=64, 
            num_heads=4,
            dropout=0.05,
            batch_first=True
        )
        
        self.global_attention = nn.MultiheadAttention(
            embed_dim=64, 
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        # åŸæœ‰çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.self_attention = nn.MultiheadAttention(
            embed_dim=128, 
            num_heads=4,  # ä»8å‡å°‘åˆ°4
            dropout=0.15,  # å¢åŠ Dropoutç‡ä»0.05åˆ°0.15
            batch_first=True
        )
        
        # ç‰¹å¾èåˆå±‚ï¼ˆæ–°å¢ï¼‰
        self.feature_fusion = nn.Sequential(
            nn.Linear(128 + 64 + 64, 128),  # èåˆåŸå§‹ç‰¹å¾ã€éœ€æ±‚ç‰¹å¾ã€åœ°å›¾ç‰¹å¾
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # å¢å¼ºåœé ç‚¹é€‰æ‹©å¤´ - 3å±‚æ·±åº¦ç½‘ç»œï¼Œå¢å¼ºæ­£åˆ™åŒ–
        self.stop_head = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.2),  # å¢åŠ dropoutç‡
            nn.Linear(96, 64),
            nn.ReLU(),
            nn.Dropout(0.15), # å¢åŠ dropoutç‡
            nn.Linear(64, action_dim["select_stop"])
        )

        # å¢å¼ºæœåŠ¡åŒºåŸŸé€‰æ‹©å¤´ - 3å±‚æ·±åº¦ç½‘ç»œï¼Œå¢å¼ºæ­£åˆ™åŒ–
        self.service_head = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.2),  # å¢åŠ dropoutç‡
            nn.Linear(96, 64),
            nn.ReLU(),
            nn.Dropout(0.15), # å¢åŠ dropoutç‡
            nn.Linear(64, action_dim["service_area"])
        )
        
        # æ–°å¢ï¼šåŒºåŸŸä¼˜å…ˆçº§è®¡ç®—æ¨¡å—
        self.regional_priority_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # è¾“å‡ºå•ä¸€ä¼˜å…ˆçº§åˆ†æ•°
        )
        
        # æ–°å¢ï¼šå¯†é›†åº¦æ„ŸçŸ¥å±‚
        self.density_aware_layer = nn.Sequential(
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.Tanh()  # ä½¿ç”¨Tanhæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºèŒƒå›´[-1,1]
        )
        
        # æ–°å¢ï¼šè¦†ç›–æ•ˆç‡è¯„ä¼°å±‚
        self.coverage_efficiency_layer = nn.Sequential(
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Sigmoid()  # ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºèŒƒå›´[0,1]
        )
        
        # æ–°å¢ï¼šè·¯çº¿è§„åˆ’æ„ŸçŸ¥å±‚
        self.route_planning_layer = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(96, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32)
        )
        
        # æ–°å¢ï¼šè·¯å¾„ä¼˜åŒ–æ½œåŠ›è¯„ä¼°å±‚
        self.path_optimization_layer = nn.Sequential(
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 24),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1,1]ï¼Œè¡¨ç¤ºä¼˜åŒ–æ½œåŠ›
        )
        
        # æ–°å¢ï¼šå¤šå¡è½¦åè°ƒæ„ŸçŸ¥å±‚
        self.coordination_awareness_layer = nn.Sequential(
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 20),
            nn.Sigmoid()  # è¾“å‡ºèŒƒå›´[0,1]ï¼Œè¡¨ç¤ºåè°ƒæœºä¼š
        )
        
        # æ–°å¢ï¼šå†å²è·¯å¾„å­¦ä¹ å±‚
        self.path_history_layer = nn.Sequential(
            nn.Linear(128, 48),
            nn.LayerNorm(48),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(48, 16),
            nn.ReLU()
        )
        
        # æ–°å¢ï¼šæœªæ¥éœ€æ±‚é¢„æµ‹æ„ŸçŸ¥å±‚
        self.future_demand_layer = nn.Sequential(
            nn.Linear(128, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 12),
            nn.Softmax(dim=-1)  # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        )
        
        # æ–°å¢ï¼šè·¯çº¿è§„åˆ’ç‰¹å¾èåˆå±‚
        self.route_feature_fusion = nn.Sequential(
            nn.Linear(32 + 24 + 20 + 16 + 12, 64),  # èåˆæ‰€æœ‰è·¯çº¿è§„åˆ’ç‰¹å¾
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.ReLU()
        )

    def forward(self, state, stop_mask=None, service_mask=None):
        # çŠ¶æ€ç¼–ç 
        x = self.state_encoder(state)
        batch_size = x.size(0)
        
        # 1. éœ€æ±‚æ„ŸçŸ¥ç‰¹å¾æå–
        demand_features = self.demand_encoder(x)  # [batch_size, 64]
        demand_seq = demand_features.unsqueeze(1)  # [batch_size, 1, 64]
        
        # éœ€æ±‚æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶
        demand_attn_output, demand_attn_weights = self.demand_attention(
            demand_seq, demand_seq, demand_seq
        )
        demand_enhanced = demand_attn_output.squeeze(1)  # [batch_size, 64]
        
        # 2. åœ°å›¾å…¨å±€ä¿¡æ¯æå–
        global_features = self.global_encoder(x)  # [batch_size, 64]
        global_seq = global_features.unsqueeze(1)  # [batch_size, 1, 64]
        
        # åœ°å›¾å…¨å±€æ³¨æ„åŠ›æœºåˆ¶
        global_attn_output, global_attn_weights = self.global_attention(
            global_seq, global_seq, global_seq
        )
        global_enhanced = global_attn_output.squeeze(1)  # [batch_size, 64]
        
        # 3. åŸæœ‰çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
        x_seq = x.unsqueeze(1)  # [batch_size, 1, 128]
        self_attn_output, _ = self.self_attention(x_seq, x_seq, x_seq)
        self_enhanced = self_attn_output.squeeze(1)  # [batch_size, 128]
        
        # æ®‹å·®è¿æ¥
        self_final = x + self_enhanced
        
        # 4. ç‰¹å¾èåˆ
        # å°†éœ€æ±‚ç‰¹å¾ã€åœ°å›¾ç‰¹å¾å’Œè‡ªæ³¨æ„åŠ›ç‰¹å¾èåˆ
        fused_features = torch.cat([
            self_final,      # [batch_size, 128] - åŸå§‹+è‡ªæ³¨æ„åŠ›ç‰¹å¾
            demand_enhanced, # [batch_size, 64]  - éœ€æ±‚æ„ŸçŸ¥ç‰¹å¾
            global_enhanced  # [batch_size, 64]  - åœ°å›¾å…¨å±€ç‰¹å¾
        ], dim=1)  # [batch_size, 256]
        
        # é€šè¿‡èåˆå±‚å¤„ç†
        x_final = self.feature_fusion(fused_features)  # [batch_size, 128]
        
        # æ·»åŠ å±‚å½’ä¸€åŒ–æå‡è®­ç»ƒç¨³å®šæ€§
        x_final = F.layer_norm(x_final, x_final.shape[1:])

        # åœé ç‚¹é€‰æ‹© - åŸºç¡€logits
        stop_logits_base = self.stop_head(x_final)
        
        # è®¡ç®—åŒºåŸŸä¼˜å…ˆçº§åˆ†æ•°
        regional_priority = self.regional_priority_head(x_final)  # [batch_size, 1]
        
        # è®¡ç®—å¯†é›†åº¦æ„ŸçŸ¥ç‰¹å¾
        density_features = self.density_aware_layer(x_final)  # [batch_size, 32]
        
        # è®¡ç®—è¦†ç›–æ•ˆç‡ç‰¹å¾
        coverage_features = self.coverage_efficiency_layer(x_final)  # [batch_size, 16]
        
        # æ–°å¢ï¼šè·¯çº¿è§„åˆ’æ„ŸçŸ¥ç‰¹å¾æå–
        route_planning_features = self.route_planning_layer(x_final)  # [batch_size, 32]
        path_optimization_features = self.path_optimization_layer(x_final)  # [batch_size, 24]
        coordination_features = self.coordination_awareness_layer(x_final)  # [batch_size, 20]
        path_history_features = self.path_history_layer(x_final)  # [batch_size, 16]
        future_demand_features = self.future_demand_layer(x_final)  # [batch_size, 12]
        
        # èåˆæ‰€æœ‰è·¯çº¿è§„åˆ’ç‰¹å¾
        route_combined_features = torch.cat([
            route_planning_features,    # [batch_size, 32]
            path_optimization_features, # [batch_size, 24]
            coordination_features,      # [batch_size, 20]
            path_history_features,      # [batch_size, 16]
            future_demand_features      # [batch_size, 12]
        ], dim=1)  # [batch_size, 104]
        
        # é€šè¿‡è·¯çº¿ç‰¹å¾èåˆå±‚å¤„ç†
        route_enhanced_features = self.route_feature_fusion(route_combined_features)  # [batch_size, 32]
        
        # å¢å¼ºåœé ç‚¹é€‰æ‹©ï¼šç»“åˆåŒºåŸŸä¼˜å…ˆçº§ã€å¯†é›†åº¦ä¿¡æ¯å’Œè·¯çº¿è§„åˆ’ç‰¹å¾
        # å°†åŒºåŸŸä¼˜å…ˆçº§å¹¿æ’­åˆ°æ‰€æœ‰åœé ç‚¹é€‰æ‹©
        priority_boost = regional_priority.expand(-1, stop_logits_base.size(1))  # [batch_size, num_stops]
        
        # å¯†é›†åº¦åŠ æƒï¼šå°†å¯†é›†åº¦ç‰¹å¾è½¬æ¢ä¸ºåœé ç‚¹æƒé‡
        density_weight = torch.mean(density_features, dim=1, keepdim=True)  # [batch_size, 1]
        density_boost = density_weight.expand(-1, stop_logits_base.size(1))  # [batch_size, num_stops]
        
        # è¦†ç›–æ•ˆç‡åŠ æƒï¼šå°†è¦†ç›–æ•ˆç‡ç‰¹å¾è½¬æ¢ä¸ºåœé ç‚¹æƒé‡
        coverage_weight = torch.mean(coverage_features, dim=1, keepdim=True)  # [batch_size, 1]
        coverage_boost = coverage_weight.expand(-1, stop_logits_base.size(1))  # [batch_size, num_stops]
        
        # è·¯çº¿è§„åˆ’åŠ æƒï¼šå°†è·¯çº¿è§„åˆ’ç‰¹å¾è½¬æ¢ä¸ºåœé ç‚¹æƒé‡
        route_weight = torch.mean(route_enhanced_features, dim=1, keepdim=True)  # [batch_size, 1]
        route_boost = route_weight.expand(-1, stop_logits_base.size(1))  # [batch_size, num_stops]
        
        # ç»¼åˆåœé ç‚¹é€‰æ‹©logitsï¼šåŸºç¡€logits + å„ç§å¢å¼ºç‰¹å¾åŠ æƒ
        stop_logits = (
            stop_logits_base + 
            0.25 * priority_boost +     # 25% åŒºåŸŸä¼˜å…ˆçº§æƒé‡
            0.30 * density_boost +      # 30% å¯†é›†åº¦æƒé‡  
            0.25 * coverage_boost +     # 25% è¦†ç›–æ•ˆç‡æƒé‡
            0.20 * route_boost          # 20% è·¯çº¿è§„åˆ’æƒé‡
        )

        # æœåŠ¡åŒºåŸŸé€‰æ‹© - åŸºç¡€logits
        service_logits_base = self.service_head(x_final)
        
        # å¢å¼ºæœåŠ¡åŒºåŸŸé€‰æ‹©ï¼šç»“åˆè·¯çº¿è§„åˆ’ç‰¹å¾
        # å°†è·¯çº¿è§„åˆ’ç‰¹å¾è½¬æ¢ä¸ºæœåŠ¡åŒºåŸŸæƒé‡
        route_service_weight = torch.mean(route_enhanced_features, dim=1, keepdim=True)  # [batch_size, 1]
        route_service_boost = route_service_weight.expand(-1, service_logits_base.size(1))  # [batch_size, num_service_areas]
        
        # è·¯å¾„ä¼˜åŒ–æ½œåŠ›åŠ æƒ
        path_opt_weight = torch.mean(path_optimization_features, dim=1, keepdim=True)  # [batch_size, 1]
        path_opt_boost = path_opt_weight.expand(-1, service_logits_base.size(1))  # [batch_size, num_service_areas]
        
        # åè°ƒæ„ŸçŸ¥åŠ æƒ
        coord_weight = torch.mean(coordination_features, dim=1, keepdim=True)  # [batch_size, 1]
        coord_boost = coord_weight.expand(-1, service_logits_base.size(1))  # [batch_size, num_service_areas]
        
        # ç»¼åˆæœåŠ¡åŒºåŸŸé€‰æ‹©logitsï¼šåŸºç¡€logits + è·¯çº¿è§„åˆ’å¢å¼º
        service_logits = (
            service_logits_base +
            0.35 * route_service_boost +  # 35% è·¯çº¿è§„åˆ’æƒé‡
            0.35 * path_opt_boost +       # 35% è·¯å¾„ä¼˜åŒ–æƒé‡
            0.30 * coord_boost            # 30% åè°ƒæ„ŸçŸ¥æƒé‡
        )

        return stop_logits, service_logits


# å¤šæ™ºèƒ½ä½“PPOç®—æ³•
class MAPPO:
    def __init__(self, num_trucks, state_dim, action_dim, lr=None):
        # è®¾å¤‡æ£€æµ‹å’Œé…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ MAPPOä½¿ç”¨è®¾å¤‡: {self.device}")
        if torch.cuda.is_available():
            print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
            print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # å¦‚æœlrä¸ºNoneï¼Œä½¿ç”¨configä¸­çš„å­¦ä¹ ç‡
        if lr is None:
            lr = config.LEARNING_RATE
        
        self.policy_net = TruckPolicyNetwork(state_dim, action_dim).to(self.device)
        self.old_policy_net = TruckPolicyNetwork(state_dim, action_dim).to(self.device)
        self.old_policy_net.load_state_dict(self.policy_net.state_dict())

        # è·¯çº¿è§„åˆ’æ„ŸçŸ¥çš„ä»·å€¼ç½‘ç»œ - ä¸“é—¨é’ˆå¯¹è·¯çº¿ä¼˜åŒ–ä»»åŠ¡è®¾è®¡
        self.value_net = RouteAwareValueNetwork(state_dim).to(self.device)

        # ä¿å­˜åŸºç¡€å­¦ä¹ ç‡ç”¨äºé¢„çƒ­æœºåˆ¶ - é˜²æ­¢è¿‡å¿«æ”¶æ•›
        self.policy_base_lr = lr * 0.3  # ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡å¤§å¹…é™ä½
        self.value_base_lr = lr * 0.2   # ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡å¤§å¹…é™ä½
        
        # æ·±åº¦ç½‘ç»œä¼˜åŒ–å™¨ï¼šé’ˆå¯¹æ›´æ·±ç½‘ç»œè°ƒæ•´å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–
        self.policy_optimizer = optim.Adam(
            self.policy_net.parameters(), 
            lr=self.policy_base_lr,  # æ·±åº¦ç½‘ç»œä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
            weight_decay=5e-4,  # å¤§å¹…å¢å¼ºL2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
            eps=1e-8,
            betas=(0.9, 0.999)
        )
        self.value_optimizer = optim.Adam(
            self.value_net.parameters(), 
            lr=self.value_base_lr,  # ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡è¿›ä¸€æ­¥é™ä½
            weight_decay=3e-4,  # ä»·å€¼ç½‘ç»œå¢å¼ºæƒé‡è¡°å‡
            eps=1e-8,
            betas=(0.9, 0.999)
        )

        self.num_trucks = num_trucks
        
        # æ·±åº¦ç½‘ç»œè®­ç»ƒå‚æ•° - ä¿®å¤å…³é”®è¶…å‚æ•°
        self.max_grad_norm = 0.5  # é€‚ä¸­çš„æ¢¯åº¦è£å‰ª
        self.clip_ratio = 0.2     # æ ‡å‡†PPOè£å‰ªæ¯”ç‡ï¼Œç¡®ä¿è¶³å¤Ÿçš„ç­–ç•¥æ›´æ–°å¹…åº¦
        
        # è‡ªé€‚åº”æ¢ç´¢æœºåˆ¶ - é˜²æ­¢è¿‡å¿«æ”¶æ•›å’Œå±€éƒ¨æœ€ä¼˜
        self.initial_entropy_coef = 0.15  # æé«˜åˆå§‹ç†µç³»æ•°ï¼Œå¢å¼ºæ—©æœŸæ¢ç´¢
        self.min_entropy_coef = 0.02      # æé«˜æœ€å°ç†µç³»æ•°ï¼Œä¿æŒé•¿æœŸæ¢ç´¢
        self.entropy_decay_rate = 0.9995  # å¤§å¹…å‡ç¼“ç†µç³»æ•°è¡°å‡ç‡
        self.entropy_coef = self.initial_entropy_coef
        
        self.value_loss_coef = 0.5  # æ ‡å‡†ä»·å€¼æŸå¤±ç³»æ•°
        
        # æ¢ç´¢å¢å¼ºå‚æ•°
        self.exploration_bonus_coef = 0.08  # å¢åŠ æ¢ç´¢å¥–åŠ±ç³»æ•°
        self.action_diversity_threshold = 0.7  # é™ä½åŠ¨ä½œå¤šæ ·æ€§é˜ˆå€¼ï¼Œæ›´å®¹æ˜“è§¦å‘æ¢ç´¢å¥–åŠ±
        
        # é˜²æ­¢è¿‡æ‹Ÿåˆçš„è®­ç»ƒè®¾ç½®
        self.batch_size = 256     # å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œå¢åŠ éšæœºæ€§
        self.mini_batch_size = 64 # å‡å°‘å°æ‰¹æ¬¡å¤§å°ï¼Œå¢åŠ æ¢¯åº¦å™ªå£°
        self.update_epochs = 3    # å‡å°‘æ›´æ–°è½®æ•°ï¼Œé˜²æ­¢è¿‡åº¦æ‹Ÿåˆ
        
        # å¹¶è¡Œå¤„ç†ä¼˜åŒ–
        self.num_workers = 4      # æ•°æ®åŠ è½½å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        
        # è®­ç»ƒæ€§èƒ½è·Ÿè¸ª
        self.best_performance = 0.0  # æœ€ä½³æ€§èƒ½è®°å½•
        self.pin_memory = True    # å¯ç”¨å†…å­˜é”å®šï¼ŒåŠ é€ŸGPUä¼ è¾“
        
        # GPUå†…å­˜ä¼˜åŒ–
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ“ä½œ
            torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æ“ä½œä»¥æå‡æ€§èƒ½
        
        # æ·±åº¦ç½‘ç»œå­¦ä¹ ç‡è°ƒåº¦å™¨ - æ·»åŠ é¢„çƒ­æœºåˆ¶
        # ç­–ç•¥ç½‘ç»œï¼šé¢„çƒ­ + é˜¶æ¢¯è¡°å‡
        self.policy_warmup_steps = 500
        self.policy_scheduler = optim.lr_scheduler.StepLR(
            self.policy_optimizer, step_size=1500, gamma=0.9  # æ›´é¢‘ç¹çš„è¡°å‡
        )
        
        # ä»·å€¼ç½‘ç»œï¼šé¢„çƒ­ + é˜¶æ¢¯è¡°å‡
        self.value_warmup_steps = 300
        self.value_scheduler = optim.lr_scheduler.StepLR(
            self.value_optimizer, step_size=1200, gamma=0.95
        )
        
        # é¢„çƒ­è®¡æ•°å™¨
        self.training_step = 0
        
        # æ¢ç´¢æœºåˆ¶ï¼šè®¿é—®çŠ¶æ€è®¡æ•°å™¨
        self.visited_state_count = {}
        
        # æ—©åœæœºåˆ¶
        self.best_performance = float('-inf')
        self.patience = 50
        self.patience_counter = 0
        self.early_stop = False
        self.performance_history = []
        print("æ—©åœæœºåˆ¶å·²é‡ç½®")

    def update_hyperparameters(self, hyperparams: Dict[str, float]):
        """
        æ›´æ–°è¶…å‚æ•°
        
        Args:
            hyperparams: è¶…å‚æ•°å­—å…¸
        """
        if 'learning_rate' in hyperparams:
            # æ›´æ–°ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡
            for param_group in self.policy_optimizer.param_groups:
                param_group['lr'] = hyperparams['learning_rate']
            # æ›´æ–°ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
            for param_group in self.value_optimizer.param_groups:
                param_group['lr'] = hyperparams['learning_rate'] * 0.5
        
        if 'clip_ratio' in hyperparams:
            self.clip_ratio = hyperparams['clip_ratio']
        
        if 'entropy_coef' in hyperparams:
            self.entropy_coef = hyperparams['entropy_coef']
        
        if 'value_loss_coef' in hyperparams:
            self.value_loss_coef = hyperparams['value_loss_coef']
        
        if 'max_grad_norm' in hyperparams:
            self.max_grad_norm = hyperparams['max_grad_norm']

    def optimize_action_space(self, env, truck_id: int) -> Dict[str, Any]:
        """
        ä¼˜åŒ–å¡è½¦åŠ¨ä½œç©ºé—´è®¾è®¡ï¼Œæä¾›æ›´å¤šæ ·åŒ–çš„åœé ç‚¹é€‰æ‹©ç­–ç•¥
        
        Args:
            env: ç¯å¢ƒå®ä¾‹
            truck_id: å¡è½¦ID
            
        Returns:
            ä¼˜åŒ–åçš„åŠ¨ä½œç©ºé—´ä¿¡æ¯
        """
        truck = env.trucks[truck_id]
        truck_pos = truck['position']
        
        # 1. åŸºäºè·ç¦»çš„åˆ†å±‚é€‰æ‹©
        distance_tiers = {
            'immediate': [],  # 0-10km
            'nearby': [],     # 10-20km  
            'distant': []     # 20km+
        }
        
        for locker_id, locker in enumerate(env.lockers_state):
            if locker['demand_del'] + locker['demand_ret'] > 0:
                distance = env._euclidean_distance(truck_pos, locker['location'])
                if distance <= 10:
                    distance_tiers['immediate'].append(locker_id)
                elif distance <= 20:
                    distance_tiers['nearby'].append(locker_id)
                else:
                    distance_tiers['distant'].append(locker_id)
        
        # 2. åŸºäºéœ€æ±‚å¯†åº¦çš„èšç±»é€‰æ‹©
        demand_clusters = self._identify_demand_clusters(env, truck_pos)
        
        # 3. åŸºäºåè°ƒæœºä¼šçš„é€‰æ‹©
        coordination_opportunities = self._find_coordination_opportunities(env, truck_id)
        
        # 4. åŸºäºæ¢ç´¢ä»·å€¼çš„é€‰æ‹©
        exploration_targets = self._identify_exploration_targets(env, truck_pos)
        
        # 5. åŠ¨æ€æƒé‡åˆ†é…
        strategy_weights = self._calculate_strategy_weights()
        
        return {
            'distance_tiers': distance_tiers,
            'demand_clusters': demand_clusters,
            'coordination_opportunities': coordination_opportunities,
            'exploration_targets': exploration_targets,
            'strategy_weights': strategy_weights,
            'recommended_actions': self._generate_action_recommendations(
                distance_tiers, demand_clusters, coordination_opportunities, 
                exploration_targets, strategy_weights
            )
        }
    
    def _identify_demand_clusters(self, env, truck_pos: Tuple[float, float]) -> List[Dict]:
        """è¯†åˆ«éœ€æ±‚èšé›†åŒºåŸŸ"""
        clusters = []
        grid_size = 3
        
        for i in range(grid_size):
            for j in range(grid_size):
                # å®šä¹‰ç½‘æ ¼åŒºåŸŸ
                x_min = i * (100 / grid_size)
                x_max = (i + 1) * (100 / grid_size)
                y_min = j * (100 / grid_size)
                y_max = (j + 1) * (100 / grid_size)
                
                # è®¡ç®—åŒºåŸŸå†…çš„éœ€æ±‚å¯†åº¦
                total_demand = 0
                locker_count = 0
                lockers_in_cluster = []
                
                for locker_id, locker in enumerate(env.lockers_state):
                    locker_x, locker_y = locker['location']
                    if (x_min <= locker_x < x_max and 
                        y_min <= locker_y < y_max):
                        demand = locker['demand_del'] + locker['demand_ret']
                        if demand > 0:
                            total_demand += demand
                            locker_count += 1
                            lockers_in_cluster.append(locker_id)
                
                if locker_count > 0:
                    center = ((x_min + x_max) / 2, (y_min + y_max) / 2)
                    distance_to_truck = env._euclidean_distance(truck_pos, center)
                    
                    clusters.append({
                        'center': center,
                        'total_demand': total_demand,
                        'locker_count': locker_count,
                        'density': total_demand / locker_count,
                        'distance': distance_to_truck,
                        'lockers': lockers_in_cluster,
                        'priority': total_demand / (1 + distance_to_truck)
                    })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        clusters.sort(key=lambda x: x['priority'], reverse=True)
        return clusters[:5]  # è¿”å›å‰5ä¸ªæœ€ä¼˜èšé›†åŒºåŸŸ
    
    def _find_coordination_opportunities(self, env, truck_id: int) -> List[Dict]:
        """å¯»æ‰¾åè°ƒæœºä¼š"""
        opportunities = []
        truck = env.trucks[truck_id]
        truck_pos = truck['position']
        
        for other_truck_id, other_truck in enumerate(env.trucks):
            if other_truck_id != truck_id:
                other_pos = other_truck['position']
                distance = env._euclidean_distance(truck_pos, other_pos)
                
                # å¯»æ‰¾ä¸¤è½¦ä¹‹é—´çš„ä¸­é—´åŒºåŸŸ
                if distance < 30:  # åœ¨åè°ƒèŒƒå›´å†…
                    mid_point = (
                        (truck_pos[0] + other_pos[0]) / 2,
                        (truck_pos[1] + other_pos[1]) / 2
                    )
                    
                    # å¯»æ‰¾ä¸­é—´åŒºåŸŸçš„éœ€æ±‚ç‚¹
                    nearby_lockers = []
                    for locker_id, locker in enumerate(env.lockers_state):
                        locker_pos = locker['location']
                        if (env._euclidean_distance(mid_point, locker_pos) < 15 and
                            locker['demand_del'] + locker['demand_ret'] > 0):
                            nearby_lockers.append(locker_id)
                    
                    if nearby_lockers:
                        opportunities.append({
                            'partner_truck': other_truck_id,
                            'coordination_point': mid_point,
                            'distance_to_partner': distance,
                            'shared_lockers': nearby_lockers,
                            'coordination_value': len(nearby_lockers) / (1 + distance)
                        })
        
        opportunities.sort(key=lambda x: x['coordination_value'], reverse=True)
        return opportunities[:3]  # è¿”å›å‰3ä¸ªåè°ƒæœºä¼š
    
    def _identify_exploration_targets(self, env, truck_pos: Tuple[float, float]) -> List[Dict]:
        """è¯†åˆ«æ¢ç´¢ç›®æ ‡"""
        exploration_targets = []
        
        for locker_id, locker in enumerate(env.lockers_state):
            visit_count = self.visited_state_count.get(locker_id, 0)
            if visit_count < 3:  # è®¿é—®æ¬¡æ•°å°‘çš„ä½ç½®
                locker_pos = locker['location']
                distance = env._euclidean_distance(truck_pos, locker_pos)
                demand = locker['demand_del'] + locker['demand_ret']
                
                # æ¢ç´¢ä»·å€¼ = éœ€æ±‚æ½œåŠ› / (1 + è®¿é—®æ¬¡æ•°) / (1 + è·ç¦»)
                exploration_value = (demand + 1) / (1 + visit_count) / (1 + distance / 10)
                
                exploration_targets.append({
                    'locker_id': locker_id,
                    'position': locker_pos,
                    'distance': distance,
                    'visit_count': visit_count,
                    'exploration_value': exploration_value
                })
        
        exploration_targets.sort(key=lambda x: x['exploration_value'], reverse=True)
        return exploration_targets[:5]  # è¿”å›å‰5ä¸ªæ¢ç´¢ç›®æ ‡
    
    def _calculate_strategy_weights(self) -> Dict[str, float]:
        """è®¡ç®—ç­–ç•¥æƒé‡"""
        # æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥æƒé‡
        progress = min(self.training_step / 10000, 1.0)
        
        return {
            'distance_priority': 0.3 + 0.2 * progress,      # è·ç¦»ä¼˜å…ˆçº§éšè®­ç»ƒå¢åŠ 
            'demand_priority': 0.4 - 0.1 * progress,        # éœ€æ±‚ä¼˜å…ˆçº§éšè®­ç»ƒå‡å°‘
            'coordination_priority': 0.1 + 0.2 * progress,  # åè°ƒä¼˜å…ˆçº§éšè®­ç»ƒå¢åŠ 
            'exploration_priority': 0.2 - 0.1 * progress    # æ¢ç´¢ä¼˜å…ˆçº§éšè®­ç»ƒå‡å°‘
        }
    
    def _generate_action_recommendations(self, distance_tiers, demand_clusters, 
                                       coordination_opportunities, exploration_targets, 
                                       strategy_weights) -> List[Dict]:
        """ç”ŸæˆåŠ¨ä½œæ¨è"""
        recommendations = []
        
        # åŸºäºè·ç¦»çš„æ¨è
        for tier_name, lockers in distance_tiers.items():
            if lockers:
                weight = strategy_weights['distance_priority']
                if tier_name == 'immediate':
                    weight *= 1.5
                elif tier_name == 'nearby':
                    weight *= 1.0
                else:
                    weight *= 0.5
                
                recommendations.append({
                    'type': 'distance_based',
                    'tier': tier_name,
                    'lockers': lockers[:3],  # æœ€å¤šæ¨è3ä¸ª
                    'weight': weight,
                    'reason': f'åŸºäº{tier_name}è·ç¦»çš„é€‰æ‹©'
                })
        
        # åŸºäºéœ€æ±‚èšé›†çš„æ¨è
        for cluster in demand_clusters[:2]:  # å‰2ä¸ªèšé›†åŒºåŸŸ
            recommendations.append({
                'type': 'demand_cluster',
                'lockers': cluster['lockers'][:2],
                'weight': strategy_weights['demand_priority'] * cluster['priority'],
                'reason': f'éœ€æ±‚èšé›†åŒºåŸŸï¼Œå¯†åº¦: {cluster["density"]:.1f}'
            })
        
        # åŸºäºåè°ƒçš„æ¨è
        for opportunity in coordination_opportunities[:1]:  # æœ€ä½³åè°ƒæœºä¼š
            recommendations.append({
                'type': 'coordination',
                'lockers': opportunity['shared_lockers'][:2],
                'weight': strategy_weights['coordination_priority'] * opportunity['coordination_value'],
                'reason': f'ä¸å¡è½¦{opportunity["partner_truck"]}åè°ƒæœºä¼š'
            })
        
        # åŸºäºæ¢ç´¢çš„æ¨è
        for target in exploration_targets[:2]:  # å‰2ä¸ªæ¢ç´¢ç›®æ ‡
            recommendations.append({
                'type': 'exploration',
                'lockers': [target['locker_id']],
                'weight': strategy_weights['exploration_priority'] * target['exploration_value'],
                'reason': f'æ¢ç´¢ç›®æ ‡ï¼Œè®¿é—®æ¬¡æ•°: {target["visit_count"]}'
            })
        
        # æŒ‰æƒé‡æ’åº
        recommendations.sort(key=lambda x: x['weight'], reverse=True)
        return recommendations[:5]  # è¿”å›å‰5ä¸ªæ¨è

    def act(self, states, action_masks, env=None):
        """
        æ™ºèƒ½ä½“åŠ¨ä½œé€‰æ‹© - é›†æˆåŒºåŸŸä¼˜å…ˆçº§è®¡ç®—
        
        Args:
            states: çŠ¶æ€åˆ—è¡¨
            action_masks: åŠ¨ä½œæ©ç åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯åŒ…å«stop_maskå’Œservice_maskçš„å­—å…¸ï¼‰
            env: ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºè·å–å…¨å±€ç‰¹å¾ä¿¡æ¯
            
        Returns:
            actions: é€‰æ‹©çš„åŠ¨ä½œåˆ—è¡¨
            log_probs: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡åˆ—è¡¨
            values: çŠ¶æ€ä»·å€¼åˆ—è¡¨
        """
        actions = []
        log_probs = []
        values = []
        
        with torch.no_grad():
            for i, (state, action_mask) in enumerate(zip(states, action_masks)):
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                # è·å–åŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€ä»·å€¼
                stop_logits, service_logits = self.policy_net(state_tensor)
                main_value, route_value = self.value_net(state_tensor)
                value = main_value  # ä½¿ç”¨ä¸»ä»·å€¼ä½œä¸ºçŠ¶æ€ä»·å€¼
                
                # å¢å¼ºæ¢ç´¢æœºåˆ¶ï¼Œå‡å°‘å¯¹é¢„è®¾ç®—æ³•çš„ä¾èµ–
                exploration_bonus_weight = max(0.05, 0.3 - self.training_step / 10000)  # æ¢ç´¢å¥–åŠ±æƒé‡
                temperature = max(1.0, 2.0 - self.training_step / 5000)  # æ¸©åº¦å‚æ•°ï¼Œå¢åŠ éšæœºæ€§
                
                # æ¢ç´¢å¥–åŠ±ï¼šé¼“åŠ±è®¿é—®è¾ƒå°‘è®¿é—®çš„çŠ¶æ€
                exploration_bonus = torch.zeros_like(stop_logits)
                for stop_idx in range(stop_logits.shape[-1]):
                    visit_count = self.visited_state_count.get(stop_idx, 0)
                    bonus = exploration_bonus_weight / (1 + visit_count)
                    exploration_bonus[0][stop_idx] = bonus
                
                # åº”ç”¨æ¢ç´¢å¥–åŠ±
                stop_logits = stop_logits + exploration_bonus
                
                # ä»…åœ¨è®­ç»ƒæ—©æœŸæä¾›æè½»å¾®çš„éœ€æ±‚å¼•å¯¼ï¼ˆæƒé‡æ›´å°ï¼Œæ—¶é—´æ›´çŸ­ï¼‰
                if env is not None and self.training_step < 500:  # å‡å°‘åˆ°å‰500æ­¥
                    guidance_weight = max(0.02 * (1.0 - self.training_step / 500), 0.0)  # æ›´å°çš„å¼•å¯¼æƒé‡
                    
                    # ç®€å•çš„éœ€æ±‚å¯†åº¦å¼•å¯¼
                    basic_guidance = []
                    for stop_idx in range(stop_logits.shape[-1]):
                        if stop_idx < len(env.lockers_state):
                            locker = env.lockers_state[stop_idx]
                            demand_score = (locker.get('demand_del', 0) + locker.get('demand_ret', 0)) / 20.0
                            basic_guidance.append(min(demand_score, 0.5))  # è¿›ä¸€æ­¥é™åˆ¶å½±å“
                        else:
                            basic_guidance.append(0.0)
                    
                    if guidance_weight > 0:
                        guidance_tensor = torch.FloatTensor(basic_guidance).unsqueeze(0).to(self.device)
                        stop_logits = stop_logits + guidance_weight * guidance_tensor
                
                # å¢å¼ºçš„è‡ªé€‚åº”æ¢ç´¢å™ªå£°
                base_noise_scale = 0.25  # å¢åŠ åŸºç¡€å™ªå£°
                adaptive_noise_scale = base_noise_scale * max(0.5, self.entropy_coef / self.initial_entropy_coef)
                
                # ä¸ºæ¯ä¸ªå¡è½¦æ·»åŠ ä¸åŒçš„éšæœºå™ªå£°
                truck_specific_noise = torch.randn_like(stop_logits) * adaptive_noise_scale * (i + 1) / len(states)
                
                # åº”ç”¨æ¸©åº¦ç¼©æ”¾å¢å¼ºæ¢ç´¢
                stop_logits = (stop_logits + truck_specific_noise) / temperature
                
                # åº”ç”¨åœé ç‚¹åŠ¨ä½œæ©ç 
                if action_mask is not None and 'stop_mask' in action_mask:
                    mask_tensor = action_mask['stop_mask'].unsqueeze(0).float().to(self.device)
                    # å°†æ©ç åº”ç”¨åˆ°logitsä¸Šï¼Œæ— æ•ˆåŠ¨ä½œè®¾ä¸ºå¾ˆå°çš„å€¼
                    masked_stop_logits = stop_logits + (mask_tensor - 1.0) * 1e9
                else:
                    masked_stop_logits = stop_logits
                
                # é€‰æ‹©åœé ç‚¹åŠ¨ä½œ
                stop_probs = F.softmax(masked_stop_logits, dim=-1)
                stop_dist = Categorical(stop_probs)
                select_stop = stop_dist.sample()
                stop_log_prob = stop_dist.log_prob(select_stop)
                
                # æ›´æ–°è®¿é—®è®¡æ•°
                stop_id = select_stop.item()
                self.visited_state_count[stop_id] = self.visited_state_count.get(stop_id, 0) + 1
                
                # åº”ç”¨æœåŠ¡åŒºåŸŸåŠ¨ä½œæ©ç å¹¶é€‰æ‹©æœåŠ¡åŒºåŸŸ
                if action_mask is not None and 'service_mask' in action_mask:
                    service_mask_tensor = action_mask['service_mask'].unsqueeze(0).float().to(self.device)
                    masked_service_logits = service_logits + (service_mask_tensor - 1.0) * 1e9
                else:
                    masked_service_logits = service_logits
                
                # ä½¿ç”¨Bernoulliåˆ†å¸ƒä¸ºæ¯ä¸ªå¿«é€’æŸœé€‰æ‹©æ˜¯å¦æœåŠ¡
                service_probs = torch.sigmoid(masked_service_logits)
                service_dist = Bernoulli(service_probs)
                service_area_tensor = service_dist.sample()
                service_log_prob = service_dist.log_prob(service_area_tensor).sum()
                
                # æ„å»ºå¤åˆåŠ¨ä½œ
                action = {
                    'select_stop': select_stop.item(),
                    'service_area': service_area_tensor.squeeze(0).cpu().numpy().astype(int).tolist()
                }
                
                # è®¡ç®—æ€»çš„å¯¹æ•°æ¦‚ç‡
                total_log_prob = stop_log_prob + service_log_prob
                
                actions.append(action)
                log_probs.append(total_log_prob.item())
                values.append(value.item())
        
        return actions, log_probs, values

    def update(self, states, actions, rewards, log_probs, values, dones, optimized_config=None):
        """
        æ›´æ–°ç­–ç•¥ç½‘ç»œ
        
        Args:
            states: çŠ¶æ€åºåˆ—
            actions: åŠ¨ä½œåºåˆ—
            rewards: å¥–åŠ±åºåˆ—
            log_probs: å¯¹æ•°æ¦‚ç‡åºåˆ—
            values: ä»·å€¼åºåˆ—
            dones: ç»“æŸæ ‡å¿—åºåˆ—
            optimized_config: ä¼˜åŒ–é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰¹æ¬¡å¤§å°å’Œæ›´æ–°é¢‘ç‡è®¾ç½®
            
        Returns:
            dict: åŒ…å«ç­–ç•¥æŸå¤±å’Œä»·å€¼æŸå¤±çš„å­—å…¸
        """
        # æ›´æ–°è®­ç»ƒæ­¥æ•°
        self.training_step += 1
        
        # åº”ç”¨é¢„çƒ­æœºåˆ¶
        self._apply_warmup()
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œç›®æ ‡ä»·å€¼
        advantages, returns = self._compute_advantages(rewards, values, dones)
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        if isinstance(states, torch.Tensor):
            states = states.to(self.device)
        else:
            states = torch.FloatTensor(states).to(self.device)
            
        if isinstance(actions, torch.Tensor):
            actions = actions.to(self.device)
        else:
            actions = torch.LongTensor(actions).to(self.device)
            
        if isinstance(log_probs, torch.Tensor):
            old_log_probs = log_probs.to(self.device)
        else:
            old_log_probs = torch.FloatTensor(log_probs).to(self.device)
            
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
        if len(advantages) > 1:
            advantages_mean = advantages.mean()
            advantages_std = advantages.std() + 1e-8
            advantages = (advantages - advantages_mean) / advantages_std
        
        # æ ‡å‡†åŒ–å›æŠ¥å€¼ä»¥æé«˜ä»·å€¼ç½‘ç»œè®­ç»ƒç¨³å®šæ€§
        if len(returns) > 1:
            returns_mean = returns.mean()
            returns_std = returns.std() + 1e-8
            normalized_returns = (returns - returns_mean) / returns_std
        else:
            normalized_returns = returns
            returns_mean = 0
            returns_std = 1
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        total_policy_loss = 0
        total_value_loss = 0
        
        # æ•°æ®é›†å¤§å°
        dataset_size = states.size(0)
        
        # ä½¿ç”¨ä¼˜åŒ–é…ç½®çš„æ‰¹æ¬¡å¤§å°å’Œæ›´æ–°é¢‘ç‡
        if optimized_config is not None:
            mini_batch_size = optimized_config.BATCH_SIZE
            update_epochs = optimized_config.UPDATE_FREQUENCY
            print(f"ğŸ”§ ä½¿ç”¨ä¼˜åŒ–æ‰¹æ¬¡é…ç½® - æ‰¹æ¬¡å¤§å°: {mini_batch_size}, æ›´æ–°é¢‘ç‡: {update_epochs}")
        else:
            mini_batch_size = self.mini_batch_size
            update_epochs = self.update_epochs
        
        # å¤šè½®æ›´æ–° - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†æé«˜è®­ç»ƒç¨³å®šæ€§
        for update_round in range(update_epochs):  # PPOæ›´æ–°è½®æ•°
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = torch.randperm(dataset_size)
            
            for start_idx in range(0, dataset_size, mini_batch_size):
                end_idx = min(start_idx + mini_batch_size, dataset_size)
                batch_indices = indices[start_idx:end_idx]
                
                # è·å–å°æ‰¹æ¬¡æ•°æ®
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = normalized_returns[batch_indices]
                # å‰å‘ä¼ æ’­
                stop_logits, service_logits = self.policy_net(batch_states)
                
                # å¤„ç†å¤åˆåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡è®¡ç®—
                # å‡è®¾batch_actionsæ˜¯å¤åˆåŠ¨ä½œçš„å­—å…¸æˆ–è€…å·²ç»è½¬æ¢ä¸ºstopåŠ¨ä½œçš„ç´¢å¼•
                if isinstance(batch_actions[0], dict) if len(batch_actions) > 0 else False:
                    # å¦‚æœæ˜¯å¤åˆåŠ¨ä½œå­—å…¸ï¼Œæå–stopåŠ¨ä½œ
                    stop_actions = torch.LongTensor([action['select_stop'] for action in batch_actions]).to(self.device)
                else:
                    # å¦‚æœå·²ç»æ˜¯stopåŠ¨ä½œç´¢å¼•
                    stop_actions = batch_actions
                
                # è®¡ç®—stopåŠ¨ä½œçš„æ¦‚ç‡å’Œç†µ
                stop_probs = F.softmax(stop_logits, dim=-1)
                stop_dist = Categorical(stop_probs)
                stop_log_probs = stop_dist.log_prob(stop_actions)
                stop_entropy = stop_dist.entropy().mean()
                
                # è®¡ç®—serviceåŠ¨ä½œçš„æ¦‚ç‡å’Œç†µï¼ˆä½¿ç”¨ä¼¯åŠªåˆ©åˆ†å¸ƒï¼‰
                service_probs = torch.sigmoid(service_logits)
                service_dist = Bernoulli(service_probs)
                service_entropy = service_dist.entropy().mean()
                
                # ç»„åˆå¯¹æ•°æ¦‚ç‡å’Œç†µ
                new_log_probs = stop_log_probs  # ä¸»è¦ä½¿ç”¨stopåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
                entropy = stop_entropy + 0.1 * service_entropy  # ç»„åˆç†µï¼Œç»™serviceè¾ƒå°æƒé‡
                
                # è·å–ä»·å€¼ç½‘ç»œçš„åŒè¾“å‡ºï¼šä¸»ä»·å€¼å’Œè·¯çº¿ä»·å€¼
                main_values, route_values = self.value_net(batch_states)
                new_values = main_values.squeeze()
                route_values = route_values.squeeze()
            
                # è®¡ç®—æ¯”ç‡ï¼Œæ·»åŠ æ•°å€¼ç¨³å®šæ€§
                log_ratio = new_log_probs - batch_old_log_probs
                log_ratio = torch.clamp(log_ratio, -20, 20)  # é˜²æ­¢æ•°å€¼æº¢å‡º
                ratio = torch.exp(log_ratio)
                
                # è®¡ç®—ç­–ç•¥æŸå¤±
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # è®¡ç®—ä»·å€¼æŸå¤±ï¼Œä½¿ç”¨æ ‡å‡†åŒ–çš„å›æŠ¥å€¼å’Œæ”¹è¿›çš„æŸå¤±å‡½æ•°
                # ä¸»ä»·å€¼æŸå¤±
                main_value_loss = F.smooth_l1_loss(new_values, batch_returns)
                
                # è·¯çº¿è§„åˆ’ä»·å€¼æŸå¤± - ä½¿ç”¨è·¯çº¿è§„åˆ’ç›¸å…³çš„å¥–åŠ±ä¿¡å·
                # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸»ä»·å€¼ä½œä¸ºè·¯çº¿ä»·å€¼çš„ç›®æ ‡ï¼Œä½†å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                route_value_loss = F.smooth_l1_loss(route_values, batch_returns * 0.8)  # è·¯çº¿ä»·å€¼ç¨å¾®ä¿å®ˆ
                
                # æ·»åŠ è·¯çº¿è§„åˆ’å¥–åŠ±é¡¹
                route_planning_bonus = self._calculate_route_planning_bonus(batch_states, batch_actions, batch_returns)
                
                # æ·»åŠ é•¿æœŸä»·å€¼ä¼°è®¡å¥–åŠ±
                long_term_value_bonus = self._calculate_long_term_value_bonus(new_values, route_values, batch_returns)
                
                # æ·»åŠ åè°ƒå¥–åŠ±é¡¹
                coordination_bonus = self._calculate_coordination_bonus(batch_states, batch_actions)
                
                # æ·»åŠ è·¯å¾„æ•ˆç‡å¥–åŠ±
                path_efficiency_bonus = self._calculate_path_efficiency_bonus(batch_states, batch_returns)
                
                # ç»„åˆä»·å€¼æŸå¤±ï¼ŒåŒ…å«æ‰€æœ‰å¥–åŠ±é¡¹
                normalized_value_loss = (main_value_loss + 0.3 * route_value_loss 
                                       - 0.1 * route_planning_bonus 
                                       - 0.15 * long_term_value_bonus
                                       - 0.05 * coordination_bonus
                                       - 0.08 * path_efficiency_bonus)
                
                # è®¡ç®—æ¢ç´¢å¥–åŠ± - é¼“åŠ±åŠ¨ä½œå¤šæ ·æ€§
                exploration_bonus = 0.0
                if entropy > self.action_diversity_threshold:
                    exploration_bonus = self.exploration_bonus_coef * (entropy - self.action_diversity_threshold)
                
                # ç­–ç•¥æŸå¤±è®¡ç®— - åŒ…å«æ¢ç´¢å¥–åŠ±
                policy_total_loss = policy_loss - self.entropy_coef * entropy - exploration_bonus
                
                # ä»·å€¼æŸå¤±è®¡ç®—ï¼ˆä½¿ç”¨æ›´å°çš„ç³»æ•°ï¼‰
                value_total_loss = self.value_loss_coef * normalized_value_loss
                
                # åˆ†åˆ«è¿›è¡Œåå‘ä¼ æ’­
                # 1. ç­–ç•¥ç½‘ç»œæ›´æ–°
                self.policy_optimizer.zero_grad()
                policy_total_loss.backward(retain_graph=True)
                torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.max_grad_norm)
                self.policy_optimizer.step()
                
                # 2. ä»·å€¼ç½‘ç»œæ›´æ–°
                self.value_optimizer.zero_grad()
                value_total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), self.max_grad_norm * 0.5)  # ä»·å€¼ç½‘ç»œä½¿ç”¨æ›´å°çš„æ¢¯åº¦è£å‰ª
                self.value_optimizer.step()
                
                # ç´¯ç§¯æŸå¤±ç”¨äºè®°å½•
                total_policy_loss += policy_loss.item()
                total_value_loss += normalized_value_loss.item()
        
        # æ›´æ–°æ—§ç­–ç•¥
        self.old_policy_net.load_state_dict(self.policy_net.state_dict())
        
        # è®¡ç®—æ€»çš„å°æ‰¹æ¬¡æ•°é‡
        total_mini_batches = self.update_epochs * ((dataset_size + self.mini_batch_size - 1) // self.mini_batch_size)
        
        # é¢„çƒ­æœŸç»“æŸåï¼Œä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.training_step > max(self.policy_warmup_steps, self.value_warmup_steps):
            self.policy_scheduler.step()
            self.value_scheduler.step()
        
        # è‡ªé€‚åº”ç†µç³»æ•°è¡°å‡ - é˜²æ­¢è¿‡æ—©æ”¶æ•›
        if self.training_step % 10 == 0:  # æ¯10æ­¥æ›´æ–°ä¸€æ¬¡
            self.entropy_coef = max(
                self.min_entropy_coef,
                self.entropy_coef * self.entropy_decay_rate
            )
        
        # è¿”å›å¹³å‡æŸå¤±å’Œæ¢ç´¢ä¿¡æ¯
        return {
            'policy_loss': total_policy_loss / total_mini_batches,
            'value_loss': total_value_loss / total_mini_batches,
            'entropy_coef': self.entropy_coef,
            'exploration_level': entropy.item() if 'entropy' in locals() else 0.0
        }

    def _apply_warmup(self):
        """
        åº”ç”¨å­¦ä¹ ç‡é¢„çƒ­æœºåˆ¶
        åœ¨è®­ç»ƒåˆæœŸé€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œå¸®åŠ©æ·±åº¦ç½‘ç»œç¨³å®šè®­ç»ƒ
        """
        # ç­–ç•¥ç½‘ç»œé¢„çƒ­
        if self.training_step <= self.policy_warmup_steps:
            warmup_factor = self.training_step / self.policy_warmup_steps
            current_lr = self.policy_base_lr * warmup_factor
            for param_group in self.policy_optimizer.param_groups:
                param_group['lr'] = current_lr
        
        # ä»·å€¼ç½‘ç»œé¢„çƒ­
        if self.training_step <= self.value_warmup_steps:
            warmup_factor = self.training_step / self.value_warmup_steps
            current_lr = self.value_base_lr * warmup_factor
            for param_group in self.value_optimizer.param_groups:
                param_group['lr'] = current_lr

    def _compute_advantages(self, rewards, values, dones, gamma=None, gae_lambda=None):
        """
        è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
        
        Args:
            rewards: å¥–åŠ±åºåˆ—
            values: ä»·å€¼åºåˆ—
            dones: ç»“æŸæ ‡å¿—åºåˆ—
            gamma: æŠ˜æ‰£å› å­ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨config.GAMMAï¼‰
            gae_lambda: GAEå‚æ•°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨config.GAE_LAMBDAï¼‰
            
        Returns:
            advantages: ä¼˜åŠ¿å‡½æ•°
            returns: å›æŠ¥
        """
        # ä½¿ç”¨configä¸­çš„å€¼ä½œä¸ºé»˜è®¤å€¼
        if gamma is None:
            gamma = config.GAMMA
        if gae_lambda is None:
            gae_lambda = config.GAE_LAMBDA
        advantages = []
        returns = []
        gae = torch.tensor(0.0, device=self.device)
        
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = torch.tensor(0.0, device=self.device)
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + gamma * next_value * (1 - dones[i]) - values[i]
            gae = delta + gamma * gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])
        
        return advantages, returns

    def _calculate_route_planning_bonus(self, batch_states: torch.Tensor, batch_actions: torch.Tensor, batch_returns: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è·¯çº¿è§„åˆ’å¥–åŠ±é¡¹
        
        Args:
            batch_states: æ‰¹æ¬¡çŠ¶æ€
            batch_actions: æ‰¹æ¬¡åŠ¨ä½œ
            batch_returns: æ‰¹æ¬¡å›æŠ¥
            
        Returns:
            è·¯çº¿è§„åˆ’å¥–åŠ±
        """
        try:
            # æå–çŠ¶æ€ä¸­çš„è·¯çº¿è§„åˆ’ç›¸å…³ç‰¹å¾
            # å‡è®¾çŠ¶æ€çš„æœ€å36ç»´æ˜¯è·¯çº¿è§„åˆ’ç‰¹å¾ (12+8+6+10)
            route_features = batch_states[:, -36:]
            
            # è·¯å¾„æ•ˆç‡å¥–åŠ± (å‰12ç»´)
            path_efficiency_features = route_features[:, :12]
            path_efficiency_score = torch.mean(path_efficiency_features, dim=1)
            
            # å†å²è·¯å¾„å¥–åŠ± (13-20ç»´)
            path_history_features = route_features[:, 12:20]
            path_history_score = torch.mean(path_history_features, dim=1)
            
            # æœªæ¥éœ€æ±‚é¢„æµ‹å¥–åŠ± (21-26ç»´)
            future_demand_features = route_features[:, 20:26]
            future_demand_score = torch.mean(future_demand_features, dim=1)
            
            # åè°ƒç‰¹å¾å¥–åŠ± (27-36ç»´)
            coordination_features = route_features[:, 26:36]
            coordination_score = torch.mean(coordination_features, dim=1)
            
            # ç»„åˆè·¯çº¿è§„åˆ’å¥–åŠ±
            route_planning_bonus = (0.4 * path_efficiency_score + 
                                  0.2 * path_history_score + 
                                  0.2 * future_demand_score + 
                                  0.2 * coordination_score)
            
            # å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´
            route_planning_bonus = torch.tanh(route_planning_bonus)
            
            return route_planning_bonus.mean()
            
        except Exception as e:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›é›¶å¥–åŠ±
            return torch.tensor(0.0, device=self.device)

    def _calculate_long_term_value_bonus(self, main_values: torch.Tensor, route_values: torch.Tensor, batch_returns: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—é•¿æœŸä»·å€¼ä¼°è®¡å¥–åŠ±
        
        Args:
            main_values: ä¸»ä»·å€¼ä¼°è®¡
            route_values: è·¯çº¿ä»·å€¼ä¼°è®¡
            batch_returns: æ‰¹æ¬¡å›æŠ¥
            
        Returns:
            é•¿æœŸä»·å€¼ä¼°è®¡å¥–åŠ±
        """
        try:
            # è®¡ç®—ä»·å€¼ä¼°è®¡çš„ä¸€è‡´æ€§
            value_consistency = 1.0 - torch.abs(main_values - route_values).mean()
            
            # è®¡ç®—ä»·å€¼ä¼°è®¡çš„å‡†ç¡®æ€§
            main_accuracy = 1.0 - torch.abs(main_values - batch_returns).mean() / (torch.abs(batch_returns).mean() + 1e-8)
            route_accuracy = 1.0 - torch.abs(route_values - batch_returns * 0.8).mean() / (torch.abs(batch_returns).mean() + 1e-8)
            
            # ç»„åˆé•¿æœŸä»·å€¼å¥–åŠ±
            long_term_bonus = 0.4 * value_consistency + 0.3 * main_accuracy + 0.3 * route_accuracy
            
            # å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´
            long_term_bonus = torch.tanh(long_term_bonus)
            
            return long_term_bonus
            
        except Exception as e:
            return torch.tensor(0.0, device=self.device)

    def _calculate_coordination_bonus(self, batch_states: torch.Tensor, batch_actions: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—åè°ƒå¥–åŠ±é¡¹
        
        Args:
            batch_states: æ‰¹æ¬¡çŠ¶æ€
            batch_actions: æ‰¹æ¬¡åŠ¨ä½œ
            
        Returns:
            åè°ƒå¥–åŠ±
        """
        try:
            # æå–åè°ƒç›¸å…³ç‰¹å¾ (çŠ¶æ€çš„æœ€å10ç»´)
            coordination_features = batch_states[:, -10:]
            
            # è®¡ç®—åè°ƒæ•ˆç‡
            coordination_efficiency = torch.mean(coordination_features[:, :5], dim=1)  # å‰5ç»´ï¼šåè°ƒæ•ˆç‡æŒ‡æ ‡
            
            # è®¡ç®—å›¢é˜Ÿåˆä½œæŒ‡æ ‡
            team_cooperation = torch.mean(coordination_features[:, 5:], dim=1)  # å5ç»´ï¼šå›¢é˜Ÿåˆä½œæŒ‡æ ‡
            
            # ç»„åˆåè°ƒå¥–åŠ±
            coordination_bonus = 0.6 * coordination_efficiency + 0.4 * team_cooperation
            
            # å½’ä¸€åŒ–
            coordination_bonus = torch.tanh(coordination_bonus)
            
            return coordination_bonus.mean()
            
        except Exception as e:
            return torch.tensor(0.0, device=self.device)

    def _calculate_path_efficiency_bonus(self, batch_states: torch.Tensor, batch_returns: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è·¯å¾„æ•ˆç‡å¥–åŠ±é¡¹
        
        Args:
            batch_states: æ‰¹æ¬¡çŠ¶æ€
            batch_returns: æ‰¹æ¬¡å›æŠ¥
            
        Returns:
            è·¯å¾„æ•ˆç‡å¥–åŠ±
        """
        try:
            # æå–è·¯å¾„æ•ˆç‡ç›¸å…³ç‰¹å¾ (çŠ¶æ€çš„-36åˆ°-24ç»´ï¼Œå³è·¯çº¿è§„åˆ’ç‰¹å¾çš„å‰12ç»´)
            path_efficiency_features = batch_states[:, -36:-24]
            
            # è·ç¦»æ•ˆç‡ (å‰4ç»´)
            distance_efficiency = torch.mean(path_efficiency_features[:, :4], dim=1)
            
            # æ—¶é—´æ•ˆç‡ (5-8ç»´)
            time_efficiency = torch.mean(path_efficiency_features[:, 4:8], dim=1)
            
            # è´Ÿè½½æ•ˆç‡ (9-12ç»´)
            load_efficiency = torch.mean(path_efficiency_features[:, 8:12], dim=1)
            
            # ç»„åˆè·¯å¾„æ•ˆç‡å¥–åŠ±
            path_efficiency_bonus = (0.4 * distance_efficiency + 
                                   0.3 * time_efficiency + 
                                   0.3 * load_efficiency)
            
            # æ ¹æ®å›æŠ¥è°ƒæ•´å¥–åŠ±å¼ºåº¦
            return_magnitude = torch.abs(batch_returns).mean()
            adjusted_bonus = path_efficiency_bonus * torch.tanh(return_magnitude)
            
            # å½’ä¸€åŒ–
            adjusted_bonus = torch.tanh(adjusted_bonus)
            
            return adjusted_bonus.mean()
            
        except Exception as e:
            return torch.tensor(0.0, device=self.device)


def validate_model(mappo, validation_env, num_validation_episodes=5):
    """
    éªŒè¯æ¨¡å‹æ€§èƒ½ï¼Œæ£€æµ‹è¿‡æ‹Ÿåˆ
    
    Args:
        mappo: è®­ç»ƒçš„MAPPOæ¨¡å‹
        validation_env: éªŒè¯ç¯å¢ƒ
        num_validation_episodes: éªŒè¯è½®æ•°
    
    Returns:
        å¹³å‡éªŒè¯å¥–åŠ±
    """
    mappo.policy_net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    mappo.value_net.eval()
    
    validation_rewards = []
    
    for _ in range(num_validation_episodes):
        state, action_mask = validation_env.reset()  # æ­£ç¡®è§£åŒ…resetè¿”å›çš„å…ƒç»„
        episode_reward = 0
        done = False
        
        while not done:
            # è·å–åŠ¨ä½œæ©ç 
            action_masks = validation_env.get_action_masks()
            
            # è·å–æ¯ä¸ªå¡è½¦çš„ç‰¹å®šçŠ¶æ€
            truck_states = validation_env.get_truck_specific_states()
            
            # è·å–åŠ¨ä½œï¼ˆä¸æ·»åŠ æ¢ç´¢å™ªå£°ï¼Œä¼ é€’ç¯å¢ƒå®ä¾‹ä»¥å¯ç”¨åŒºåŸŸä¼˜å…ˆçº§è®¡ç®—ï¼‰
            with torch.no_grad():
                actions, _, _ = mappo.act(truck_states, action_masks, validation_env)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = validation_env.step(actions)
            # å¦‚æœrewardæ˜¯åˆ—è¡¨ï¼Œæ±‚å’Œï¼›å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(reward, list):
                episode_reward += sum(reward)
            else:
                episode_reward += reward
            state = next_state
        
        validation_rewards.append(episode_reward)
    
    mappo.policy_net.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
    mappo.value_net.train()
    
    return np.mean(validation_rewards)

def train_marl(env, num_episodes=200000, training_manager=None, curriculum_manager=None, 
               optimized_config=None, reward_normalizer=None, lr_scheduler=None):
    """
    å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒå‡½æ•°
    
    å‚æ•°:
    - env: ç¯å¢ƒå®ä¾‹
    - num_episodes: è®­ç»ƒè½®æ•°
    - training_manager: è®­ç»ƒç®¡ç†å™¨ï¼Œç”¨äºè®°å½•è®­ç»ƒè¿›åº¦
    - curriculum_manager: è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤çš„
    - optimized_config: ä¼˜åŒ–çš„è®­ç»ƒé…ç½®
    - reward_normalizer: å¥–åŠ±å½’ä¸€åŒ–å™¨
    - lr_scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    è¿”å›:
    - è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œ
    """
    # å¦‚æœæ²¡æœ‰ä¼ å…¥è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼Œåˆ™ä¸ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ï¼Œä¿æŒåŸå§‹ç¯å¢ƒé…ç½®
    if curriculum_manager is None:
        print("ğŸ“ ä½¿ç”¨åŸå§‹ç¯å¢ƒé…ç½®ï¼Œä¸åº”ç”¨è¯¾ç¨‹å­¦ä¹ ")
        print(f"   å¿«é€’æŸœæ•°é‡: {env.num_lockers}")
        print(f"   å¡è½¦æ•°é‡: {env.num_trucks}")
    else:
        # ä½¿ç”¨ä¼ å…¥çš„è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨é…ç½®
        initial_curriculum_config = curriculum_manager.get_current_config()
        print(f"ğŸ“ åº”ç”¨è¯¾ç¨‹å­¦ä¹ é…ç½®: {curriculum_manager.current_stage.name}")
        print(f"   å¿«é€’æŸœæ•°é‡: {initial_curriculum_config['num_lockers']}")
        print(f"   å¡è½¦æ•°é‡: {initial_curriculum_config['num_trucks']}")
        
        # æ›´æ–°ç¯å¢ƒé…ç½®
        if hasattr(env, 'update_curriculum_config'):
            env.update_curriculum_config(initial_curriculum_config)
    
    # ä½¿ç”¨æ›´æ–°åçš„ç¯å¢ƒé…ç½®åˆ›å»ºMAPPOå®ä¾‹
    num_trucks = env.num_trucks
    # è·å–å•ä¸ªå¡è½¦çš„çŠ¶æ€ç»´åº¦ï¼ˆè€Œä¸æ˜¯æ‰€æœ‰å¡è½¦çš„æ€»çŠ¶æ€ç»´åº¦ï¼‰
    dummy_state, _ = env.reset()
    truck_states = env.get_truck_specific_states()
    state_dim = len(truck_states[0]) if truck_states else env.state_dim  # å•ä¸ªå¡è½¦çš„çŠ¶æ€ç»´åº¦

    action_dim = {
        "select_stop": env.num_lockers + 1,  # 0:ä»“åº“, 1-n:å¿«é€’æŸœ
        "service_area": env.num_lockers  # æ¯ä¸ªå¿«é€’æŸœä¸€ä¸ªäºŒè¿›åˆ¶é€‰æ‹©
    }
    
    print(f"ğŸ¤– åˆ›å»ºMAPPOå®ä¾‹: å¡è½¦æ•°é‡={num_trucks}, çŠ¶æ€ç»´åº¦={state_dim}")
    
    # ä½¿ç”¨ä¼˜åŒ–é…ç½®åˆ›å»ºMAPPOæ™ºèƒ½ä½“
    if optimized_config is not None:
        print(f"ğŸ“Š ä½¿ç”¨ä¼˜åŒ–è®­ç»ƒé…ç½®: å­¦ä¹ ç‡={optimized_config.LEARNING_RATE}, è£å‰ªèŒƒå›´={optimized_config.CLIP_RANGE}")
        mappo = MAPPO(num_trucks, state_dim, action_dim, lr=optimized_config.LEARNING_RATE)
        # åº”ç”¨ä¼˜åŒ–çš„è¶…å‚æ•°
        mappo.update_hyperparameters({
            'clip_ratio': optimized_config.CLIP_RANGE,
            'value_coef': optimized_config.VF_COEF,
            'entropy_coef': optimized_config.ENT_COEF,
            'max_grad_norm': optimized_config.MAX_GRAD_NORM
        })
    else:
        # ä½¿ç”¨configä¸­çš„å­¦ä¹ ç‡
        import config
        mappo = MAPPO(num_trucks, state_dim, action_dim, lr=config.LEARNING_RATE)
    
    # åˆå§‹åŒ–è‡ªé€‚åº”å¥–åŠ±è°ƒåº¦å™¨
    reward_function = RewardFunction(max_timesteps=env.max_timesteps)
    adaptive_scheduler = AdaptiveRewardScheduler(reward_function)
    print(f"ğŸ¯ è‡ªé€‚åº”å¥–åŠ±è°ƒåº¦å™¨å·²åˆå§‹åŒ–")
    
    # åˆ›å»ºéªŒè¯ç¯å¢ƒï¼ˆç”¨äºæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰
    validation_env = TruckSchedulingEnv(verbose=False)
    if curriculum_manager is not None:
        validation_env.update_curriculum_config(curriculum_manager.get_current_config())
    else:
        # ä½¿ç”¨ä¸è®­ç»ƒç¯å¢ƒç›¸åŒçš„é…ç½®
        validation_env.num_lockers = env.num_lockers
        validation_env.num_trucks = env.num_trucks
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    validation_rewards = []
    best_reward = float('-inf')
    best_validation_reward = float('-inf')
    best_episode = 0
    
    # æ€§èƒ½ç›‘æ§
    performance_window = 50  # ä¿®æ”¹ä¸º50ä¸ªepisodeçš„çª—å£
    recent_rewards = []
    validation_frequency = 50  # æ¯50ä¸ªepisodeè¿›è¡Œä¸€æ¬¡éªŒè¯
    
    # æ—©åœæœºåˆ¶å‚æ•°ï¼ˆä¿®æ”¹ä¸º50ä¸ªepisodeä¸è¶…è¿‡æœ€ä½³å°±åœæ­¢ï¼‰
    early_stop_patience = 50  # ä¿®æ”¹ä¸º50ä¸ªepisodeæ²¡æœ‰æ”¹å–„å°±åœæ­¢
    early_stop_min_delta = 0.0  # è®¾ç½®ä¸º0ï¼Œåªè¦ä¸è¶…è¿‡æœ€ä½³å°±åœæ­¢
    validation_patience = 15    # éªŒè¯é›†æ€§èƒ½ä¸‹é™å®¹å¿åº¦
    no_improvement_count = 0
    validation_decline_count = 0
    best_avg_reward = float('-inf')
    
    # æ¨¡å‹ä¿å­˜è·¯å¾„
    best_model_path = "best_model.pth"
    
    # åˆå§‹åŒ–æŸå¤±ä¿¡æ¯
    loss_info = {'policy_loss': 0.0, 'value_loss': 0.0}
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    if curriculum_manager is not None:
        print(f"è½®æ•°: {num_episodes} | é˜¶æ®µ: {curriculum_manager.current_stage.name} | æ­¥æ•°: {env.max_timesteps}")
    else:
        print(f"è½®æ•°: {num_episodes} | åŸå§‹ç¯å¢ƒé…ç½® | æ­¥æ•°: {env.max_timesteps}")
    print("=" * 50)

    # è®­ç»ƒå¼€å§‹æ—¶é—´
    training_start_time = time.time()
    
    # åˆ›å»ºtqdmè¿›åº¦æ¡ï¼Œä¼˜åŒ–æ˜¾ç¤ºæ ¼å¼
    progress_bar = tqdm(range(num_episodes), desc="ğŸš€ MAPPOè®­ç»ƒ", ncols=120, 
                       bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
                       dynamic_ncols=True, leave=True)
    
    for episode in progress_bar:
        # è¿›åº¦æŠ¥å‘Šï¼ˆæ¯200è½®ï¼‰
        if episode % 200 == 0 and episode > 0:
            elapsed_time = time.time() - training_start_time
            avg_reward = np.mean(recent_rewards) if recent_rewards else 0
            progress_percent = (episode / num_episodes) * 100
            
            # print(f"\nğŸ“Š è®­ç»ƒè¿›åº¦æŠ¥å‘Š - Episode {episode}/{num_episodes} ({progress_percent:.1f}%)")
            # print(f"   â±ï¸  å·²ç”¨æ—¶é—´: {elapsed_time:.1f}ç§’ ({elapsed_time/60:.1f}åˆ†é’Ÿ)")
            # print(f"   ğŸ¯ å¹³å‡å¥–åŠ±: {avg_reward:.2f} | ğŸ† æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
            # print(f"   ğŸ“š è¯¾ç¨‹é˜¶æ®µ: {curriculum_manager.current_stage.name}")
            # print(f"   ğŸ“ˆ é˜¶æ®µè¿›åº¦: {curriculum_manager.episodes_in_stage}/{curriculum_manager.current_stage.episodes_required}")
            
            # # é¢„ä¼°å‰©ä½™æ—¶é—´
            # if episode > 0:
            #     avg_time_per_episode = elapsed_time / episode
            #     remaining_episodes = num_episodes - episode
            #     estimated_remaining = avg_time_per_episode * remaining_episodes
            #     print(f"   â³ é¢„è®¡å‰©ä½™: {estimated_remaining/60:.1f}åˆ†é’Ÿ")
                
            # # æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
            # if len(recent_rewards) >= 100:
            #     recent_100_avg = np.mean(recent_rewards[-100:])
            #     if recent_100_avg > avg_reward * 1.1:
            #         print(f"   âœ… æ€§èƒ½æå‡ä¸­ (æœ€è¿‘100è½®: {recent_100_avg:.2f})")
            #     elif recent_100_avg < avg_reward * 0.9:
            #         print(f"   âš ï¸  æ€§èƒ½ä¸‹é™ (æœ€è¿‘100è½®: {recent_100_avg:.2f})")
            #     else:
            #         print(f"   ğŸ“ˆ æ€§èƒ½ç¨³å®š (æœ€è¿‘100è½®: {recent_100_avg:.2f})")
            # print("-" * 60)
        # è·å–å½“å‰è¯¾ç¨‹é…ç½®ï¼ˆå¦‚æœå¯ç”¨è¯¾ç¨‹å­¦ä¹ ï¼‰
        if curriculum_manager is not None:
            curriculum_config = curriculum_manager.get_current_config()
            
            # è®°å½•æ›´æ–°å‰çš„ç¯å¢ƒé…ç½®
            old_num_trucks = env.num_trucks
            old_state_dim = env.state_dim
            
            # æ›´æ–°ç¯å¢ƒé…ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if hasattr(env, 'update_curriculum_config'):
                env.update_curriculum_config(curriculum_config)
            
            # è·å–å½“å‰å•ä¸ªå¡è½¦çš„çŠ¶æ€ç»´åº¦
            current_truck_states = env.get_truck_specific_states()
            current_single_truck_state_dim = len(current_truck_states[0]) if current_truck_states else env.state_dim
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ›å»ºMAPPOå®ä¾‹
            if env.num_trucks != old_num_trucks or current_single_truck_state_dim != state_dim:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ç¯å¢ƒé…ç½®å˜åŒ–: å¡è½¦æ•°é‡ {old_num_trucks} -> {env.num_trucks}, å•å¡è½¦çŠ¶æ€ç»´åº¦ {state_dim} -> {current_single_truck_state_dim}")
                print(f"   é‡æ–°åˆ›å»ºMAPPOå®ä¾‹ä»¥é€‚åº”æ–°é…ç½®...")
                
                # æ›´æ–°ç›¸å…³å˜é‡
                num_trucks = env.num_trucks
                state_dim = current_single_truck_state_dim
                action_dim = {
                    "select_stop": env.num_lockers + 1,  # 0:ä»“åº“, 1-n:å¿«é€’æŸœ
                    "service_area": env.num_lockers  # æ¯ä¸ªå¿«é€’æŸœä¸€ä¸ªäºŒè¿›åˆ¶é€‰æ‹©
                }
                
                # é‡æ–°åˆ›å»ºMAPPOæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨configä¸­çš„å­¦ä¹ ç‡ï¼‰
                import config
                mappo = MAPPO(num_trucks, state_dim, action_dim, lr=config.LEARNING_RATE)
                print(f"   âœ… MAPPOå®ä¾‹é‡æ–°åˆ›å»ºå®Œæˆ")
            
            # è·å–è‡ªé€‚åº”è¶…å‚æ•°
            hyperparams = curriculum_manager.get_adaptive_hyperparameters()
            mappo.update_hyperparameters(hyperparams)
        
        state, action_mask = env.reset()
        episode_reward = 0
        done = False
        step_count = 0
        
        # åˆå§‹åŒ–å¥–åŠ±åˆ†è§£ç´¯è®¡
        episode_breakdown = {
            "service_reward": 0.0,
            "efficiency_reward": 0.0,
            "cost_penalty": 0.0,
            # total_reward å·²ç”± episode_reward å˜é‡è·Ÿè¸ª
        }
        
        # å­˜å‚¨è½¨è¿¹æ•°æ®
        trajectory = {
            'states': [],
            'actions': [],
            'rewards': [],
            'log_probs': [],
            'values': [],
            'action_masks': []
        }

        while not done and step_count < env.max_timesteps:
            # è·å–æ‰€æœ‰å¡è½¦çš„åŠ¨ä½œæ©ç 
            action_masks = env.get_action_masks()
            
            # è·å–æ¯ä¸ªå¡è½¦çš„ç‰¹å®šçŠ¶æ€
            truck_states = env.get_truck_specific_states()
            
            # è·å–åŠ¨ä½œï¼ˆå¸¦æ©ç ï¼Œä¼ é€’ç¯å¢ƒå®ä¾‹ä»¥å¯ç”¨åŒºåŸŸä¼˜å…ˆçº§è®¡ç®—ï¼‰
            actions, log_probs, values = mappo.act(
                truck_states,
                action_masks,
                env  # ä¼ é€’ç¯å¢ƒå®ä¾‹ä»¥å¯ç”¨åŒºåŸŸä¼˜å…ˆçº§è®¡ç®—
            )

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, rewards, done, next_action_mask = env.step(actions)
            episode_reward += sum(rewards)
            
            # ç´¯ç§¯å¥–åŠ±åˆ†è§£
            if hasattr(env, 'last_reward_breakdown') and env.last_reward_breakdown:
                for bd in env.last_reward_breakdown:
                    for key in episode_breakdown:
                        if key in bd:
                            episode_breakdown[key] += bd[key]
            
            # å­˜å‚¨è½¨è¿¹æ•°æ®ï¼ˆä½¿ç”¨å¡è½¦ç‰¹å®šçŠ¶æ€ï¼‰
            trajectory['states'].append(truck_states)
            trajectory['actions'].append(actions)
            trajectory['rewards'].append(rewards)
            trajectory['log_probs'].append(log_probs)
            trajectory['values'].append(values)
            trajectory['action_masks'].append(action_masks)

            # æ›´æ–°çŠ¶æ€å’Œæ©ç 
            state = next_state
            action_mask = next_action_mask
            step_count += 1

        # åº”ç”¨å¥–åŠ±å½’ä¸€åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if reward_normalizer is not None and len(trajectory['states']) > 0:
            # æ”¶é›†æ‰€æœ‰å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–
            all_episode_rewards = []
            for t in range(len(trajectory['rewards'])):
                all_episode_rewards.extend(trajectory['rewards'][t])
            
            # æ›´æ–°å½’ä¸€åŒ–å™¨å¹¶åº”ç”¨å½’ä¸€åŒ–
            reward_normalizer.update(all_episode_rewards)
            
            # å½’ä¸€åŒ–è½¨è¿¹ä¸­çš„å¥–åŠ±
            for t in range(len(trajectory['rewards'])):
                normalized_rewards = reward_normalizer.normalize(trajectory['rewards'][t])
                trajectory['rewards'][t] = normalized_rewards
        
        # è½¨è¿¹ç»“æŸåè¿›è¡Œç­–ç•¥æ›´æ–°
        if len(trajectory['states']) > 0:
            # ä½¿ç”¨å®Œæ•´GAEç®—æ³•å¤„ç†æ•´ä¸ªè½¨è¿¹
            T = len(trajectory['rewards'])
            
            # å‡†å¤‡æ•°æ®
            rewards_tensor = []
            values_tensor = []
            next_values_tensor = []
            dones_list = []
            
            for t in range(T):
                rewards_tensor.append(torch.tensor(trajectory['rewards'][t], dtype=torch.float32, device=mappo.device))
                values_tensor.append(torch.tensor(trajectory['values'][t], dtype=torch.float32, device=mappo.device))
                
                # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼
                if t < T - 1:
                    next_values_tensor.append(torch.tensor(trajectory['values'][t + 1], dtype=torch.float32, device=mappo.device))
                else:
                    # æœ€åä¸€æ­¥ï¼Œè®¡ç®—ç»ˆç«¯çŠ¶æ€ä»·å€¼
                    final_state_tensor = torch.FloatTensor(state).unsqueeze(0).to(mappo.device)
                    final_value = mappo.value_net(final_state_tensor).squeeze() if not done else 0.0
                    next_values_tensor.append(torch.tensor([final_value] * num_trucks, dtype=torch.float32, device=mappo.device))
                
                dones_list.append(done if t == T - 1 else False)
            
            # ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
            advantages, returns = mappo._compute_advantages(
                rewards_tensor, values_tensor, dones_list
            )
            
            # å‡†å¤‡æ•°æ®ç”¨äºMAPPOæ‰¹é‡æ›´æ–°
            all_states = []
            all_actions = []
            all_rewards = []
            all_log_probs = []
            all_values = []
            all_dones = []
            
            for t in range(T):
                for i in range(num_trucks):
                    all_states.append(trajectory['states'][t][i])
                    all_actions.append(trajectory['actions'][t][i])
                    all_rewards.append(trajectory['rewards'][t][i])
                    all_log_probs.append(trajectory['log_probs'][t][i])
                    all_values.append(trajectory['values'][t][i])
                    all_dones.append(True if t == T - 1 else False)
            
            # è½¬æ¢ä¸ºå¼ é‡ - å…ˆè½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥æé«˜æ€§èƒ½
            states_array = np.array(all_states)
            states_tensor = torch.FloatTensor(states_array).to(mappo.device)
            # å¯¹äºå¤åˆåŠ¨ä½œï¼Œæˆ‘ä»¬åªæå–select_stopéƒ¨åˆ†ç”¨äºè®­ç»ƒ
            if isinstance(all_actions[0], dict):
                actions_tensor = torch.LongTensor([action['select_stop'] for action in all_actions]).to(mappo.device)
            else:
                actions_tensor = torch.LongTensor(all_actions).to(mappo.device)
            rewards_tensor = torch.FloatTensor(all_rewards).to(mappo.device)
            log_probs_tensor = torch.FloatTensor(all_log_probs).to(mappo.device)
            values_tensor = torch.FloatTensor(all_values).to(mappo.device)
            
            # ä½¿ç”¨ä¼˜åŒ–é…ç½®çš„æ‰¹æ¬¡å¤§å°è¿›è¡Œæ›´æ–°
            if optimized_config is not None:
                # ä½¿ç”¨ä¼˜åŒ–é…ç½®çš„æ‰¹æ¬¡å¤§å°å’Œæ›´æ–°é¢‘ç‡
                batch_size = optimized_config.BATCH_SIZE
                n_epochs = optimized_config.N_EPOCHS
                
                # åˆ†æ‰¹æ¬¡æ›´æ–°ä»¥æé«˜ç¨³å®šæ€§
                total_samples = len(all_states)
                indices = torch.randperm(total_samples)
                
                for epoch in range(n_epochs):
                    for start_idx in range(0, total_samples, batch_size):
                        end_idx = min(start_idx + batch_size, total_samples)
                        batch_indices = indices[start_idx:end_idx]
                        
                        batch_states = states_tensor[batch_indices]
                        batch_actions = actions_tensor[batch_indices]
                        batch_rewards = rewards_tensor[batch_indices]
                        batch_log_probs = log_probs_tensor[batch_indices]
                        batch_values = values_tensor[batch_indices]
                        batch_dones = [all_dones[i] for i in batch_indices]
                        
                        # æ‰§è¡Œæ‰¹æ¬¡æ›´æ–°
                        loss_info = mappo.update(
                            batch_states, batch_actions, batch_rewards, 
                            batch_log_probs, batch_values, batch_dones, optimized_config
                        )
            else:
                # ä½¿ç”¨åŸå§‹çš„æ›´æ–°æ–¹æ³•
                loss_info = mappo.update(
                    states_tensor, actions_tensor, rewards_tensor, 
                    log_probs_tensor, values_tensor, all_dones, optimized_config
                )
        
        # æ›´æ–°æ—§ç­–ç•¥ç½‘ç»œï¼ˆæ¯éš”ä¸€å®šæ­¥æ•°ï¼‰
        if episode % 10 == 0:
            mappo.old_policy_net.load_state_dict(mappo.policy_net.state_dict())
        
        # åº”ç”¨å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¦‚æœå¯ç”¨ä¼˜åŒ–é…ç½®ï¼‰
        if lr_scheduler is not None and optimized_config is not None:
            # ä½¿ç”¨ä¼˜åŒ–é…ç½®çš„å­¦ä¹ ç‡è°ƒåº¦
            if episode % optimized_config.LR_SCHEDULE['step_size'] == 0 and episode > 0:
                new_lr = lr_scheduler.step(episode)
                # æ›´æ–°MAPPOçš„å­¦ä¹ ç‡
                for param_group in mappo.policy_optimizer.param_groups:
                    param_group['lr'] = new_lr
                for param_group in mappo.value_optimizer.param_groups:
                    param_group['lr'] = new_lr
                print(f"ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦æ›´æ–° - Episode {episode}, æ–°å­¦ä¹ ç‡: {new_lr:.6f}")
        else:
            # ä½¿ç”¨åŸå§‹çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ¯2000ä¸ªepisodeï¼‰
            if episode % 2000 == 0 and episode > 0:
                if hasattr(mappo, 'policy_scheduler') and hasattr(mappo, 'value_scheduler'):
                    mappo.policy_scheduler.step()
                    mappo.value_scheduler.step()
                    current_policy_lr = mappo.policy_optimizer.param_groups[0]['lr']
                    current_value_lr = mappo.value_optimizer.param_groups[0]['lr']
                    print(f"ğŸ“‰ å­¦ä¹ ç‡æ›´æ–° - ç­–ç•¥: {current_policy_lr:.6f}, ä»·å€¼: {current_value_lr:.6f}")
        
        # å¥–åŠ±å¹³æ»‘å¤„ç†ï¼ˆå¦‚æœè®­ç»ƒç®¡ç†å™¨å¯ç”¨äº†ä¼˜åŒ–åŠŸèƒ½ï¼‰
        smoothed_reward = episode_reward
        if training_manager and hasattr(training_manager, 'reward_smoother') and training_manager.reward_smoother:
            smoothed_reward = training_manager.reward_smoother.smooth(episode_reward)
        
        # è®°å½•å¥–åŠ±å’Œæ€§èƒ½ç›‘æ§
        episode_rewards.append(episode_reward)
        recent_rewards.append(smoothed_reward)  # ä½¿ç”¨å¹³æ»‘åçš„å¥–åŠ±è®¡ç®—å¹³å‡å€¼
        if len(recent_rewards) > performance_window:
            recent_rewards.pop(0)
        
        # æ›´æ–°æœ€ä½³å¥–åŠ±
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_episode = episode
            # åŒæ—¶æ›´æ–°MAPPOå®ä¾‹çš„æœ€ä½³æ€§èƒ½è®°å½•
            mappo.best_performance = best_reward
        
        # è®¡ç®—å½“å‰æ€§èƒ½æŒ‡æ ‡
        current_avg_reward = np.mean(recent_rewards) if recent_rewards else 0
        episode_success = episode_reward > 0  # ç®€å•çš„æˆåŠŸåˆ¤æ–­æ ‡å‡†
        
        # æ”¶æ•›æ£€æµ‹ï¼ˆå¦‚æœè®­ç»ƒç®¡ç†å™¨å¯ç”¨äº†ä¼˜åŒ–åŠŸèƒ½ï¼‰
        convergence_info = None
        if training_manager and hasattr(training_manager, 'convergence_detector') and training_manager.convergence_detector:
            convergence_info = training_manager.convergence_detector.check_convergence(smoothed_reward)
            
            # æ ¹æ®æ”¶æ•›çŠ¶æ€è¿›è¡Œç›¸åº”å¤„ç†
            status = convergence_info['status']
            if status in ['converged', 'converging_with_improvement']:
                print(f"\nğŸ¯ æ”¶æ•›æ£€æµ‹: {convergence_info['message']}")
                print(f"   ç½®ä¿¡åº¦: {convergence_info['confidence']:.3f}")
                
                # å¦‚æœæ˜¯å±€éƒ¨æœ€ä¼˜ï¼Œå¢åŠ æ¢ç´¢
                if status == 'local_optimum':
                    if hasattr(training_manager, 'exploration_scheduler'):
                        current_params = training_manager.exploration_scheduler.update(episode, 0.5)  # å¢åŠ æ–¹å·®
                        print(f"   ğŸ” å¢åŠ æ¢ç´¢ç‡: {current_params['epsilon']:.3f}")
                
                # å¦‚æœæ”¶æ•›ä¸”æœ‰æ”¹è¿›ï¼Œå¯ä»¥é™ä½å­¦ä¹ ç‡ä»¥ç¨³å®šè®­ç»ƒ
                elif status == 'converging_with_improvement':
                    if hasattr(training_manager, 'lr_scheduler'):
                        new_lr = training_manager.lr_scheduler.update(current_avg_reward, episode)
                        # æ›´æ–°MAPPOçš„å­¦ä¹ ç‡
                        for param_group in mappo.policy_optimizer.param_groups:
                            param_group['lr'] = new_lr
                        for param_group in mappo.value_optimizer.param_groups:
                            param_group['lr'] = new_lr
                        print(f"   ğŸ“‰ è°ƒæ•´å­¦ä¹ ç‡: {new_lr:.2e}")
        
        # æ—©åœæœºåˆ¶ï¼šæ£€æŸ¥æ€§èƒ½æ”¹å–„
        if len(recent_rewards) >= performance_window:  # åªæœ‰åœ¨æœ‰è¶³å¤Ÿæ•°æ®æ—¶æ‰è¿›è¡Œæ—©åœæ£€æŸ¥
            if current_avg_reward > best_avg_reward + early_stop_min_delta:
                best_avg_reward = current_avg_reward
                no_improvement_count = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({
                    'policy_net_state_dict': mappo.policy_net.state_dict(),
                    'value_net_state_dict': mappo.value_net.state_dict(),
                    'policy_optimizer_state_dict': mappo.policy_optimizer.state_dict(),
                    'value_optimizer_state_dict': mappo.value_optimizer.state_dict(),
                    'episode': episode,
                    'best_avg_reward': best_avg_reward,
                    'episode_reward': episode_reward
                }, best_model_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ - Episode {episode}, å¹³å‡å¥–åŠ±: {best_avg_reward:.2f}")
                
                # ç®€åŒ–çš„è‡ªé€‚åº”å¥–åŠ±è°ƒåº¦å™¨çŠ¶æ€æŠ¥å‘Šï¼ˆä»…åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹æ—¶ï¼‰
                stability_metrics = adaptive_scheduler.get_stability_metrics()
            else:
                no_improvement_count += 1
                
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
            if no_improvement_count >= early_stop_patience:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {early_stop_patience} ä¸ªepisodeæ²¡æœ‰æ”¹å–„")
                print(f"   æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.2f}")
                print(f"   å½“å‰å¹³å‡å¥–åŠ±: {current_avg_reward:.2f}")
                print(f"   è®­ç»ƒåœ¨ Episode {episode} åœæ­¢")
                break
        
        # æ›´æ–°è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if curriculum_manager is not None:
            curriculum_manager.update_performance(episode_reward, episode_success)
        
        # æ›´æ–°è‡ªé€‚åº”å¥–åŠ±è°ƒåº¦å™¨
        completion_rate = env._calculate_completion_rate()
        efficiency = env._calculate_path_efficiency()
        episode_performance = {
            'total_reward': episode_reward,
            'completion_rate': completion_rate,
            'efficiency': efficiency,
            'step_count': step_count,
            'episode_success': episode_success
        }
        adaptive_scheduler.update_weights(episode_performance)
        
        # è®°å½•è®­ç»ƒè¿›åº¦åˆ°è®­ç»ƒç®¡ç†å™¨
        if training_manager is not None:
            metrics = {
                'episode_success': episode_success,
                'current_avg_reward': current_avg_reward,
                'best_reward': best_reward,
                'step_count': step_count,
                'completion_rate': completion_rate,
                'efficiency': efficiency,
                'smoothed_reward': smoothed_reward,  # æ·»åŠ å¹³æ»‘å¥–åŠ±
                'raw_reward': episode_reward,  # åŸå§‹å¥–åŠ±
            }
            
            # æ·»åŠ å¥–åŠ±åˆ†è§£ä¿¡æ¯
            if 'episode_breakdown' in locals():
                metrics.update({
                    'reward_service': episode_breakdown['service_reward'],
                    'reward_efficiency': episode_breakdown['efficiency_reward'],
                    'reward_cost': episode_breakdown['cost_penalty']
                })
            
            # æ·»åŠ æ”¶æ•›ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if convergence_info:
                metrics.update({
                    'convergence_status': convergence_info['status'],
                    'convergence_message': convergence_info['message'],
                    'convergence_confidence': convergence_info['confidence']
                })
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'statistics' in convergence_info:
                    stats = convergence_info['statistics']
                    metrics.update({
                        'convergence_mean_reward': stats.get('mean_reward', 0),
                        'convergence_std_reward': stats.get('std_reward', 0),
                        'convergence_cv': stats.get('coefficient_of_variation', 0),
                        'convergence_improvement_rate': stats.get('improvement_rate', 0)
                    })
            
            # æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€ä¿¡æ¯
            if hasattr(training_manager, 'lr_scheduler') and training_manager.lr_scheduler:
                metrics['current_learning_rate'] = training_manager.lr_scheduler.current_lr
            
            if hasattr(training_manager, 'exploration_scheduler') and training_manager.exploration_scheduler:
                exploration_params = training_manager.exploration_scheduler.get_current_params()
                metrics.update({
                    'exploration_epsilon': exploration_params.get('epsilon', 0),
                    'exploration_entropy': exploration_params.get('entropy_coef', 0)
                })

            if 'loss_info' in locals() and loss_info:
                metrics.update({
                    'policy_loss': loss_info.get('policy_loss', 0.0),
                    'value_loss': loss_info.get('value_loss', 0.0)
                })
            
            training_manager.log_training_progress(episode, episode_reward, metrics)
        
        # æ¯è½®éƒ½æ›´æ–°è¿›åº¦æ¡ï¼Œä¿è¯ç»ˆç«¯åªæ˜¾ç¤ºä¸€è¡Œ
        # è®¡ç®—è®­ç»ƒé€Ÿåº¦ï¼ˆepisodes per secondï¼‰
        current_time = time.time()
        elapsed = current_time - training_start_time
        eps_per_sec = (episode + 1) / elapsed if elapsed > 0 else 0
        
        # æ›´æ–°è¿›åº¦æ¡æè¿°ï¼Œæ˜¾ç¤ºå½“å‰å¥–åŠ±å’Œå¹³å‡å¥–åŠ±
        progress_desc = f"ğŸš€ MAPPOè®­ç»ƒ | å½“å‰: {episode_reward:.1f} | å¹³å‡: {current_avg_reward:.1f}"
        progress_bar.set_description(progress_desc)
        
        # æ›´æ–°è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        postfix_dict = {
            'Best': f'{best_reward:.1f}',
            'EPS': f'{eps_per_sec:.1f}/s'
        }
        
        # åªåœ¨æœ‰æŸå¤±ä¿¡æ¯æ—¶æ·»åŠ æŸå¤±æŒ‡æ ‡
        if 'loss_info' in locals() and loss_info:
            postfix_dict['PLoss'] = f'{loss_info["policy_loss"]:.3f}'
            postfix_dict['VLoss'] = f'{loss_info["value_loss"]:.3f}'
        
        # æ·»åŠ è¯¾ç¨‹å­¦ä¹ ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if curriculum_manager:
            current_stage = curriculum_manager.current_stage
            postfix_dict['Stage'] = current_stage.name[:4]  # ç¼©çŸ­æ˜¾ç¤º
        
        progress_bar.set_postfix(postfix_dict)
        progress_bar.refresh()
        
        # è¯¾ç¨‹å­¦ä¹ å·²ç§»é™¤ï¼Œæ— éœ€æ£€æŸ¥é˜¶æ®µè½¬æ¢
        
        # å¥–åŠ±è°ƒåº¦å™¨çŠ¶æ€æŠ¥å‘Šå·²ç§»è‡³æ¨¡å‹ä¿å­˜æ—¶è¾“å‡º
        
        # è®¡ç®—è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ç”¨äºè‡ªé€‚åº”å¥–åŠ±è°ƒåº¦
        if episode > 0 and episode % 50 == 0:
            # è®¡ç®—å®Œæˆç‡
            completion_rate = sum(1 for r in episode_rewards[-50:] if r > 0) / 50
            
            # è®¡ç®—æ•ˆç‡ï¼ˆå¹³å‡å¥–åŠ±ï¼‰
            efficiency = np.mean(episode_rewards[-50:])
            
            # æ›´æ–°è‡ªé€‚åº”å¥–åŠ±æƒé‡
            performance_metrics = {
                'completion_rate': completion_rate,
                'efficiency': efficiency,
                'total_reward': efficiency  # ä½¿ç”¨æ•ˆç‡ä½œä¸ºæ€»å¥–åŠ±çš„ä»£ç†
            }
            env.reward_scheduler.update_weights(performance_metrics)
        
        # æ”¹è¿›çš„æ—©åœæœºåˆ¶ï¼šæ›´å®½æ¾çš„æ¡ä»¶ï¼Œç¡®ä¿å……åˆ†è®­ç»ƒ
        if episode > 5000:  # è‡³å°‘è®­ç»ƒ5000è½®
            # è®¡ç®—æœ€è¿‘100è½®çš„å¹³å‡å¥–åŠ±
            recent_avg = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else current_avg_reward
            
            # æ›´å®½æ¾çš„æ—©åœæ¡ä»¶ï¼šåªæœ‰åœ¨æ€§èƒ½ä¸¥é‡ä¸‹é™ä¸”é•¿æ—¶é—´æ— æ”¹å–„æ—¶æ‰åœæ­¢
            if (recent_avg < best_reward * 0.3 and  # ä»0.5æ”¾å®½åˆ°0.3
                episode - best_episode > 500 and   # ä»200å¢åŠ åˆ°500è½®
                episode > 8000):                   # ä»2000å¢åŠ åˆ°8000è½®
                print(f"ğŸ›‘ é•¿æœŸæ€§èƒ½ä¸‹é™ï¼Œåœ¨ç¬¬ {episode} è½®åœæ­¢è®­ç»ƒ")
                print(f"   æœ€è¿‘100è½®å¹³å‡å¥–åŠ±: {recent_avg:.2f}")
                print(f"   æœ€ä½³å¥–åŠ±: {best_reward:.2f} (ç¬¬{best_episode}è½®)")
                print(f"   å·²è¿ç»­ {episode - best_episode} è½®æ— æ”¹å–„")
                break
        
        # è®¾ç½®ç®€åŒ–çš„è¿›åº¦æ¡æè¿°
        if episode == 1:  # åªåœ¨ç¬¬ä¸€è½®è®¾ç½®ä¸€æ¬¡
            progress_bar.set_description("ğŸš€ è®­ç»ƒè¿›åº¦ - ä¸“å®¶çº§é…ç½®")
        
        # æ¯50ä¸ªepisodeè¿›è¡ŒéªŒè¯é›†è¯„ä¼°
        if episode % validation_frequency == 0 and episode > 0:
            current_validation_reward = validate_model(mappo, validation_env)
            validation_rewards.append(current_validation_reward)
            
            # æ£€æµ‹è¿‡æ‹Ÿåˆï¼šéªŒè¯é›†æ€§èƒ½ä¸‹é™
            if len(validation_rewards) > 1:
                if current_validation_reward < validation_rewards[-2]:
                    validation_decline_count += 1
                else:
                    validation_decline_count = 0
                    
                # å¦‚æœéªŒè¯é›†æ€§èƒ½æŒç»­ä¸‹é™ï¼Œæå‰åœæ­¢
                if validation_decline_count >= validation_patience:
                    print(f"ğŸ›‘ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼ŒéªŒè¯é›†æ€§èƒ½è¿ç»­{validation_patience}æ¬¡ä¸‹é™ï¼Œåœ¨ç¬¬ {episode} è½®åœæ­¢è®­ç»ƒ")
                    print(f"   å½“å‰éªŒè¯å¥–åŠ±: {current_validation_reward:.2f}")
                    print(f"   æœ€ä½³éªŒè¯å¥–åŠ±: {max(validation_rewards):.2f}")
                    break
            
            print(f"ğŸ“Š éªŒè¯è¯„ä¼° (ç¬¬{episode}è½®): éªŒè¯å¥–åŠ± = {current_validation_reward:.2f}, è®­ç»ƒå¥–åŠ± = {current_avg_reward:.2f}")
        
        # æ¯50ä¸ªepisodeè¿›è¡Œè¯¦ç»†æŠ¥å‘Š
        if episode % 50 == 0 and episode > 0:
            # è·å–ç¯å¢ƒçŠ¶æ€ä¿¡æ¯
            env_state = env._get_current_state()
            
            # è®¡ç®—æœåŠ¡å®Œæˆç‡
            total_demand = sum(locker.get('demand_del', 0) + locker.get('demand_ret', 0) for locker in env.lockers_state)
            served_lockers = sum(1 for locker in env.lockers_state if locker.get('served', False))
            completion_rate = (served_lockers / len(env.lockers_state) * 100) if len(env.lockers_state) > 0 else 0
            
            # è®¡ç®—å¹³å‡å¡è½¦å®¹é‡åˆ©ç”¨ç‡
            total_capacity_used = 0
            total_capacity = 0
            for truck in env.trucks:
                current_load = truck.get('current_delivery_load', 0) + truck.get('current_return_load', 0)
                capacity = truck.get('capacity', 250)
                total_capacity_used += current_load
                total_capacity += capacity
            avg_capacity_utilization = (total_capacity_used / total_capacity * 100) if total_capacity > 0 else 0
            
            # è·å–æœ€åä¸€æ­¥çš„åŠ¨ä½œä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            truck_decisions = ""
            if 'trajectory' in locals() and len(trajectory['actions']) > 0:
                last_actions = trajectory['actions'][-1]
                truck_decisions = "\nğŸš› å¡è½¦å†³ç­–:"
                for i, action in enumerate(last_actions):
                    if isinstance(action, dict):
                        stop_action = action.get('select_stop', 0)
                        if stop_action == 0:
                            action_desc = "ActionType.RETURN_TO_DEPOT -> None"
                        else:
                            action_desc = f"ActionType.MOVE_TO_LOCKER -> {stop_action}"
                    else:
                        action_desc = f"Action -> {action}"
                    truck_decisions += f"\n     å¡è½¦truck_{i}: {action_desc}"
            
            # è®¡ç®—æˆæœ¬ä¿¡æ¯
            if 'trajectory' in locals() and len(trajectory['rewards']) > 0:
                step_reward = trajectory['rewards'][-1]
                avg_step_reward = sum(step_reward) / len(step_reward) if step_reward else 0
            else:
                avg_step_reward = 0
            
            # ä½¿ç”¨tqdm.writeè¾“å‡ºè¯¦ç»†æŠ¥å‘Š
            detailed_report = (f"\n{'='*80}\n"
                             f"ğŸ“Š Episode {episode} | ä¸“å®¶çº§é…ç½®è®­ç»ƒ\n"
                             f"ğŸ† å¹³å‡å¥–åŠ±: {current_avg_reward:.2f} | æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
            
            if len(trajectory['states']) > 0 and 'loss_info' in locals():
                detailed_report += f"\nğŸ“ˆ ç­–ç•¥æŸå¤±: {loss_info['policy_loss']:.4f} | ä»·å€¼æŸå¤±: {loss_info['value_loss']:.4f}"
            
            # æ·»åŠ è¯¦ç»†çš„å†³ç­–å’ŒçŠ¶æ€ä¿¡æ¯
            detailed_report += truck_decisions
            detailed_report += (f"\nğŸ“Š ç¯å¢ƒçŠ¶æ€:"
                              f"\n     æœåŠ¡å®Œæˆç‡: {completion_rate:.2f}%"
                              f"\n     å¹³å‡å¡è½¦å®¹é‡åˆ©ç”¨ç‡: {avg_capacity_utilization:.2f}%")
            
            # è·å–æ­¥æ•°ä¿¡æ¯
            current_step_count = env.time_step if hasattr(env, 'time_step') else 0
            
            detailed_report += (f"\nğŸ’° æˆæœ¬åˆ†æ:"
                              f"\n     æ­¥éª¤å¥–åŠ±: {avg_step_reward:.2f}"
                              f"\n     æ€»æˆæœ¬: {abs(avg_step_reward * current_step_count):.2f}")
            
            # è®¡ç®—æ€§èƒ½æ¯”ç‡ï¼ˆåŸºäºå¥–åŠ±ç›¸å¯¹äºæœ€ä½³å¥–åŠ±çš„æ¯”ä¾‹ï¼‰
            performance_ratio = max(0.1, episode_reward / max(best_reward, 1.0)) if best_reward > 0 else 1.0
            
            # è®¡ç®—çœŸæ­£çš„è·¯å¾„æ•ˆç‡ï¼ˆåŸºäºæ­¥æ•°ï¼‰
            actual_path_efficiency = env._calculate_path_efficiency()
            
            detailed_report += (f"\nğŸ“ˆ ç¬¬ {episode} å›åˆæ€»ç»“:"
                              f"\n    æ€»æ­¥æ•°: {current_step_count}"
                              f"\n    æ€»å¥–åŠ±: {episode_reward:.2f}"
                              f"\n    å®Œæˆç‡: {completion_rate:.2f}%"
                              f"\n    æ•ˆç‡æŒ‡æ ‡:"
                              f"\n       å¹³å‡æ­¥éª¤å¥–åŠ±: {avg_step_reward:.2f}"
                              f"\n       è·¯å¾„æ•ˆç‡: {(actual_path_efficiency * 100):.2f}%"
                              f"\n       èµ„æºåˆ©ç”¨ç‡: {avg_capacity_utilization:.2f}%")
            
            detailed_report += f"\n{'='*80}"
            progress_bar.write(detailed_report)

    
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æ€»è®­ç»ƒè½®æ•°: {episode + 1}")
    print(f"æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
    print("æœ€ç»ˆç¯å¢ƒé…ç½®: ä¸“å®¶çº§é…ç½®")
    
    print("\nğŸ“ˆ æœ€ç»ˆè®­ç»ƒç»Ÿè®¡:")
    print(f"   è®­ç»ƒè½®æ•°: {episode + 1}")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
    print(f"   ç¯å¢ƒé…ç½®: ä¸“å®¶çº§ (4å¡è½¦, 15å‚¨ç‰©æŸœ, 300æ— äººæœºèˆªç¨‹)")
    
    return mappo