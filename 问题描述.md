# 无人机-卡车协同配送路径优化问题

## 📋 问题背景

在最后一公里配送场景中，传统单一配送方式存在效率低、成本高的问题。本项目研究**卡车与无人机协同配送**的路径优化问题，旨在通过智能调度提高配送效率、降低运营成本。

## 🎯 问题场景

### 场景设定
- **配送中心**：位于坐标原点 (0, 0) 的仓库
- **配送目标**：分布在二维平面上的多个快递柜（默认50个）
- **配送工具**：
  - **卡车**：容量大（100单位），速度慢（20单位/时间），可携带无人机
  - **无人机**：容量小，速度快（1.0单位/时间），续航有限（单程25单位，往返50单位）
- **配送需求**：每个快递柜有取货需求（delivery）和退货需求（return）

## 🔍 求解问题

### 问题定义

**给定**：
- 仓库位置：$D = (0, 0)$
- 快递柜集合：$L = \{l_1, l_2, ..., l_N\}$，每个快递柜 $l_i$ 有位置 $(x_i, y_i)$ 和需求 $(d_i^{del}, d_i^{ret})$
- 卡车集合：$T = \{t_1, t_2, ..., t_M\}$，每辆卡车容量为 $C = 100$
- 无人机集合：每辆卡车携带 $K = 3$ 架无人机

**求解**：
1. 每辆卡车的停靠点序列：$P_t = \{p_{t,1}, p_{t,2}, ..., p_{t,n_t}\}$，其中 $p_{t,j} \in L \cup \{D\}$
2. 每个停靠点的服务区域：$S_{t,j} \subseteq L$，表示从停靠点 $p_{t,j}$ 用无人机服务的快递柜集合
3. 无人机任务分配：为每个服务区域内的快递柜分配无人机任务序列

**目标**：在满足所有约束条件下，最大化综合奖励（服务完成率、效率、成本控制）

## ⚙️ 约束条件

### 1. 容量约束
- **卡车容量限制**：
  $$
  \sum_{l_i \in \text{served}} d_i^{del} \leq C \quad \forall t \in T
  $$
  每辆卡车的取货货物总量不能超过容量 $C = 100$

- **负载平衡**：
  $$
  \text{current\_load}_t + \text{new\_demand} \leq C
  $$
  每次服务后，卡车当前负载不能超过容量

### 2. 续航约束
- **无人机航程限制**：
  $$
  2 \times \text{distance}(p_{t,j}, l_i) \leq R_{drone} \quad \forall l_i \in S_{t,j}
  $$
  其中 $R_{drone} = 50$ 为无人机最大往返航程

- **服务可达性**：
  $$
  \text{distance}(p_{t,j}, l_i) \leq R_{drone}/2 = 25
  $$
  快递柜必须在卡车停靠点的服务半径内

### 3. 时间约束
- **最大时间步数**：
  $$
  t \leq T_{max} = 600
  $$
  整个配送过程不能超过最大时间步数

- **无人机服务时间窗**：
  $$
  \sum_{task \in \text{schedule}} \text{time}(task) \leq T_{window} = 300
  $$
  卡车停靠后，无人机必须在300秒内完成所有任务

- **软时间窗约束**：
  $$
  \text{penalty} = \begin{cases}
  \alpha \times (t_{early})^2 & \text{if } t < t_{start} \\
  0 & \text{if } t_{start} \leq t \leq t_{end} \\
  \beta \times (t - t_{end})^2 & \text{if } t > t_{end}
  \end{cases}
  $$
  每个快递柜有期望服务时间窗，超时会产生惩罚

### 4. 动作约束
- **停靠点选择**：
  $$
  p_{t,j} \in \{0, 1, 2, ..., N\}
  $$
  其中 $0$ 表示仓库，$1..N$ 表示快递柜编号

- **服务区域选择**：
  $$
  S_{t,j} \subseteq \{l_i : \text{distance}(p_{t,j}, l_i) \leq R_{drone}/2\}
  $$
  只能服务在航程范围内的快递柜

### 5. 需求约束
- **需求非负**：
  $$
  d_i^{del} \geq 0, \quad d_i^{ret} \geq 0 \quad \forall l_i \in L
  $$

- **服务完整性**：
  一旦开始服务某个快递柜，必须完成其所有需求（取货+退货）

## 🎯 目标函数

### 综合奖励函数

总奖励 $R_{total}$ 由以下5个部分组成：

$$
R_{total} = R_{service} + R_{efficiency} - P_{cost} - P_{violation} + R_{strategy}
$$

### 1. 服务完成奖励 $R_{service}$

$$
R_{service} = w_1 \times R_{completion} + w_2 \times R_{demand} + w_3 \times R_{early}
$$

- **服务完成奖励**：$R_{completion} = \sum \text{新完成的服务数}$
- **需求满足奖励**：$R_{demand} = \sum \text{新服务的需求量}$
- **提前完成奖励**：$R_{early} = (1 - t/T_{max}) \times \text{completion\_bonus}$，鼓励快速完成

**权重**：$w_1 = 0$, $w_2 = 0.001$, $w_3 = 5.0$

### 2. 运营效率奖励 $R_{efficiency}$

$$
R_{efficiency} = w_4 \times R_{operational} + w_5 \times R_{weighted} + w_6 \times R_{utilization} + w_7 \times R_{coordination}
$$

- **路径效率**：$R_{operational} = \frac{\text{服务快递柜数}}{\text{移动距离}}$
- **需求量加权路径效率**：$R_{weighted} = \frac{\sum \text{需求量权重}}{\text{移动距离}}$（核心奖励）
- **资源利用率**：$R_{utilization} = \frac{\text{已用容量}}{\text{总容量}}$
- **协调奖励**：$R_{coordination} = f(\text{多卡车协同效果})$

**权重**：$w_4 = 60.0$, $w_5 = 50.0$, $w_6 = 15.0$, $w_7 = 100.0$

### 3. 成本控制惩罚 $P_{cost}$

$$
P_{cost} = w_8 \times P_{travel} + w_9 \times P_{time} + w_{10} \times P_{step}
$$

- **路径成本惩罚**：$P_{travel} = \text{移动距离} \times \text{单位距离成本}$
- **时间成本惩罚**：$P_{time} = \text{服务时间} \times \text{单位时间成本}$
- **步数惩罚**：$P_{step} = \text{当前步数} \times \text{步数惩罚系数}$（鼓励用更少步数完成）

**权重**：$w_8 = 12.0$, $w_9 = 3.0$, $w_{10} = 25.0$

### 4. 约束违规惩罚 $P_{violation}$

$$
P_{violation} = w_{11} \times P_{constraint} + w_{12} \times P_{invalid} + w_{13} \times P_{unserved}
$$

- **约束违规惩罚**：容量超限、航程超限等
- **无效动作惩罚**：选择不可达的停靠点或服务区域
- **未服务需求惩罚**：回合结束时未完成的需求

**权重**：$w_{11} = 3.0$, $w_{12} = 2.0$, $w_{13} = 1.0$

### 5. 策略优化奖励 $R_{strategy}$

$$
R_{strategy} = w_{14} \times R_{strategic} + w_{15} \times R_{coverage}
$$

- **智能决策奖励**：选择高需求区域、避免重复访问等
- **覆盖优化奖励**：最大化服务覆盖范围

**权重**：$w_{14} = 35.0$, $w_{15} = 30.0$

## 💡 解决方案

### 核心策略：多智能体强化学习（MAPPO）

采用 **Multi-Agent Proximal Policy Optimization (MAPPO)** 算法，将每辆卡车视为一个智能体，通过协同学习优化配送策略。

### 算法实现与功能

#### 1. MAPPO算法（Multi-Agent Proximal Policy Optimization）

**功能**：多智能体协同决策优化

**实现**：
- **策略网络**：使用多头注意力机制处理状态信息，输出停靠点选择和服务区域选择
- **价值网络**：估计状态价值，用于优势函数计算
- **GAE优势估计**：使用广义优势估计（GAE）计算动作优势
- **PPO裁剪**：使用裁剪机制保证策略更新稳定性

**关键参数**：
- 学习率：$lr = 3 \times 10^{-4}$
- 折扣因子：$\gamma = 0.99$
- GAE参数：$\lambda = 0.95$
- 裁剪范围：$\epsilon = 0.05$
- 批次大小：$batch\_size = 256$

#### 2. 动态无人机调度器（DynamicDroneScheduler）

**功能**：在300秒时间窗内优化无人机任务分配

**实现**：
- **动态规划**：使用DP算法在时间窗内优化任务序列
- **任务创建**：根据卡车位置和航程限制创建有效任务
- **调度优化**：考虑飞行时间、服务时间、时间窗约束
- **软时间窗处理**：超时任务产生惩罚而非直接拒绝

**约束处理**：
- 航程约束：$2 \times distance \leq 50$
- 时间约束：$\sum time_{task} \leq 300$
- 任务冲突：同一无人机不能同时执行多个任务

#### 3. 状态表示（StateRepresentation）

**功能**：将环境状态编码为神经网络可处理的向量

**实现**：
- **局部特征**（~100维）：
  - 卡车位置、速度、容量、当前负载
  - 卡车到各快递柜的距离
  - 卡车到仓库的距离
- **全局特征**（~200维）：
  - 所有快递柜的需求状态
  - 所有快递柜的服务状态
  - 全局需求分布统计
- **时空特征**（~100维）：
  - 当前时间步
  - 历史访问记录
  - 时间窗信息
- **动态特征**（~50维）：
  - 需求变化趋势
  - 服务进度
  - 协调信息

**总状态维度**：约400-600维（取决于快递柜数量）

#### 4. 动作掩码（ActionMask）

**功能**：过滤无效动作，提高训练效率

**实现**：
- **停靠点掩码**：标记可达的停靠点（考虑当前位置和距离）
- **服务区域掩码**：标记可服务的快递柜（考虑航程限制）
- **容量掩码**：标记容量允许的服务组合

#### 5. 奖励函数（RewardFunction）

**功能**：将多目标优化问题转化为强化学习奖励信号

**实现**：
- **5类奖励组件**：服务完成、运营效率、成本控制、约束违规、策略优化
- **权重自适应**：根据训练进度动态调整奖励权重
- **奖励平滑**：使用指数移动平均平滑奖励信号

#### 6. 自适应训练优化

**功能**：提高训练稳定性和收敛速度

**实现**：
- **自适应学习率调度器**：根据性能变化动态调整学习率
- **探索策略调度器**：平衡探索与利用，动态调整熵系数
- **奖励平滑器**：使用EMA平滑奖励，减少异常值影响
- **收敛检测器**：检测训练收敛状态，提供早停建议

#### 7. 软时间窗管理器（SoftTimeWindowManager）

**功能**：处理时间窗约束，允许超时但产生惩罚

**实现**：
- **二次惩罚函数**：$penalty = \alpha \times (delay)^2$
- **早到惩罚**：提前到达也产生小惩罚
- **超时惩罚**：延迟到达产生递增惩罚

#### 8. 补货优化器（ReplenishmentOptimizer）

**功能**：智能决策卡车何时返回仓库补货

**实现**：
- **容量利用率监控**：跟踪当前容量使用率
- **自适应策略**：根据剩余需求和距离决定是否补货
- **补货触发条件**：容量低于阈值或距离仓库较近

## 📊 问题复杂度

### 状态空间
- **维度**：~400-600维（取决于快递柜数量）
- **组成**：局部特征 + 全局特征 + 时空特征 + 动态特征

### 动作空间
- **停靠点选择**：$N+1$ 个选择（N个快递柜 + 1个仓库）
- **服务区域选择**：$2^N$ 种组合（每个快递柜是否服务）
- **实际有效动作**：通过动作掩码大幅减少（约减少90%+）

### 多智能体规模
- **智能体数量**：2-3辆卡车（根据需求动态计算）
- **协同方式**：共享经验缓冲区，独立策略网络

### 动态性
- **需求变化**：每个episode需求随机生成
- **在线适应**：智能体需要实时响应环境变化

## 🎯 优化目标总结

1. **最大化服务完成率**：$\max \frac{\sum \text{已服务需求}}{\sum \text{总需求}}$
2. **最小化总成本**：$\min (\text{路径成本} + \text{时间成本} + \text{惩罚成本})$
3. **最大化路径效率**：$\max \frac{\text{服务快递柜数}}{\text{总移动距离}}$
4. **最大化资源利用率**：$\max \frac{\text{已用容量}}{\text{总容量}}$
5. **最小化步数**：$\min \text{完成配送所需步数}$

---

**项目类型**：多智能体强化学习  
**核心算法**：MAPPO (Multi-Agent Proximal Policy Optimization)  
**应用领域**：物流配送、路径优化、资源调度

