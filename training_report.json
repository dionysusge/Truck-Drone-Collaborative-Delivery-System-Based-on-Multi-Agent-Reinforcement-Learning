{
  "training_summary": {
    "total_episodes": 209,
    "training_time": 61.359766721725464,
    "average_reward": 4478.46145816073,
    "recent_average_reward": 4491.470918296189,
    "best_reward": 4887.368889678801,
    "reward_improvement": 343.892563440927,
    "convergence_status": "训练已收敛",
    "final_learning_rate": 0.00023999999999999998,
    "final_exploration_rate": 0.1052313472454502,
    "reward_stability": 0.03831582206779509,
    "optimization_enabled": true
  },
  "environment_config": {
    "num_trucks": 2,
    "num_lockers": 30,
    "truck_capacity": 100,
    "state_dimension": 424
  },
  "training_config": {
    "total_timesteps": 50000,
    "learning_rate": 0.0003,
    "batch_size": 256,
    "gamma": 0.99,
    "optimization_enabled": true
  },
  "training_log": [
    {
      "episode": 0,
      "reward": 4197.140301279942,
      "timestamp": 1.1026058197021484,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4197.140301279942,
        "best_reward": 4197.140301279942,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4197.140301279942,
        "raw_reward": 4197.140301279942,
        "reward_service": 728.427414567373,
        "reward_efficiency": 3468.712886712569,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2985,
        "exploration_entropy": 0.01,
        "policy_loss": 0.2848466833432515,
        "value_loss": 1.24404776096344,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0098
      }
    },
    {
      "episode": 1,
      "reward": 4566.944323785939,
      "timestamp": 1.374678373336792,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4215.630502405242,
        "best_reward": 4566.944323785939,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4267.403065556082,
        "raw_reward": 4566.944323785939,
        "reward_service": 802.3101460789161,
        "reward_efficiency": 3764.634177707022,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2970075,
        "exploration_entropy": 0.0098,
        "policy_loss": 0.2861115137736003,
        "value_loss": 1.2051695585250854,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.009604
      }
    },
    {
      "episode": 2,
      "reward": 4887.368889678801,
      "timestamp": 1.640636920928955,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4253.553550926279,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4385.196572139399,
        "raw_reward": 4887.368889678801,
        "reward_service": 939.667101517959,
        "reward_efficiency": 3947.701788160842,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.29552246249999997,
        "exploration_entropy": 0.009604,
        "policy_loss": 0.30443323651949566,
        "value_loss": 1.2559841076533,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.009411919999999999
      }
    },
    {
      "episode": 3,
      "reward": 4169.074957564653,
      "timestamp": 1.913557767868042,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4281.06126586519,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4344.133465370197,
        "raw_reward": 4169.074957564653,
        "reward_service": 735.3628158213282,
        "reward_efficiency": 3433.7121417433245,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2940448501875,
        "exploration_entropy": 0.009411919999999999,
        "policy_loss": 0.3044702609380086,
        "value_loss": 1.1266897122065227,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.009223681599999999
      }
    },
    {
      "episode": 4,
      "reward": 4633.70628189458,
      "timestamp": 2.1727888584136963,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4299.467162096679,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4399.152300509831,
        "raw_reward": 4633.70628189458,
        "reward_service": 782.914382032998,
        "reward_efficiency": 3850.7918998615814,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.29257462593656247,
        "exploration_entropy": 0.009223681599999999,
        "policy_loss": 0.30148665110270184,
        "value_loss": 1.1575750907262166,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.009039207967999998
      }
    },
    {
      "episode": 5,
      "reward": 4397.972367070852,
      "timestamp": 2.40511155128479,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4316.061686274888,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4398.928113156425,
        "raw_reward": 4397.972367070852,
        "reward_service": 670.4323625744627,
        "reward_efficiency": 3727.540004496389,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2911117528068797,
        "exploration_entropy": 0.009039207967999998,
        "policy_loss": 0.3010917603969574,
        "value_loss": 1.2274359067281086,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.008858423808639998
      }
    },
    {
      "episode": 6,
      "reward": 4182.560654313534,
      "timestamp": 2.6452383995056152,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4324.80878356021,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4357.818295976275,
        "raw_reward": 4182.560654313534,
        "reward_service": 770.0084001316602,
        "reward_efficiency": 3412.552254181874,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.28965619404284526,
        "exploration_entropy": 0.008858423808639998,
        "policy_loss": 0.3009696106115977,
        "value_loss": 1.1180158853530884,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.008681255332467197
      }
    },
    {
      "episode": 7,
      "reward": 4568.452505299442,
      "timestamp": 2.8868792057037354,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4331.567900228757,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4397.838795747677,
        "raw_reward": 4568.452505299442,
        "reward_service": 753.6909012937011,
        "reward_efficiency": 3814.761604005741,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.28820791307263105,
        "exploration_entropy": 0.008681255332467197,
        "policy_loss": 0.2813952664534251,
        "value_loss": 1.3108997344970703,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.008507630225817853
      }
    },
    {
      "episode": 8,
      "reward": 4624.1650478607535,
      "timestamp": 3.1225533485412598,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4341.446069198782,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4440.8407836491615,
        "raw_reward": 4624.1650478607535,
        "reward_service": 746.2763568925391,
        "reward_efficiency": 3877.8886909682137,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.28676687350726787,
        "exploration_entropy": 0.008507630225817853,
        "policy_loss": 0.3004784385363261,
        "value_loss": 1.1807771523793538,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.008337477621301496
      }
    },
    {
      "episode": 9,
      "reward": 4736.274467796476,
      "timestamp": 3.356365442276001,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4354.339877485294,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4496.973183637152,
        "raw_reward": 4736.274467796476,
        "reward_service": 806.0407732277538,
        "reward_efficiency": 3930.2336945687202,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2853330391397315,
        "exploration_entropy": 0.008337477621301496,
        "policy_loss": 0.3018561104933421,
        "value_loss": 1.226352572441101,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.008170728068875466
      }
    },
    {
      "episode": 10,
      "reward": 4748.951070389006,
      "timestamp": 3.608909845352173,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4369.597249742298,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4544.848982120005,
        "raw_reward": 4748.951070389006,
        "reward_service": 873.5397717520899,
        "reward_efficiency": 3875.4112986369164,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.28390637394403284,
        "exploration_entropy": 0.008170728068875466,
        "policy_loss": 0.29115115602811176,
        "value_loss": 1.1826766729354858,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.008007313507497957
      }
    },
    {
      "episode": 11,
      "reward": 4647.696968119715,
      "timestamp": 3.842257261276245,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4385.05862732377,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4564.390099459951,
        "raw_reward": 4647.696968119715,
        "reward_service": 790.3345901314843,
        "reward_efficiency": 3857.36237798823,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2824868420743127,
        "exploration_entropy": 0.008007313507497957,
        "policy_loss": 0.307661106189092,
        "value_loss": 1.2454807758331299,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.007847167237347998
      }
    },
    {
      "episode": 12,
      "reward": 4142.368200206809,
      "timestamp": 4.071043968200684,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4399.30035196443,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4527.417762907785,
        "raw_reward": 4142.368200206809,
        "reward_service": 684.9292894099609,
        "reward_efficiency": 3457.4389107968473,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2810744078639411,
        "exploration_entropy": 0.007847167237347998,
        "policy_loss": 0.29601014653841656,
        "value_loss": 1.0927849213282268,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.007690223892601038
      }
    },
    {
      "episode": 13,
      "reward": 4587.886044992484,
      "timestamp": 4.306493043899536,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4408.883511903846,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4538.906736503878,
        "raw_reward": 4587.886044992484,
        "reward_service": 788.3330855310644,
        "reward_efficiency": 3799.5529594614204,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2796690358246214,
        "exploration_entropy": 0.007690223892601038,
        "policy_loss": 0.3105878035227458,
        "value_loss": 1.2299040953318279,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.007536419414749017
      }
    },
    {
      "episode": 14,
      "reward": 4251.602931365127,
      "timestamp": 4.544719219207764,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4415.636368176256,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4484.319013527515,
        "raw_reward": 4251.602931365127,
        "reward_service": 741.9311359611522,
        "reward_efficiency": 3509.6717954039746,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.27827069064549825,
        "exploration_entropy": 0.007536419414749017,
        "policy_loss": 0.3000430961449941,
        "value_loss": 1.1140986680984497,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.007385691026454037
      }
    },
    {
      "episode": 15,
      "reward": 4406.03486671027,
      "timestamp": 4.798844337463379,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4419.439757593102,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4469.445025632238,
        "raw_reward": 4406.03486671027,
        "reward_service": 692.5941697064452,
        "reward_efficiency": 3713.440697003825,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.27687933719227076,
        "exploration_entropy": 0.007385691026454037,
        "policy_loss": 0.2886359492937724,
        "value_loss": 1.1070117553075154,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0072379772059249555
      }
    },
    {
      "episode": 16,
      "reward": 4272.091198166441,
      "timestamp": 5.038011074066162,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4421.220339080899,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4431.947798413737,
        "raw_reward": 4272.091198166441,
        "reward_service": 750.5872337984474,
        "reward_efficiency": 3521.503964367994,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2754949405063094,
        "exploration_entropy": 0.0072379772059249555,
        "policy_loss": 0.3003713587919871,
        "value_loss": 1.1010593175888062,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.007093217661806457
      }
    },
    {
      "episode": 17,
      "reward": 4627.75536926048,
      "timestamp": 5.280325174331665,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4422.904128881873,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4469.151236874618,
        "raw_reward": 4627.75536926048,
        "reward_service": 776.969548561709,
        "reward_efficiency": 3850.785820698771,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2741174658037779,
        "exploration_entropy": 0.007093217661806457,
        "policy_loss": 0.2741100788116455,
        "value_loss": 1.0844186147054036,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006951353308570328
      }
    },
    {
      "episode": 18,
      "reward": 4486.687826478108,
      "timestamp": 5.5125226974487305,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4425.430485037298,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4472.483188899281,
        "raw_reward": 4486.687826478108,
        "reward_service": 707.235536839795,
        "reward_efficiency": 3779.4522896383132,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.272746878474759,
        "exploration_entropy": 0.006951353308570328,
        "policy_loss": 0.3014528254667918,
        "value_loss": 1.16094704469045,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006812326242398921
      }
    },
    {
      "episode": 19,
      "reward": 4480.362003726755,
      "timestamp": 5.744133949279785,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4427.822514304535,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4473.980163716501,
        "raw_reward": 4480.362003726755,
        "reward_service": 685.4564980779395,
        "reward_efficiency": 3794.9055056488155,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2713831440823852,
        "exploration_entropy": 0.006812326242398921,
        "policy_loss": 0.30479588111241657,
        "value_loss": 1.0692603985468547,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006676079717550942
      }
    },
    {
      "episode": 20,
      "reward": 4708.538188465967,
      "timestamp": 5.977779865264893,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4431.137440584864,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4518.5461884189,
        "raw_reward": 4708.538188465967,
        "reward_service": 841.8176324725097,
        "reward_efficiency": 3866.720555993457,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.27002622836197326,
        "exploration_entropy": 0.006676079717550942,
        "policy_loss": 0.315207839012146,
        "value_loss": 1.0200051267941792,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006542558123199923
      }
    },
    {
      "episode": 21,
      "reward": 4576.3162652243045,
      "timestamp": 6.214168548583984,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4435.3731567446175,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4529.522503011926,
        "raw_reward": 4576.3162652243045,
        "reward_service": 720.7657539968454,
        "reward_efficiency": 3855.5505112274595,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2686760972201634,
        "exploration_entropy": 0.006542558123199923,
        "policy_loss": 0.28971479336420697,
        "value_loss": 0.9262744784355164,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0064117069607359245
      }
    },
    {
      "episode": 22,
      "reward": 4191.987568408243,
      "timestamp": 6.444138526916504,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4439.478922441171,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4496.023947829615,
        "raw_reward": 4191.987568408243,
        "reward_service": 714.0375954617189,
        "reward_efficiency": 3477.9499729465238,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.26733271673406256,
        "exploration_entropy": 0.0064117069607359245,
        "policy_loss": 0.2933453718821208,
        "value_loss": 1.0391503969828289,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006283472821521206
      }
    },
    {
      "episode": 23,
      "reward": 4578.03896967895,
      "timestamp": 6.676351308822632,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4442.176694423394,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4511.606801980989,
        "raw_reward": 4578.03896967895,
        "reward_service": 773.0431366206934,
        "reward_efficiency": 3804.995833058256,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.26599605315039226,
        "exploration_entropy": 0.006283472821521206,
        "policy_loss": 0.30063366889953613,
        "value_loss": 1.0117972095807393,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006157803365090782
      }
    },
    {
      "episode": 24,
      "reward": 4069.539618441859,
      "timestamp": 6.904331207275391,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4443.185629991542,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4427.614037108555,
        "raw_reward": 4069.539618441859,
        "reward_service": 649.344181449414,
        "reward_efficiency": 3420.1954369924447,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2646660728846403,
        "exploration_entropy": 0.006157803365090782,
        "policy_loss": 0.2883160312970479,
        "value_loss": 0.9232160449028015,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.006034647297788966
      }
    },
    {
      "episode": 25,
      "reward": 4153.788350416161,
      "timestamp": 7.14348292350769,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4441.533546854917,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4375.587156637,
        "raw_reward": 4153.788350416161,
        "reward_service": 681.4124131358107,
        "reward_efficiency": 3472.3759372803493,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.26334274252021705,
        "exploration_entropy": 0.006034647297788966,
        "policy_loss": 0.29785967866579693,
        "value_loss": 0.9634578227996826,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005913954351833187
      }
    },
    {
      "episode": 26,
      "reward": 4576.720662119436,
      "timestamp": 7.396705389022827,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4439.836026867152,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4413.802522678663,
        "raw_reward": 4576.720662119436,
        "reward_service": 769.1021598831205,
        "reward_efficiency": 3807.6185022363156,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.26202602880761594,
        "exploration_entropy": 0.005913954351833187,
        "policy_loss": 0.2917639911174774,
        "value_loss": 0.8951630393664042,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0057956752647965225
      }
    },
    {
      "episode": 27,
      "reward": 4260.555936792909,
      "timestamp": 7.620173215866089,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4438.358949625114,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4384.68567136037,
        "raw_reward": 4260.555936792909,
        "reward_service": 746.5688601207032,
        "reward_efficiency": 3513.987076672206,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.26071589866357786,
        "exploration_entropy": 0.0057956752647965225,
        "policy_loss": 0.2914060354232788,
        "value_loss": 0.821467121442159,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005679761759500592
      }
    },
    {
      "episode": 28,
      "reward": 4600.901650635497,
      "timestamp": 7.962374687194824,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4437.253719268658,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4425.766707422645,
        "raw_reward": 4600.901650635497,
        "reward_service": 771.2889540478418,
        "reward_efficiency": 3829.6126965876556,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.25941231917026,
        "exploration_entropy": 0.005679761759500592,
        "policy_loss": 0.3046812315781911,
        "value_loss": 0.8022668759028116,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.00556616652431058
      }
    },
    {
      "episode": 29,
      "reward": 4644.348787596787,
      "timestamp": 8.235387086868286,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4437.599425807705,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4467.297302655732,
        "raw_reward": 4644.348787596787,
        "reward_service": 820.8655465888771,
        "reward_efficiency": 3823.4832410079102,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2581152575744087,
        "exploration_entropy": 0.00556616652431058,
        "policy_loss": 0.283636877934138,
        "value_loss": 0.8182913064956665,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005454843193824369
      }
    },
    {
      "episode": 30,
      "reward": 4420.718248525046,
      "timestamp": 8.501989126205444,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4438.407166821736,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4458.447282370902,
        "raw_reward": 4420.718248525046,
        "reward_service": 687.2657846877443,
        "reward_efficiency": 3733.4524638373023,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2568246812865366,
        "exploration_entropy": 0.005454843193824369,
        "policy_loss": 0.285513311624527,
        "value_loss": 0.8880972464879354,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005345746329947882
      }
    },
    {
      "episode": 31,
      "reward": 4245.5292350369955,
      "timestamp": 8.743430137634277,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4438.368051534728,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4417.99285337746,
        "raw_reward": 4245.5292350369955,
        "reward_service": 748.1106536558594,
        "reward_efficiency": 3497.418581381136,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.25554055788010394,
        "exploration_entropy": 0.005345746329947882,
        "policy_loss": 0.28947120904922485,
        "value_loss": 0.7449037233988444,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005238831403348924
      }
    },
    {
      "episode": 32,
      "reward": 4219.355145310292,
      "timestamp": 8.981285572052002,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4437.14868883885,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4380.251688844699,
        "raw_reward": 4219.355145310292,
        "reward_service": 737.5725562468945,
        "reward_efficiency": 3481.7825890633967,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2542628550907034,
        "exploration_entropy": 0.005238831403348924,
        "policy_loss": 0.2966710726420085,
        "value_loss": 0.7012830972671509,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005134054775281945
      }
    },
    {
      "episode": 33,
      "reward": 4796.8086885544735,
      "timestamp": 9.221195697784424,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4436.700415308756,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4459.397518789556,
        "raw_reward": 4796.8086885544735,
        "reward_service": 866.8158506585546,
        "reward_efficiency": 3929.9928378959194,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2529915408152499,
        "exploration_entropy": 0.005134054775281945,
        "policy_loss": 0.28913216789563495,
        "value_loss": 0.7123771707216898,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.005031373679776306
      }
    },
    {
      "episode": 34,
      "reward": 4342.613309978793,
      "timestamp": 9.440131187438965,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4437.015234811605,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4437.208519115511,
        "raw_reward": 4342.613309978793,
        "reward_service": 796.1593819123046,
        "reward_efficiency": 3546.453928066489,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.25172658311117363,
        "exploration_entropy": 0.005031373679776306,
        "policy_loss": 0.3024740715821584,
        "value_loss": 0.7267964283625284,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.00493074620618078
      }
    },
    {
      "episode": 35,
      "reward": 4297.638675863713,
      "timestamp": 9.680644989013672,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4436.6329098110145,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4410.690248897669,
        "raw_reward": 4297.638675863713,
        "reward_service": 804.5924234529783,
        "reward_efficiency": 3493.0462524107343,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.25046795019561774,
        "exploration_entropy": 0.00493074620618078,
        "policy_loss": 0.2939172089099884,
        "value_loss": 0.6030319929122925,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004832131282057164
      }
    },
    {
      "episode": 36,
      "reward": 4464.595099179665,
      "timestamp": 9.919391870498657,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4436.077445597903,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4420.932170451249,
        "raw_reward": 4464.595099179665,
        "reward_service": 705.0866866400195,
        "reward_efficiency": 3759.508412539645,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24921561044463966,
        "exploration_entropy": 0.004832131282057164,
        "policy_loss": 0.2973080078760783,
        "value_loss": 0.631035566329956,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004735488656416021
      }
    },
    {
      "episode": 37,
      "reward": 4497.517469094364,
      "timestamp": 10.15589451789856,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4435.8804259852095,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4435.483377193441,
        "raw_reward": 4497.517469094364,
        "reward_service": 887.7742718622851,
        "reward_efficiency": 3609.7431972320787,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24796953239241645,
        "exploration_entropy": 0.004735488656416021,
        "policy_loss": 0.3173726697762807,
        "value_loss": 0.5516577263673147,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004640778883287701
      }
    },
    {
      "episode": 38,
      "reward": 4336.100616051986,
      "timestamp": 10.39385199546814,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4435.615417654289,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4416.600652576565,
        "raw_reward": 4336.100616051986,
        "reward_service": 835.9114885826855,
        "reward_efficiency": 3500.1891274693016,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24672968473045437,
        "exploration_entropy": 0.004640778883287701,
        "policy_loss": 0.29134078820546466,
        "value_loss": 0.49262576301892597,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0045479633056219465
      }
    },
    {
      "episode": 39,
      "reward": 4543.611842242084,
      "timestamp": 10.624217510223389,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4435.45757650151,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4440.732778613014,
        "raw_reward": 4543.611842242084,
        "reward_service": 717.046486862881,
        "reward_efficiency": 3826.565355379204,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24549603630680208,
        "exploration_entropy": 0.0045479633056219465,
        "policy_loss": 0.30449244379997253,
        "value_loss": 0.551945815483729,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004457004039509507
      }
    },
    {
      "episode": 40,
      "reward": 4521.510581212527,
      "timestamp": 10.860801219940186,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4435.783258998375,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4456.080561106921,
        "raw_reward": 4521.510581212527,
        "reward_service": 749.2615947089649,
        "reward_efficiency": 3772.2489865035614,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24426855612526807,
        "exploration_entropy": 0.004457004039509507,
        "policy_loss": 0.28796056906382245,
        "value_loss": 0.5199980239073435,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004367863958719317
      }
    },
    {
      "episode": 41,
      "reward": 4256.188060214508,
      "timestamp": 11.087982892990112,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4435.790593570263,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4418.100985937363,
        "raw_reward": 4256.188060214508,
        "reward_service": 740.6337282029295,
        "reward_efficiency": 3515.5543320115785,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24304721334464174,
        "exploration_entropy": 0.004367863958719317,
        "policy_loss": 0.2923191885153453,
        "value_loss": 0.5455164313316345,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004280506679544931
      }
    },
    {
      "episode": 42,
      "reward": 4105.087780968076,
      "timestamp": 11.315853357315063,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4434.651269660267,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4358.628476993198,
        "raw_reward": 4105.087780968076,
        "reward_service": 651.2410990896483,
        "reward_efficiency": 3453.846681878428,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24183197727791853,
        "exploration_entropy": 0.004280506679544931,
        "policy_loss": 0.2858465413252513,
        "value_loss": 0.41572171449661255,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004194896545954032
      }
    },
    {
      "episode": 43,
      "reward": 4580.407105843919,
      "timestamp": 11.54992389678955,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.427521256131,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4400.766416474836,
        "raw_reward": 4580.407105843919,
        "reward_service": 748.293530633662,
        "reward_efficiency": 3832.1135752102573,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.24062281739152894,
        "exploration_entropy": 0.004194896545954032,
        "policy_loss": 0.31822410225868225,
        "value_loss": 0.43484964966773987,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004110998615034951
      }
    },
    {
      "episode": 44,
      "reward": 4557.672415113697,
      "timestamp": 11.781760931015015,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.050398924633,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4430.578556216219,
        "raw_reward": 4557.672415113697,
        "reward_service": 710.1640843282812,
        "reward_efficiency": 3847.5083307854156,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2394197033045713,
        "exploration_entropy": 0.004110998615034951,
        "policy_loss": 0.2990204195181529,
        "value_loss": 0.486519734064738,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.004028778642734252
      }
    },
    {
      "episode": 45,
      "reward": 4505.876957123479,
      "timestamp": 12.014177799224854,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.160355389466,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4444.885252388599,
        "raw_reward": 4505.876957123479,
        "reward_service": 725.184093299873,
        "reward_efficiency": 3780.692863823607,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.23822260478804844,
        "exploration_entropy": 0.004028778642734252,
        "policy_loss": 0.2996596395969391,
        "value_loss": 0.3431037465731303,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.003948203069879567
      }
    },
    {
      "episode": 46,
      "reward": 4499.822525794026,
      "timestamp": 12.250269174575806,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.52670909882,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4455.3233343356305,
        "raw_reward": 4499.822525794026,
        "reward_service": 735.7113228034276,
        "reward_efficiency": 3764.111202990597,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2370314917641082,
        "exploration_entropy": 0.003948203069879567,
        "policy_loss": 0.2897200087706248,
        "value_loss": 0.3215814431508382,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.003869239008481976
      }
    },
    {
      "episode": 47,
      "reward": 4204.7880011594025,
      "timestamp": 12.478141069412231,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.458856847136,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4407.721621032147,
        "raw_reward": 4204.7880011594025,
        "reward_service": 720.3812389224609,
        "reward_efficiency": 3484.406762236941,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.23584633430528767,
        "exploration_entropy": 0.003869239008481976,
        "policy_loss": 0.2859632571538289,
        "value_loss": 0.3097471098105113,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0037918542283123364
      }
    },
    {
      "episode": 48,
      "reward": 4627.014523936646,
      "timestamp": 12.715012788772583,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.381143673167,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4449.387272584001,
        "raw_reward": 4627.014523936646,
        "reward_service": 773.1667145928222,
        "reward_efficiency": 3853.847809343823,
        "reward_cost": 0.0,
        "convergence_status": "insufficient_data",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.23466710263376123,
        "exploration_entropy": 0.0037918542283123364,
        "policy_loss": 0.29603902498881024,
        "value_loss": 0.3530579209327698,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0037160171437460895
      }
    },
    {
      "episode": 49,
      "reward": 4480.355633750665,
      "timestamp": 12.996083974838257,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4433.763202973717,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4455.2712612056675,
        "raw_reward": 4480.355633750665,
        "reward_service": 701.7455236152441,
        "reward_efficiency": 3778.61011013542,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "需要至少100轮数据",
        "convergence_confidence": 0.8,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.23349376712059242,
        "exploration_entropy": 0.0037160171437460895,
        "policy_loss": 0.2834031085173289,
        "value_loss": 0.28661709030469257,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.003641696800871168
      }
    },
    {
      "episode": 50,
      "reward": 4486.8396057289465,
      "timestamp": 13.296247959136963,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4438.988958861277,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4461.26924666509,
        "raw_reward": 4486.8396057289465,
        "reward_service": 703.6789903190431,
        "reward_efficiency": 3783.160615409903,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4437.166964957506,
        "convergence_std_reward": 62.06140021211582,
        "convergence_cv": 0.013986717358676218,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.23232629828498946,
        "exploration_entropy": 0.003641696800871168,
        "policy_loss": 0.2997080286343892,
        "value_loss": 0.2969045986731847,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0035688628648537445
      }
    },
    {
      "episode": 51,
      "reward": 4580.54100637736,
      "timestamp": 14.313636779785156,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4443.770473243393,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4483.930881010421,
        "raw_reward": 4580.54100637736,
        "reward_service": 785.9448021621484,
        "reward_efficiency": 3794.596204215211,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4442.199011602414,
        "convergence_std_reward": 53.447923115279096,
        "convergence_cv": 0.012031861466737634,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2311646667935645,
        "exploration_entropy": 0.0035688628648537445,
        "policy_loss": 0.29741103450457257,
        "value_loss": 0.2636977285146713,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0034974856075566697
      }
    },
    {
      "episode": 52,
      "reward": 4439.946187230693,
      "timestamp": 14.63327169418335,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4446.773128516674,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4475.573789192273,
        "raw_reward": 4439.946187230693,
        "reward_service": 841.0935177478319,
        "reward_efficiency": 3598.8526694828606,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4445.865617393599,
        "convergence_std_reward": 49.41474083939848,
        "convergence_cv": 0.011114762588880951,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.23000884345959668,
        "exploration_entropy": 0.0034974856075566697,
        "policy_loss": 0.2987568477789561,
        "value_loss": 0.23826957742373148,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0034275358954055364
      }
    },
    {
      "episode": 53,
      "reward": 4418.787220774464,
      "timestamp": 14.973076343536377,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4448.899342950046,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4464.784341192889,
        "raw_reward": 4418.787220774464,
        "reward_service": 892.5528192940822,
        "reward_efficiency": 3526.234401480382,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4447.832496780814,
        "convergence_std_reward": 48.45656434383315,
        "convergence_cv": 0.010894422031158845,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2288587992422987,
        "exploration_entropy": 0.0034275358954055364,
        "policy_loss": 0.29606714844703674,
        "value_loss": 0.26652926206588745,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0033589851774974257
      }
    },
    {
      "episode": 54,
      "reward": 4618.41333261056,
      "timestamp": 15.303786277770996,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4451.040472816287,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4493.973849562247,
        "raw_reward": 4618.41333261056,
        "reward_service": 761.5528131219337,
        "reward_efficiency": 3856.860519488626,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4450.109570472161,
        "convergence_std_reward": 46.82703767576264,
        "convergence_cv": 0.010522670719497418,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.22771450524608722,
        "exploration_entropy": 0.0033589851774974257,
        "policy_loss": 0.29494460423787433,
        "value_loss": 0.17115427056948343,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0032918054739474773
      }
    },
    {
      "episode": 55,
      "reward": 4676.8680727314995,
      "timestamp": 15.596313714981079,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4453.3050521105515,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4528.723751964405,
        "raw_reward": 4676.8680727314995,
        "reward_service": 786.014391492412,
        "reward_efficiency": 3890.853681239087,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4452.190075609817,
        "convergence_std_reward": 46.84294261474385,
        "convergence_cv": 0.010521325868668751,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2265759327198568,
        "exploration_entropy": 0.0032918054739474773,
        "policy_loss": 0.3010737995306651,
        "value_loss": 0.17823716004689535,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.003225969364468528
      }
    },
    {
      "episode": 56,
      "reward": 4416.295278145931,
      "timestamp": 15.89916706085205,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4456.10884285676,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4507.362341938895,
        "raw_reward": 4416.295278145931,
        "reward_service": 691.7070386102832,
        "reward_efficiency": 3724.588239535647,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4454.889927371001,
        "convergence_std_reward": 46.94367370975969,
        "convergence_cv": 0.01053756085449746,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2254430530562575,
        "exploration_entropy": 0.003225969364468528,
        "policy_loss": 0.3036109407742818,
        "value_loss": 0.15547955532868704,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0031614499771791572
      }
    },
    {
      "episode": 57,
      "reward": 4290.647754468824,
      "timestamp": 16.18215012550354,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4458.245026182426,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4466.186570319582,
        "raw_reward": 4290.647754468824,
        "reward_service": 785.1626386068066,
        "reward_efficiency": 3505.4851158620177,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4457.453459493459,
        "convergence_std_reward": 45.63064583514945,
        "convergence_cv": 0.010236931523752773,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.22431583779097622,
        "exploration_entropy": 0.0031614499771791572,
        "policy_loss": 0.28873475392659503,
        "value_loss": 0.15514578421910605,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.003098220977635574
      }
    },
    {
      "episode": 58,
      "reward": 4457.134022209703,
      "timestamp": 16.459009885787964,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4459.141224073417,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4464.466586178705,
        "raw_reward": 4457.134022209703,
        "reward_service": 809.1129181242187,
        "reward_efficiency": 3648.021104085484,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4458.585036184675,
        "convergence_std_reward": 45.089189458148475,
        "convergence_cv": 0.0101128921153722,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.22319425860202133,
        "exploration_entropy": 0.003098220977635574,
        "policy_loss": 0.2976238429546356,
        "value_loss": 0.1183762972553571,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0030362565580828627
      }
    },
    {
      "episode": 59,
      "reward": 4191.387449417036,
      "timestamp": 16.69245982170105,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4458.476714482191,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4412.581550193988,
        "raw_reward": 4191.387449417036,
        "reward_service": 720.9730173751954,
        "reward_efficiency": 3470.414432041841,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4458.489039414358,
        "convergence_std_reward": 45.09347880998824,
        "convergence_cv": 0.010114071922426767,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.22207828730901122,
        "exploration_entropy": 0.0030362565580828627,
        "policy_loss": 0.28612227241198224,
        "value_loss": 0.11687761793533961,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0029755314269212054
      }
    },
    {
      "episode": 60,
      "reward": 4680.835800174149,
      "timestamp": 16.94267249107361,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4456.821434539784,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4463.549857690218,
        "raw_reward": 4680.835800174149,
        "reward_service": 765.7408008602267,
        "reward_efficiency": 3915.0949993139216,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4456.817483108723,
        "convergence_std_reward": 44.721133469280005,
        "convergence_cv": 0.010034320148575186,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.22096789587246615,
        "exploration_entropy": 0.0029755314269212054,
        "policy_loss": 0.2951223949591319,
        "value_loss": 0.2099189559618632,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0029160207983827814
      }
    },
    {
      "episode": 61,
      "reward": 4555.5097208853595,
      "timestamp": 17.182995796203613,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4455.1736758055795,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4481.022231697295,
        "raw_reward": 4555.5097208853595,
        "reward_service": 770.0109751578418,
        "reward_efficiency": 3785.4987457275174,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4455.180612497323,
        "convergence_std_reward": 42.7332940097308,
        "convergence_cv": 0.009591820787210898,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.21986305639310383,
        "exploration_entropy": 0.0029160207983827814,
        "policy_loss": 0.2875588039557139,
        "value_loss": 0.16785016655921936,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0028577003824151255
      }
    },
    {
      "episode": 62,
      "reward": 4577.090065326064,
      "timestamp": 17.419278383255005,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4453.582235153736,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4499.275120086761,
        "raw_reward": 4577.090065326064,
        "reward_service": 749.97337415708,
        "reward_efficiency": 3827.1166911689834,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4453.551213493774,
        "convergence_std_reward": 39.88933611407661,
        "convergence_cv": 0.008956748042599416,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2187637411111383,
        "exploration_entropy": 0.0028577003824151255,
        "policy_loss": 0.28652288516362506,
        "value_loss": 0.08650320023298264,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002800546374766823
      }
    },
    {
      "episode": 63,
      "reward": 4597.4710345843705,
      "timestamp": 17.657527208328247,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4453.094837562141,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4517.932343841307,
        "raw_reward": 4597.4710345843705,
        "reward_service": 747.5765465300879,
        "reward_efficiency": 3849.8944880542817,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4453.026088269766,
        "convergence_std_reward": 39.04121907364981,
        "convergence_cv": 0.008767345688023889,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2176699224055826,
        "exploration_entropy": 0.002800546374766823,
        "policy_loss": 0.28916773200035095,
        "value_loss": 0.03631997046371301,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0027445354472714865
      }
    },
    {
      "episode": 64,
      "reward": 4629.757689555171,
      "timestamp": 17.896735668182373,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4453.473608010595,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4539.179159526941,
        "raw_reward": 4629.757689555171,
        "reward_service": 747.1194335911817,
        "reward_efficiency": 3882.6382559639887,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4453.005729567367,
        "convergence_std_reward": 38.95925106879262,
        "convergence_cv": 0.008748978428235195,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2165815727935547,
        "exploration_entropy": 0.0027445354472714865,
        "policy_loss": 0.31697820623715717,
        "value_loss": 0.06333776377141476,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0026896447383260567
      }
    },
    {
      "episode": 65,
      "reward": 4274.816604787187,
      "timestamp": 18.157629251480103,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4454.784082790296,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4515.29456452651,
        "raw_reward": 4274.816604787187,
        "reward_service": 772.9944415000294,
        "reward_efficiency": 3501.8221632871587,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4454.209568417213,
        "convergence_std_reward": 40.676004366836914,
        "convergence_cv": 0.00913203650211074,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.21549866492958691,
        "exploration_entropy": 0.0026896447383260567,
        "policy_loss": 0.29969561100006104,
        "value_loss": 0.06773942895233631,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0026358518435595354
      }
    },
    {
      "episode": 66,
      "reward": 4403.023094044682,
      "timestamp": 18.43955421447754,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4455.87123828215,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4493.962985134963,
        "raw_reward": 4403.023094044682,
        "reward_service": 695.2757636894823,
        "reward_efficiency": 3707.747330355199,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4455.211641552082,
        "convergence_std_reward": 41.39059451670926,
        "convergence_cv": 0.00929037671985653,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.21442117160493898,
        "exploration_entropy": 0.0026358518435595354,
        "policy_loss": 0.2885015606880188,
        "value_loss": 0.026635782172282536,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0025831348066883445
      }
    },
    {
      "episode": 67,
      "reward": 4553.710114903344,
      "timestamp": 18.70511555671692,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4456.839421134418,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4505.314939790956,
        "raw_reward": 4553.710114903344,
        "reward_service": 709.2636229433984,
        "reward_efficiency": 3844.446491959946,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4456.315884845429,
        "convergence_std_reward": 41.73050579180554,
        "convergence_cv": 0.00936435092802067,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.21334906574691428,
        "exploration_entropy": 0.0025831348066883445,
        "policy_loss": 0.307235191265742,
        "value_loss": 0.15588387350241342,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0025314721105545775
      }
    },
    {
      "episode": 68,
      "reward": 4606.289129394013,
      "timestamp": 18.988495588302612,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4457.729570392744,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4524.500035815537,
        "raw_reward": 4606.289129394013,
        "reward_service": 738.8107282144922,
        "reward_efficiency": 3867.4784011795205,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4457.122596503755,
        "convergence_std_reward": 42.37854977512978,
        "convergence_cv": 0.009508051182700754,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2122823204181797,
        "exploration_entropy": 0.0025314721105545775,
        "policy_loss": 0.3022453188896179,
        "value_loss": 0.03713750963409742,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002480842668343486
      }
    },
    {
      "episode": 69,
      "reward": 4502.3798429484505,
      "timestamp": 19.22386074066162,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4458.709909315679,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4520.29719917079,
        "raw_reward": 4502.3798429484505,
        "reward_service": 724.6783975740527,
        "reward_efficiency": 3777.7014453743973,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4458.132934434385,
        "convergence_std_reward": 43.33377437044071,
        "convergence_cv": 0.009720162006774833,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.21122090881608882,
        "exploration_entropy": 0.002480842668343486,
        "policy_loss": 0.29575108488400775,
        "value_loss": 0.08375398566325505,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002431225814976616
      }
    },
    {
      "episode": 70,
      "reward": 4521.6837090553045,
      "timestamp": 19.525526762008667,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4459.1699069950355,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4520.560636048847,
        "raw_reward": 4521.6837090553045,
        "reward_service": 786.4662636321871,
        "reward_efficiency": 3735.2174454231176,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4458.826103628606,
        "convergence_std_reward": 44.00951464200264,
        "convergence_cv": 0.0098702020709414,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.21016480427200837,
        "exploration_entropy": 0.002431225814976616,
        "policy_loss": 0.2966473698616028,
        "value_loss": -0.025593099494775135,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0023826012986770837
      }
    },
    {
      "episode": 71,
      "reward": 4551.667952741666,
      "timestamp": 19.761930227279663,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4459.15687042741,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4526.4710262204835,
        "raw_reward": 4551.667952741666,
        "reward_service": 750.6083267529102,
        "reward_efficiency": 3801.0596259887557,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4458.839729821092,
        "convergence_std_reward": 44.02765295284832,
        "convergence_cv": 0.009874239851768545,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.20911398025064834,
        "exploration_entropy": 0.0023826012986770837,
        "policy_loss": 0.2830815215905507,
        "value_loss": 0.11112732688585918,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002334949272703542
      }
    },
    {
      "episode": 72,
      "reward": 4570.288028968649,
      "timestamp": 20.0654137134552,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4459.17780960201,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4534.796256742635,
        "raw_reward": 4570.288028968649,
        "reward_service": 804.432297484669,
        "reward_efficiency": 3765.8557314839786,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4458.819684640478,
        "convergence_std_reward": 43.996705927595485,
        "convergence_cv": 0.009867343610945553,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2080684103493951,
        "exploration_entropy": 0.002334949272703542,
        "policy_loss": 0.2986501653989156,
        "value_loss": -0.023939277976751328,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002288250287249471
      }
    },
    {
      "episode": 73,
      "reward": 4492.7521492084625,
      "timestamp": 20.363748788833618,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4459.705137521502,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4526.807876311142,
        "raw_reward": 4492.7521492084625,
        "reward_service": 727.2677784121279,
        "reward_efficiency": 3765.484370796335,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4459.471071689354,
        "convergence_std_reward": 44.8337400473794,
        "convergence_cv": 0.010053600376960245,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.20702806829764814,
        "exploration_entropy": 0.002288250287249471,
        "policy_loss": 0.2931823829809825,
        "value_loss": 0.018084308442970116,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002242485281504482
      }
    },
    {
      "episode": 74,
      "reward": 4685.528816899395,
      "timestamp": 20.674951791763306,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4460.931816090521,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4544.413492560128,
        "raw_reward": 4685.528816899395,
        "reward_service": 804.8218946088424,
        "reward_efficiency": 3880.706922290554,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4460.236421717164,
        "convergence_std_reward": 45.55199722972569,
        "convergence_cv": 0.010212910913854303,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2059929279561599,
        "exploration_entropy": 0.002242485281504482,
        "policy_loss": 0.2725573678811391,
        "value_loss": 0.05248637435336908,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0021976355758743923
      }
    },
    {
      "episode": 75,
      "reward": 4141.052136996816,
      "timestamp": 20.975764274597168,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4463.779271536154,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4502.449030348582,
        "raw_reward": 4141.052136996816,
        "reward_service": 743.4083134073925,
        "reward_efficiency": 3397.6438235894225,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4462.828143994498,
        "convergence_std_reward": 46.47926044948781,
        "convergence_cv": 0.010414754713786962,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2049629633163791,
        "exploration_entropy": 0.0021976355758743923,
        "policy_loss": 0.2971355617046356,
        "value_loss": -0.0037477711836496987,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0021536828643569043
      }
    },
    {
      "episode": 76,
      "reward": 4461.39103414633,
      "timestamp": 21.279865264892578,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4465.832126007017,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4494.648011070154,
        "raw_reward": 4461.39103414633,
        "reward_service": 745.9413783452725,
        "reward_efficiency": 3715.449655801058,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4465.123189967044,
        "convergence_std_reward": 45.411393737036114,
        "convergence_cv": 0.010170244314663866,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2039381484997972,
        "exploration_entropy": 0.0021536828643569043,
        "policy_loss": 0.2872927486896515,
        "value_loss": 0.020696488209068775,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002110609207069766
      }
    },
    {
      "episode": 77,
      "reward": 4571.8823570084305,
      "timestamp": 21.57676076889038,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4467.909997638494,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4509.322536798427,
        "raw_reward": 4571.8823570084305,
        "reward_service": 785.1150084599902,
        "reward_efficiency": 3786.76734854844,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4466.970580666698,
        "convergence_std_reward": 44.83992865022869,
        "convergence_cv": 0.010038106999024897,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2029184577572982,
        "exploration_entropy": 0.002110609207069766,
        "policy_loss": 0.3166329562664032,
        "value_loss": 0.056306141118208565,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0020683970229283706
      }
    },
    {
      "episode": 78,
      "reward": 4451.032896402783,
      "timestamp": 21.87385606765747,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4469.853723707914,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4498.247505123255,
        "raw_reward": 4451.032896402783,
        "reward_service": 724.2014931209668,
        "reward_efficiency": 3726.8314032818166,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4469.188812355788,
        "convergence_std_reward": 43.95982693842935,
        "convergence_cv": 0.009836198197063273,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.2019038654685117,
        "exploration_entropy": 0.0020683970229283706,
        "policy_loss": 0.3039434353510539,
        "value_loss": 0.015641193836927414,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.002027029082469803
      }
    },
    {
      "episode": 79,
      "reward": 4522.8005186024475,
      "timestamp": 22.275845766067505,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4470.915281528536,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4502.912577684301,
        "raw_reward": 4522.8005186024475,
        "reward_service": 702.037945645254,
        "reward_efficiency": 3820.7625729571937,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4470.444399243105,
        "convergence_std_reward": 43.88348746464289,
        "convergence_cv": 0.00981635907876918,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.20089434614116913,
        "exploration_entropy": 0.002027029082469803,
        "policy_loss": 0.3106734851996104,
        "value_loss": 0.000622196743885676,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001986488500820407
      }
    },
    {
      "episode": 80,
      "reward": 4547.694132453794,
      "timestamp": 22.585787057876587,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4471.810308246908,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4511.421073090504,
        "raw_reward": 4547.694132453794,
        "reward_service": 701.6119879345802,
        "reward_efficiency": 3846.0821445192137,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4471.248065352577,
        "convergence_std_reward": 44.14293275401156,
        "convergence_cv": 0.009872619928219237,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1998898744104633,
        "exploration_entropy": 0.001986488500820407,
        "policy_loss": 0.2769785324732463,
        "value_loss": 0.041713969161113106,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001946758730803999
      }
    },
    {
      "episode": 81,
      "reward": 4535.224253584037,
      "timestamp": 22.90852952003479,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4473.343226516955,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4515.943677384275,
        "raw_reward": 4535.224253584037,
        "reward_service": 758.4659317828224,
        "reward_efficiency": 3776.7583218012146,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4472.544262394797,
        "convergence_std_reward": 44.36105612353129,
        "convergence_cv": 0.009918528139904518,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.19889042503841098,
        "exploration_entropy": 0.001946758730803999,
        "policy_loss": 0.3050248324871063,
        "value_loss": 0.0212177565942208,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001907823556187919
      }
    },
    {
      "episode": 82,
      "reward": 4199.949941951438,
      "timestamp": 23.228139638900757,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.644698238666,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4481.877395985821,
        "raw_reward": 4199.949941951438,
        "reward_service": 775.4026305507617,
        "reward_efficiency": 3424.547311400676,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4474.67450649572,
        "convergence_std_reward": 43.7446769394505,
        "convergence_cv": 0.009776057873248202,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.19789597291321892,
        "exploration_entropy": 0.001907823556187919,
        "policy_loss": 0.29564515749613446,
        "value_loss": -0.053106418500343956,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0018696670850641606
      }
    },
    {
      "episode": 83,
      "reward": 4457.383959338834,
      "timestamp": 23.533979177474976,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4476.795111508775,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4477.223643022893,
        "raw_reward": 4457.383959338834,
        "reward_service": 666.0532650154491,
        "reward_efficiency": 3791.330694323385,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4476.2659702021865,
        "convergence_std_reward": 42.36517921528156,
        "convergence_cv": 0.009464401690449145,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.19690649304865282,
        "exploration_entropy": 0.0018696670850641606,
        "policy_loss": 0.30141525467236835,
        "value_loss": -0.010881210366884867,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0018322737433628773
      }
    },
    {
      "episode": 84,
      "reward": 4331.3460829582,
      "timestamp": 23.84912872314453,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4477.093447290934,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4449.506906610602,
        "raw_reward": 4331.3460829582,
        "reward_service": 834.7985265246289,
        "reward_efficiency": 3496.5475564335707,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4476.593399335599,
        "convergence_std_reward": 42.256046637684946,
        "convergence_cv": 0.009439330952852775,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.19592196058340955,
        "exploration_entropy": 0.0018322737433628773,
        "policy_loss": 0.294002244869868,
        "value_loss": -0.017217862109343212,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0017956282684956197
      }
    },
    {
      "episode": 85,
      "reward": 4080.077234416578,
      "timestamp": 24.104262590408325,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4476.8796953829515,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4379.315268893738,
        "raw_reward": 4080.077234416578,
        "reward_service": 663.9367266617188,
        "reward_efficiency": 3416.1405077548593,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4476.6095072565595,
        "convergence_std_reward": 42.307711864848606,
        "convergence_cv": 0.009450838139057704,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1949423507804925,
        "exploration_entropy": 0.0017956282684956197,
        "policy_loss": 0.29090283314387005,
        "value_loss": 0.05376463383436203,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0017597157031257072
      }
    },
    {
      "episode": 86,
      "reward": 4518.3076447903895,
      "timestamp": 24.36396598815918,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4476.422370834102,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4405.723820314102,
        "raw_reward": 4518.3076447903895,
        "reward_service": 735.8027758,
        "reward_efficiency": 3782.5048689903892,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4476.067095182096,
        "convergence_std_reward": 43.28702359681302,
        "convergence_cv": 0.009670771835258206,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.19396763902659003,
        "exploration_entropy": 0.0017597157031257072,
        "policy_loss": 0.304983655611674,
        "value_loss": -0.008595156793793043,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001724521389063193
      }
    },
    {
      "episode": 87,
      "reward": 4539.4730712703995,
      "timestamp": 24.636740684509277,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4476.232531735985,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4431.136177995799,
        "raw_reward": 4539.4730712703995,
        "reward_service": 729.410490004707,
        "reward_efficiency": 3810.0625812656917,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4475.820092131665,
        "convergence_std_reward": 43.619977227957555,
        "convergence_cv": 0.009745694940831056,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1929978008314571,
        "exploration_entropy": 0.001724521389063193,
        "policy_loss": 0.28263192375500995,
        "value_loss": -0.051919045547644295,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0016900309612819292
      }
    },
    {
      "episode": 88,
      "reward": 4130.555725438855,
      "timestamp": 24.89467453956604,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.743192369202,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4374.0258920099795,
        "raw_reward": 4130.555725438855,
        "reward_service": 679.9993627923828,
        "reward_efficiency": 3450.5563626464723,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4475.531950456298,
        "convergence_std_reward": 44.010263138626314,
        "convergence_cv": 0.00983352674627634,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1920328118272998,
        "exploration_entropy": 0.0016900309612819292,
        "policy_loss": 0.2953456739584605,
        "value_loss": -0.055133609722057976,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0016562303420562905
      }
    },
    {
      "episode": 89,
      "reward": 4568.761690185629,
      "timestamp": 25.17617440223694,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.02714637489,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4411.025693663353,
        "raw_reward": 4568.761690185629,
        "reward_service": 772.508179407812,
        "reward_efficiency": 3796.2535107778176,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4474.7481798534745,
        "convergence_std_reward": 45.28262102136733,
        "convergence_cv": 0.010119590913571836,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.19107264776816332,
        "exploration_entropy": 0.0016562303420562905,
        "policy_loss": 0.2853570580482483,
        "value_loss": -0.05362533529599508,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0016231057352151647
      }
    },
    {
      "episode": 90,
      "reward": 4509.299430170887,
      "timestamp": 25.44363522529602,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4474.4679965437135,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4429.697703599785,
        "raw_reward": 4509.299430170887,
        "reward_service": 784.0400687122656,
        "reward_efficiency": 3725.259361458622,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4474.171534088389,
        "convergence_std_reward": 45.84252996081718,
        "convergence_cv": 0.010246037643292454,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1901172845293225,
        "exploration_entropy": 0.0016231057352151647,
        "policy_loss": 0.2782683273156484,
        "value_loss": -0.030158786724011104,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0015906436205108613
      }
    },
    {
      "episode": 91,
      "reward": 4157.825560997655,
      "timestamp": 25.70054340362549,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4473.7963801101505,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4378.041996505381,
        "raw_reward": 4157.825560997655,
        "reward_service": 707.2802740240429,
        "reward_efficiency": 3450.5452869736123,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4473.571897296538,
        "convergence_std_reward": 46.41698386721371,
        "convergence_cv": 0.01037582158794952,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18916669810667588,
        "exploration_entropy": 0.0015906436205108613,
        "policy_loss": 0.30085642139116925,
        "value_loss": -0.04606268865366777,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0015588307481006441
      }
    },
    {
      "episode": 92,
      "reward": 4638.66027317128,
      "timestamp": 25.94055461883545,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4474.142463284781,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4427.559469071902,
        "raw_reward": 4638.66027317128,
        "reward_service": 800.9206782864844,
        "reward_efficiency": 3837.7395948847957,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4473.344348989534,
        "convergence_std_reward": 46.77539889837578,
        "convergence_cv": 0.010456471769033762,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1882208646161425,
        "exploration_entropy": 0.0015588307481006441,
        "policy_loss": 0.28917307655016583,
        "value_loss": -0.06723332529266675,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0015276541331386312
      }
    },
    {
      "episode": 93,
      "reward": 4390.402410683077,
      "timestamp": 26.19378089904785,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.003211751877,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4420.499627978024,
        "raw_reward": 4390.402410683077,
        "reward_service": 617.1621218368375,
        "reward_efficiency": 3773.2402888462384,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4474.464033143869,
        "convergence_std_reward": 44.887857350382056,
        "convergence_cv": 0.010032007636642627,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1872797602930618,
        "exploration_entropy": 0.0015276541331386312,
        "policy_loss": 0.26962848504384357,
        "value_loss": 0.06218342607220014,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0014971010504758585
      }
    },
    {
      "episode": 94,
      "reward": 4504.848394612655,
      "timestamp": 26.430426359176636,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.2527615179315,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4436.525893638604,
        "raw_reward": 4504.848394612655,
        "reward_service": 670.0933048813085,
        "reward_efficiency": 3834.755089731346,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4474.786140141929,
        "convergence_std_reward": 44.46015527955668,
        "convergence_cv": 0.009935705056542997,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18634336149159647,
        "exploration_entropy": 0.0014971010504758585,
        "policy_loss": 0.28512538472811383,
        "value_loss": 0.06195855885744095,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0014671590294663413
      }
    },
    {
      "episode": 95,
      "reward": 4626.957208610228,
      "timestamp": 26.6639666557312,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.6019740945085,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4472.707843483213,
        "raw_reward": 4626.957208610228,
        "reward_service": 768.9881517500195,
        "reward_efficiency": 3857.969056860208,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4475.020219804439,
        "convergence_std_reward": 44.29431543907988,
        "convergence_cv": 0.009898126324223752,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18541164468413848,
        "exploration_entropy": 0.0014671590294663413,
        "policy_loss": 0.31800838311513263,
        "value_loss": -0.006296815350651741,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0014378158488770143
      }
    },
    {
      "episode": 96,
      "reward": 4545.833594761755,
      "timestamp": 26.919570446014404,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4476.194802872147,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4486.6017362261355,
        "raw_reward": 4545.833594761755,
        "reward_service": 748.3330332376446,
        "reward_efficiency": 3797.5005615241116,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4475.594860104205,
        "convergence_std_reward": 44.12263126480719,
        "convergence_cv": 0.009858495383065099,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18448458646071778,
        "exploration_entropy": 0.0014378158488770143,
        "policy_loss": 0.2925692598025004,
        "value_loss": -0.05968367805083593,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001409059531899474
      }
    },
    {
      "episode": 97,
      "reward": 4545.527803454385,
      "timestamp": 27.221247911453247,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4477.439293710765,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4497.797688999503,
        "raw_reward": 4545.527803454385,
        "reward_service": 760.2318089183497,
        "reward_efficiency": 3785.295994536036,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4476.5298895424185,
        "convergence_std_reward": 43.87877380604564,
        "convergence_cv": 0.009801961539126647,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18356216352841417,
        "exploration_entropy": 0.001409059531899474,
        "policy_loss": 0.2804919083913167,
        "value_loss": -0.026429904624819756,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0013808783412614844
      }
    },
    {
      "episode": 98,
      "reward": 4672.673095725691,
      "timestamp": 27.509614944458008,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4479.151980077757,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4531.024016277479,
        "raw_reward": 4672.673095725691,
        "reward_service": 819.9146671700878,
        "reward_efficiency": 3852.7584285556036,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4478.286993405587,
        "convergence_std_reward": 43.27216390956371,
        "convergence_cv": 0.009662659845892698,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1826443527107721,
        "exploration_entropy": 0.0013808783412614844,
        "policy_loss": 0.3148932258288066,
        "value_loss": -0.037194483603040375,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0013532607744362547
      }
    },
    {
      "episode": 99,
      "reward": 4444.520637660752,
      "timestamp": 27.789652824401855,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4480.549771472059,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4514.588374340301,
        "raw_reward": 4444.520637660752,
        "reward_service": 675.8773885205662,
        "reward_efficiency": 3768.6432491401856,
        "reward_cost": 0.0,
        "convergence_status": "converging_with_improvement",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.9,
        "convergence_mean_reward": 4479.802256539673,
        "convergence_std_reward": 43.61676986652979,
        "convergence_cv": 0.009736315883777564,
        "convergence_improvement_rate": 0,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18173113094721824,
        "exploration_entropy": 0.0013532607744362547,
        "policy_loss": 0.271910160779953,
        "value_loss": -0.03928464464843273,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0013261955589475296
      }
    },
    {
      "episode": 100,
      "reward": 4636.5208005848,
      "timestamp": 28.09165048599243,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4481.916841898194,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4537.755535326756,
        "raw_reward": 4636.5208005848,
        "reward_service": 756.1851847559376,
        "reward_efficiency": 3880.335615828863,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4481.078962884088,
        "convergence_std_reward": 43.867729968863316,
        "convergence_cv": 0.009789546297267077,
        "convergence_improvement_rate": 0.009896404231207936,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.18082247529248216,
        "exploration_entropy": 0.0013261955589475296,
        "policy_loss": 0.31215400497118634,
        "value_loss": 0.003982743248343468,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001299671647768579
      }
    },
    {
      "episode": 101,
      "reward": 4554.674484099817,
      "timestamp": 29.130008459091187,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4483.241862049549,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4540.970135593638,
        "raw_reward": 4554.674484099817,
        "reward_service": 744.6703493130664,
        "reward_efficiency": 3810.0041347867514,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4482.506335846383,
        "convergence_std_reward": 44.54195622651287,
        "convergence_cv": 0.009936841777626288,
        "convergence_improvement_rate": 0.00907373220756014,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17991836291601976,
        "exploration_entropy": 0.001299671647768579,
        "policy_loss": 0.30143605669339496,
        "value_loss": -0.04062750438849131,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0012736782148132075
      }
    },
    {
      "episode": 102,
      "reward": 4458.506481400767,
      "timestamp": 29.432542085647583,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4484.305689220387,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4525.302041296993,
        "raw_reward": 4458.506481400767,
        "reward_service": 694.8384748037109,
        "reward_efficiency": 3763.668006597057,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4483.608641977633,
        "convergence_std_reward": 45.18915927575727,
        "convergence_cv": 0.010078747474227635,
        "convergence_improvement_rate": 0.008489465906565375,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17901877110143966,
        "exploration_entropy": 0.0012736782148132075,
        "policy_loss": 0.27698907256126404,
        "value_loss": -0.04810939480861028,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0012482046505169432
      }
    },
    {
      "episode": 103,
      "reward": 4361.09709612708,
      "timestamp": 29.729939222335815,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.085417508977,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4494.10310171471,
        "raw_reward": 4361.09709612708,
        "reward_service": 636.6257491521583,
        "reward_efficiency": 3724.471346974921,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.495788642976,
        "convergence_std_reward": 45.41494784129671,
        "convergence_cv": 0.010127102350349053,
        "convergence_improvement_rate": 0.008242956965824888,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17812367724593245,
        "exploration_entropy": 0.0012482046505169432,
        "policy_loss": 0.2985921998818715,
        "value_loss": 0.013725011919935545,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0012232405575066043
      }
    },
    {
      "episode": 104,
      "reward": 4621.836214923804,
      "timestamp": 30.099706649780273,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.6200009629965,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4518.3723932244375,
        "raw_reward": 4621.836214923804,
        "reward_service": 735.942855847422,
        "reward_efficiency": 3885.893359076383,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.056267975203,
        "convergence_std_reward": 45.4319355535597,
        "convergence_cv": 0.01012962443257599,
        "convergence_improvement_rate": 0.007852997089088293,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17723305885970278,
        "exploration_entropy": 0.0012232405575066043,
        "policy_loss": 0.28742390871047974,
        "value_loss": -0.06930613766113917,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.001198775746356472
      }
    },
    {
      "episode": 105,
      "reward": 4340.574380181619,
      "timestamp": 30.37591862678528,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.386587363816,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4484.590770746302,
        "raw_reward": 4340.574380181619,
        "reward_service": 653.0100966974521,
        "reward_efficiency": 3687.5642834841665,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.183546612236,
        "convergence_std_reward": 45.49022467186645,
        "convergence_cv": 0.010142332905467445,
        "convergence_improvement_rate": 0.00741061599844205,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17634689356540426,
        "exploration_entropy": 0.001198775746356472,
        "policy_loss": 0.2877993683020274,
        "value_loss": -0.07740137726068497,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0011748002314293426
      }
    },
    {
      "episode": 106,
      "reward": 4502.398850067169,
      "timestamp": 30.613839387893677,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4484.764400845733,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4487.974305817267,
        "raw_reward": 4502.398850067169,
        "reward_service": 726.4003330447166,
        "reward_efficiency": 3775.998517022452,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.431123541012,
        "convergence_std_reward": 45.15997497910324,
        "convergence_cv": 0.010070391034001178,
        "convergence_improvement_rate": 0.006631184305701676,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17546515909757723,
        "exploration_entropy": 0.0011748002314293426,
        "policy_loss": 0.29567448298136395,
        "value_loss": -0.028827776511510212,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0011513042268007558
      }
    },
    {
      "episode": 107,
      "reward": 4364.36828262105,
      "timestamp": 30.851665496826172,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4484.562857251848,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4464.489161409986,
        "raw_reward": 4364.36828262105,
        "reward_service": 657.5788350748145,
        "reward_efficiency": 3706.789447546235,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.136471382853,
        "convergence_std_reward": 45.11058914347793,
        "convergence_cv": 0.010060039303301216,
        "convergence_improvement_rate": 0.0059861560265008955,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17458783330208935,
        "exploration_entropy": 0.0011513042268007558,
        "policy_loss": 0.30506380399068195,
        "value_loss": 0.020829909791549046,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0011282781422647407
      }
    },
    {
      "episode": 108,
      "reward": 4659.187374708492,
      "timestamp": 31.097121715545654,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4484.936410596473,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4501.4818219367025,
        "raw_reward": 4659.187374708492,
        "reward_service": 832.6659286097754,
        "reward_efficiency": 3826.521446098717,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.30627396607,
        "convergence_std_reward": 45.07792968134236,
        "convergence_cv": 0.010052375312329847,
        "convergence_improvement_rate": 0.005768923901338252,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1737148941355789,
        "exploration_entropy": 0.0011282781422647407,
        "policy_loss": 0.28194090723991394,
        "value_loss": -0.061569741616646446,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0011057125794194458
      }
    },
    {
      "episode": 109,
      "reward": 4519.3402239093175,
      "timestamp": 31.402108907699585,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4486.258590389101,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4504.8749183115,
        "raw_reward": 4519.3402239093175,
        "reward_service": 755.6826981046875,
        "reward_efficiency": 3763.65752580463,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.3375162199645,
        "convergence_std_reward": 44.84792665565085,
        "convergence_cv": 0.009998785262752448,
        "convergence_improvement_rate": 0.006021877943011282,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.172846319664901,
        "exploration_entropy": 0.0011057125794194458,
        "policy_loss": 0.3006046215693156,
        "value_loss": -0.028526026134689648,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0010835983278310569
      }
    },
    {
      "episode": 110,
      "reward": 4647.668825705972,
      "timestamp": 31.703741788864136,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4487.853537066279,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4532.005760716449,
        "raw_reward": 4647.668825705972,
        "reward_service": 795.8109954767189,
        "reward_efficiency": 3851.8578302292535,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4487.0579232397295,
        "convergence_std_reward": 44.154265124228246,
        "convergence_cv": 0.009840359959594223,
        "convergence_improvement_rate": 0.006785209456213473,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.1719820880665765,
        "exploration_entropy": 0.0010835983278310569,
        "policy_loss": 0.31687795122464496,
        "value_loss": -0.07116162528594334,
        "learning_rate": 0.0003,
        "entropy_coefficient": 0.0010619263612744356
      }
    },
    {
      "episode": 111,
      "reward": 4484.850518122985,
      "timestamp": 32.24952816963196,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4488.944424915227,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4523.046264623691,
        "raw_reward": 4484.850518122985,
        "reward_service": 758.1359328598045,
        "reward_efficiency": 3726.7145852631793,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4488.2879261944645,
        "convergence_std_reward": 44.46235536969297,
        "convergence_cv": 0.009906306391397616,
        "convergence_improvement_rate": 0.007431194507417214,
        "current_learning_rate": 0.0003,
        "exploration_epsilon": 0.17112217762624363,
        "exploration_entropy": 0.0010619263612744356,
        "policy_loss": 0.3031351963678996,
        "value_loss": -0.03083661012351513,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.0010406878340489469
      }
    },
    {
      "episode": 112,
      "reward": 4510.30217035257,
      "timestamp": 32.568989276885986,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4489.567281717956,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4520.624886712178,
        "raw_reward": 4510.30217035257,
        "reward_service": 720.026771005117,
        "reward_efficiency": 3790.275399347453,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4489.019594925093,
        "convergence_std_reward": 44.711029418603694,
        "convergence_cv": 0.009960087826114685,
        "convergence_improvement_rate": 0.007964067264760113,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1702665667381124,
        "exploration_entropy": 0.0010406878340489469,
        "policy_loss": 0.27611324191093445,
        "value_loss": -0.051156160732110344,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001019874077367968
      }
    },
    {
      "episode": 113,
      "reward": 4369.395290576061,
      "timestamp": 32.81638860702515,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4489.495426029197,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4491.891263446315,
        "raw_reward": 4369.395290576061,
        "reward_service": 685.7046992166797,
        "reward_efficiency": 3683.690591359381,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4489.197164746967,
        "convergence_std_reward": 44.7958522301429,
        "convergence_cv": 0.009978588728942096,
        "convergence_improvement_rate": 0.008122808121983372,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16941523390442184,
        "exploration_entropy": 0.001019874077367968,
        "policy_loss": 0.2623911201953888,
        "value_loss": 0.02631963137537241,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 114,
      "reward": 4455.807252778358,
      "timestamp": 33.071686029434204,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4488.678785708533,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4485.035301419403,
        "raw_reward": 4455.807252778358,
        "reward_service": 657.7803676808886,
        "reward_efficiency": 3798.026885097469,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4488.528433782685,
        "convergence_std_reward": 44.52090265560205,
        "convergence_cv": 0.009918819344112359,
        "convergence_improvement_rate": 0.00797724197376443,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16856815773489972,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28275060653686523,
        "value_loss": -0.04716618669529756,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 115,
      "reward": 4554.0831503645695,
      "timestamp": 33.31618523597717,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4487.677300678193,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4498.154392718985,
        "raw_reward": 4554.0831503645695,
        "reward_service": 739.813250687295,
        "reward_efficiency": 3814.2698996772756,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4487.486252686441,
        "convergence_std_reward": 43.897615718812276,
        "convergence_cv": 0.009782228456417644,
        "convergence_improvement_rate": 0.00747083938420366,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16772531694622522,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2887166639169057,
        "value_loss": -0.07449772953987122,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 116,
      "reward": 4211.871944454142,
      "timestamp": 33.563864946365356,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4487.457911035713,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4464.975336264274,
        "raw_reward": 4211.871944454142,
        "reward_service": 712.3847463872364,
        "reward_efficiency": 3499.487198066905,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4487.205156147125,
        "convergence_std_reward": 43.793747043850715,
        "convergence_cv": 0.009759693510749482,
        "convergence_improvement_rate": 0.007181143606434222,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1668866903614941,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2942056159178416,
        "value_loss": -0.09691841651995976,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 117,
      "reward": 4384.028858077658,
      "timestamp": 33.81094527244568,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4486.596770842389,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4449.595505408817,
        "raw_reward": 4384.028858077658,
        "reward_service": 668.3632049324219,
        "reward_efficiency": 3715.6656531452363,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4486.484709561756,
        "convergence_std_reward": 43.925308882435246,
        "convergence_cv": 0.009790584773156601,
        "convergence_improvement_rate": 0.006769902649612971,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1660522569096866,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29217277963956195,
        "value_loss": 0.03051312391956647,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 118,
      "reward": 4731.163791211073,
      "timestamp": 34.047348737716675,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.843570347145,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4503.093479711246,
        "raw_reward": 4731.163791211073,
        "reward_service": 849.0039437525488,
        "reward_efficiency": 3882.159847458523,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.550914970313,
        "convergence_std_reward": 43.94334535693636,
        "convergence_cv": 0.00979664397750509,
        "convergence_improvement_rate": 0.006378177366908853,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16522199562513817,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2991354465484619,
        "value_loss": -0.04420716129243374,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 119,
      "reward": 4615.661333459836,
      "timestamp": 34.286125898361206,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.684815318291,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4524.481371923478,
        "raw_reward": 4615.661333459836,
        "reward_service": 790.8053626651074,
        "reward_efficiency": 3824.8559707947293,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.257471894843,
        "convergence_std_reward": 43.7451305554446,
        "convergence_cv": 0.009753092398721097,
        "convergence_improvement_rate": 0.006084281886470737,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1643958856470125,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3004771371682485,
        "value_loss": -0.014534308885534605,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 120,
      "reward": 4674.817276892363,
      "timestamp": 34.53016686439514,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4486.066397563513,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4553.045193867567,
        "raw_reward": 4674.817276892363,
        "reward_service": 806.1402633985936,
        "reward_efficiency": 3868.677013493769,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.490104744982,
        "convergence_std_reward": 43.97446096925927,
        "convergence_cv": 0.009803713739718391,
        "convergence_improvement_rate": 0.005980049568355462,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16357390621877743,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3098066548506419,
        "value_loss": -0.08291603873173396,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 121,
      "reward": 4569.252455389867,
      "timestamp": 34.77268052101135,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4486.686288609546,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4556.1245735568045,
        "raw_reward": 4569.252455389867,
        "reward_service": 766.6530166238379,
        "reward_efficiency": 3802.599438766029,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4486.124895846186,
        "convergence_std_reward": 44.72094120841038,
        "convergence_cv": 0.009968724065132162,
        "convergence_improvement_rate": 0.006119342178326811,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16275603668768354,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2834511399269104,
        "value_loss": -0.07116246471802394,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 122,
      "reward": 4561.018755641922,
      "timestamp": 35.01487588882446,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4487.201513914947,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4557.054468152978,
        "raw_reward": 4561.018755641922,
        "reward_service": 725.2170199736231,
        "reward_efficiency": 3835.8017356682994,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4486.679043972249,
        "convergence_std_reward": 45.409891950364646,
        "convergence_cv": 0.010121047551054894,
        "convergence_improvement_rate": 0.006248146662611077,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16194225650424512,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2793509264787038,
        "value_loss": -0.05285931875308355,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 123,
      "reward": 4172.019654223072,
      "timestamp": 35.25966000556946,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4487.765594039556,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4520.11823242004,
        "raw_reward": 4172.019654223072,
        "reward_service": 720.3795337421192,
        "reward_efficiency": 3451.6401204809526,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4487.183666148657,
        "convergence_std_reward": 46.05326246368789,
        "convergence_cv": 0.010263288933571852,
        "convergence_improvement_rate": 0.006214323181785871,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1611325452217239,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2984478374322255,
        "value_loss": -0.04075061281522115,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 124,
      "reward": 4127.405896058718,
      "timestamp": 35.484984397888184,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4486.807853773676,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4445.502888511389,
        "raw_reward": 4127.405896058718,
        "reward_service": 691.6833540687498,
        "reward_efficiency": 3435.7225419899682,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4486.6378995768055,
        "convergence_std_reward": 45.813627008867684,
        "convergence_cv": 0.01021112646803723,
        "convergence_improvement_rate": 0.00591930009160292,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.16032688249561527,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2945765554904938,
        "value_loss": -0.014332015067338943,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 125,
      "reward": 4711.966899764259,
      "timestamp": 35.72777986526489,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.398754751988,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4496.131050649435,
        "raw_reward": 4711.966899764259,
        "reward_service": 786.1441820982812,
        "reward_efficiency": 3925.822717665979,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.944244025475,
        "convergence_std_reward": 45.28065299063341,
        "convergence_cv": 0.010096146245508647,
        "convergence_improvement_rate": 0.0049556243972196405,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1595252480831372,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2844153344631195,
        "value_loss": -0.07037075733145078,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 126,
      "reward": 4543.182334022062,
      "timestamp": 35.99729299545288,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.448613717155,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4505.070794490234,
        "raw_reward": 4543.182334022062,
        "reward_service": 725.6606064939551,
        "reward_efficiency": 3817.5217275281057,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.905993711067,
        "convergence_std_reward": 45.26868178636667,
        "convergence_cv": 0.010093563131500284,
        "convergence_improvement_rate": 0.004430516897825864,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15872762184272152,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3026942511399587,
        "value_loss": -0.04676516850789388,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 127,
      "reward": 4532.077662470167,
      "timestamp": 36.30143189430237,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.556614429641,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4510.202099406421,
        "raw_reward": 4532.077662470167,
        "reward_service": 722.2962429910449,
        "reward_efficiency": 3809.781419479122,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4485.06422190151,
        "convergence_std_reward": 45.32685211328578,
        "convergence_cv": 0.010106176828404206,
        "convergence_improvement_rate": 0.004050539601295387,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15793398373350792,
        "exploration_entropy": 0.001,
        "policy_loss": 0.26421940326690674,
        "value_loss": -0.03952649049460888,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 128,
      "reward": 4208.560893183051,
      "timestamp": 36.555354833602905,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.087502550145,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4452.890270223981,
        "raw_reward": 4208.560893183051,
        "reward_service": 729.4341436738184,
        "reward_efficiency": 3479.126749509233,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.838461587842,
        "convergence_std_reward": 45.29639330771394,
        "convergence_cv": 0.010099894053191139,
        "convergence_improvement_rate": 0.00350167555883697,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15714431381484037,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2959739565849304,
        "value_loss": -0.05953888346751531,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 129,
      "reward": 4384.787000496422,
      "timestamp": 36.79529857635498,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4483.995045285746,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4439.950648975744,
        "raw_reward": 4384.787000496422,
        "reward_service": 855.8813619369139,
        "reward_efficiency": 3528.905638559508,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4483.838660606649,
        "convergence_std_reward": 45.515688971658086,
        "convergence_cv": 0.010151054133937094,
        "convergence_improvement_rate": 0.0029961811773816906,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15635859224576618,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29061631361643475,
        "value_loss": -0.08195585509141286,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 130,
      "reward": 4584.879714765835,
      "timestamp": 37.060622453689575,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4482.936101733616,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4467.487171475862,
        "raw_reward": 4584.879714765835,
        "reward_service": 790.1781248842103,
        "reward_efficiency": 3794.7015898816253,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4482.6795695434985,
        "convergence_std_reward": 45.70614404856621,
        "convergence_cv": 0.010196165784212136,
        "convergence_improvement_rate": 0.0025566696420856795,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15557679928453735,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29194995760917664,
        "value_loss": -0.09076172361771266,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 131,
      "reward": 4542.626120945077,
      "timestamp": 37.297696352005005,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4482.160095239274,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4481.763571875013,
        "raw_reward": 4542.626120945077,
        "reward_service": 742.5721454172364,
        "reward_efficiency": 3800.0539755278405,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4481.852227280181,
        "convergence_std_reward": 45.5337059133521,
        "convergence_cv": 0.010159573230950611,
        "convergence_improvement_rate": 0.002081134213392833,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15479891528811468,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2967219849427541,
        "value_loss": -0.02210205855468909,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 132,
      "reward": 4492.232648908064,
      "timestamp": 37.551674127578735,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4481.552251457715,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4483.752696511293,
        "raw_reward": 4492.232648908064,
        "reward_service": 761.1133373748438,
        "reward_efficiency": 3731.1193115332203,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4481.206504334309,
        "convergence_std_reward": 45.293044782908005,
        "convergence_cv": 0.010107332643362832,
        "convergence_improvement_rate": 0.0014597704993081534,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15402492071167412,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2922653754552205,
        "value_loss": -0.08037643879652023,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 133,
      "reward": 4583.777047190489,
      "timestamp": 37.79256868362427,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4481.838793042876,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4502.75732314034,
        "raw_reward": 4583.777047190489,
        "reward_service": 766.9833855520313,
        "reward_efficiency": 3816.7936616384577,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4481.368528132145,
        "convergence_std_reward": 45.31045172503876,
        "convergence_cv": 0.010110851504534567,
        "convergence_improvement_rate": 0.0011399139291376867,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15325479610811574,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3058032989501953,
        "value_loss": -0.08935879170894623,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 134,
      "reward": 4799.985165418088,
      "timestamp": 38.02856135368347,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4482.75406776498,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4537.558177351286,
        "raw_reward": 4799.985165418088,
        "reward_service": 863.3097595192287,
        "reward_efficiency": 3936.675405898859,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4482.081502294372,
        "convergence_std_reward": 45.39525735906792,
        "convergence_cv": 0.010128164187962701,
        "convergence_improvement_rate": 0.0012259552005743505,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15248852212757516,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3107750614484151,
        "value_loss": -0.06014443809787432,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 135,
      "reward": 4553.525964601313,
      "timestamp": 38.276854515075684,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4485.285888098682,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4540.592056928792,
        "raw_reward": 4553.525964601313,
        "reward_service": 731.6270313789648,
        "reward_efficiency": 3821.8989332223487,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.2279251686305,
        "convergence_std_reward": 45.393112993062815,
        "convergence_cv": 0.010122838033786117,
        "convergence_improvement_rate": 0.0017018276666127719,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.15172607951693728,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2889844477176666,
        "value_loss": -0.051006268709897995,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 136,
      "reward": 4365.130305478915,
      "timestamp": 38.51672554016113,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4487.8825156046905,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4507.254324153315,
        "raw_reward": 4365.130305478915,
        "reward_service": 858.0201525972657,
        "reward_efficiency": 3507.110152881649,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4487.139006801984,
        "convergence_std_reward": 43.657722383321854,
        "convergence_cv": 0.009729523047345267,
        "convergence_improvement_rate": 0.002473580351779225,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1509674491193526,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2944958408673604,
        "value_loss": -0.07950318356355031,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 137,
      "reward": 4680.2996617126,
      "timestamp": 38.80382966995239,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4489.99171785468,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4540.132938289579,
        "raw_reward": 4680.2996617126,
        "reward_service": 824.7802816173923,
        "reward_efficiency": 3855.519380095207,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4489.208912965371,
        "convergence_std_reward": 42.51096125768613,
        "convergence_cv": 0.009469588535946588,
        "convergence_improvement_rate": 0.0029913670697452096,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1502126118737558,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3060874044895172,
        "value_loss": -0.11148875703414281,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 138,
      "reward": 4124.058050108473,
      "timestamp": 39.11937141418457,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4492.793691547435,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4499.464940650893,
        "raw_reward": 4124.058050108473,
        "reward_service": 688.1347924669532,
        "reward_efficiency": 3435.9232576415197,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4491.699867414686,
        "convergence_std_reward": 41.74351923025601,
        "convergence_cv": 0.009293479186596358,
        "convergence_improvement_rate": 0.0036125129118428434,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14946154881438703,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28914857904116315,
        "value_loss": -0.11418009052673976,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 139,
      "reward": 4692.147642937608,
      "timestamp": 39.421815395355225,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4495.298366328475,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4536.07465408537,
        "raw_reward": 4692.147642937608,
        "reward_service": 829.1898314173827,
        "reward_efficiency": 3862.957811520225,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4494.206595291616,
        "convergence_std_reward": 38.84898803675755,
        "convergence_cv": 0.00864423724478041,
        "convergence_improvement_rate": 0.004348493961235383,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14871424107031508,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28197861711184186,
        "value_loss": -0.11412631968657176,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 140,
      "reward": 4554.828106447629,
      "timestamp": 39.72819542884827,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4497.640304968625,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4539.637810034199,
        "raw_reward": 4554.828106447629,
        "reward_service": 739.410875551924,
        "reward_efficiency": 3815.4172308957045,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4496.628054215911,
        "convergence_std_reward": 37.64367916352728,
        "convergence_cv": 0.008371535005710253,
        "convergence_improvement_rate": 0.005019145993046354,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14797066986496352,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29061852892239887,
        "value_loss": -0.08759211003780365,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 141,
      "reward": 4614.765603711739,
      "timestamp": 40.02627229690552,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4500.533106969873,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4553.912090832931,
        "raw_reward": 4614.765603711739,
        "reward_service": 759.8382087353416,
        "reward_efficiency": 3854.927394976396,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4499.173856280879,
        "convergence_std_reward": 36.34846971835027,
        "convergence_cv": 0.00807892090402498,
        "convergence_improvement_rate": 0.00572293450783983,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14723081651563868,
        "exploration_entropy": 0.001,
        "policy_loss": 0.302188237508138,
        "value_loss": -0.07492385432124138,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 142,
      "reward": 4625.270650132693,
      "timestamp": 40.31381940841675,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4503.671989421691,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4567.470217099886,
        "raw_reward": 4625.270650132693,
        "reward_service": 776.92418226582,
        "reward_efficiency": 3848.3464678668724,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4502.501998450063,
        "convergence_std_reward": 33.78267470076138,
        "convergence_cv": 0.007503089329530714,
        "convergence_improvement_rate": 0.006518087405257562,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1464946624330605,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2860569854577382,
        "value_loss": -0.09656571348508199,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 143,
      "reward": 4356.447331666426,
      "timestamp": 40.71393036842346,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4506.122472728162,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4527.375868867529,
        "raw_reward": 4356.447331666426,
        "reward_service": 797.3963052322265,
        "reward_efficiency": 3559.0510264341997,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4505.1263475835785,
        "convergence_std_reward": 32.82983075464587,
        "convergence_cv": 0.0072872164333981125,
        "convergence_improvement_rate": 0.0068527345873346576,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1457621891208952,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2928200562795003,
        "value_loss": -0.07023821398615837,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 144,
      "reward": 4543.452794450863,
      "timestamp": 41.0245897769928,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4508.12345386385,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4530.430484728362,
        "raw_reward": 4543.452794450863,
        "reward_service": 746.1931224033594,
        "reward_efficiency": 3797.259672047504,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4507.195600560317,
        "convergence_std_reward": 30.89480979020051,
        "convergence_cv": 0.006854552703761023,
        "convergence_improvement_rate": 0.007242683650879546,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14503337817529072,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27704012393951416,
        "value_loss": -0.09471674511830012,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 145,
      "reward": 4449.871564432017,
      "timestamp": 41.319424867630005,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4509.459565215108,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4515.124289872057,
        "raw_reward": 4449.871564432017,
        "reward_service": 845.6657203568359,
        "reward_efficiency": 3604.2058440751807,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4508.802702146843,
        "convergence_std_reward": 29.71555157228246,
        "convergence_cv": 0.0065905637339450564,
        "convergence_improvement_rate": 0.007549123955440067,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14430821128441426,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2906890610853831,
        "value_loss": -0.08471699804067612,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 146,
      "reward": 4677.4340698654905,
      "timestamp": 41.61903238296509,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4510.486262200316,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4545.963148070809,
        "raw_reward": 4677.4340698654905,
        "reward_service": 818.8974345503905,
        "reward_efficiency": 3858.5366353151,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4509.740215103337,
        "convergence_std_reward": 29.433115617410497,
        "convergence_cv": 0.00652656565866867,
        "convergence_improvement_rate": 0.007629232776073256,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14358667022799218,
        "exploration_entropy": 0.001,
        "policy_loss": 0.32039718826611835,
        "value_loss": -0.08511774490276973,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 147,
      "reward": 4569.741708857333,
      "timestamp": 41.92639994621277,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4511.603195424325,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4550.481074620249,
        "raw_reward": 4569.741708857333,
        "reward_service": 764.2569087574708,
        "reward_efficiency": 3805.4848000998627,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4510.892295833788,
        "convergence_std_reward": 29.743472051942984,
        "convergence_cv": 0.006593700337162503,
        "convergence_improvement_rate": 0.007676125735615719,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14286873687685223,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2807081838448842,
        "value_loss": -0.08114748199780782,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 148,
      "reward": 4621.310908143938,
      "timestamp": 42.237977266311646,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4512.448771990336,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4563.9387429897515,
        "raw_reward": 4621.310908143938,
        "reward_service": 746.2686268001953,
        "reward_efficiency": 3875.042281343743,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4511.841917973001,
        "convergence_std_reward": 30.32028907197911,
        "convergence_cv": 0.006720157670240553,
        "convergence_improvement_rate": 0.007492803524388769,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14215439319246798,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28330928087234497,
        "value_loss": -0.09601472069819768,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 149,
      "reward": 4292.459085686464,
      "timestamp": 42.52064752578735,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4513.246721472382,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4535.28994583496,
        "raw_reward": 4292.459085686464,
        "reward_service": 695.5290794672362,
        "reward_efficiency": 3596.9300062192283,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4512.570039981147,
        "convergence_std_reward": 31.08975599376889,
        "convergence_cv": 0.006889589683553982,
        "convergence_improvement_rate": 0.007314560233911816,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14144362122650564,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3103472093741099,
        "value_loss": -0.09396382669607799,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 150,
      "reward": 4758.519212208319,
      "timestamp": 42.80228114128113,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4513.863346582533,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4577.703506445899,
        "raw_reward": 4758.519212208319,
        "reward_service": 844.8642402947363,
        "reward_efficiency": 3913.654971913582,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.085368251168,
        "convergence_std_reward": 31.459457201214725,
        "convergence_cv": 0.0069707206122283805,
        "convergence_improvement_rate": 0.00714256669703507,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1407364031203731,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28379664818445843,
        "value_loss": -0.070833886663119,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 151,
      "reward": 4361.571634703645,
      "timestamp": 43.78212308883667,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4514.196204363885,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4536.63845081487,
        "raw_reward": 4361.571634703645,
        "reward_service": 652.855799426024,
        "reward_efficiency": 3708.715835277621,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.6512768530365,
        "convergence_std_reward": 32.19612921253469,
        "convergence_cv": 0.007133056418789659,
        "convergence_improvement_rate": 0.006948108641272598,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.14003272110477125,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2828238407770793,
        "value_loss": -0.08511820683876674,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 152,
      "reward": 4342.767058072583,
      "timestamp": 44.04498267173767,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4513.886755191211,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4499.8028861938365,
        "raw_reward": 4342.767058072583,
        "reward_service": 802.1811846882811,
        "reward_efficiency": 3540.5858733843006,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.453235418911,
        "convergence_std_reward": 32.107077406419755,
        "convergence_cv": 0.007113639098874982,
        "convergence_improvement_rate": 0.006656377892097776,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1393325574992474,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2882397671540578,
        "value_loss": -0.10643489162127177,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 153,
      "reward": 4591.485725948876,
      "timestamp": 44.319058418273926,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4513.888547658997,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4517.222625747294,
        "raw_reward": 4591.485725948876,
        "reward_service": 808.2720003945116,
        "reward_efficiency": 3783.213725554364,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.199140101774,
        "convergence_std_reward": 32.11309887262271,
        "convergence_cv": 0.007115373790463531,
        "convergence_improvement_rate": 0.006400574961289926,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13863589471175114,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2819924255212148,
        "value_loss": -0.1128134752313296,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 154,
      "reward": 4527.884925584798,
      "timestamp": 44.57092094421387,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4514.116796512906,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4519.248462716419,
        "raw_reward": 4527.884925584798,
        "reward_service": 712.5768083947265,
        "reward_efficiency": 3815.3081171900712,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.544459769053,
        "convergence_std_reward": 32.05474152685004,
        "convergence_cv": 0.007101900028362676,
        "convergence_improvement_rate": 0.006351802539750789,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13794271523819238,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2976336677869161,
        "value_loss": -0.09150717407464981,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 155,
      "reward": 4536.048578818887,
      "timestamp": 44.87371230125427,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4514.5235141610365,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4522.440484775889,
        "raw_reward": 4536.048578818887,
        "reward_service": 706.8430119630566,
        "reward_efficiency": 3829.205566855831,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.756579288038,
        "convergence_std_reward": 32.03781288800293,
        "convergence_cv": 0.007097815827067996,
        "convergence_improvement_rate": 0.006370538101475017,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1372530016620014,
        "exploration_entropy": 0.001,
        "policy_loss": 0.305329829454422,
        "value_loss": -0.06170910348494848,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 156,
      "reward": 4502.031943993295,
      "timestamp": 45.17151498794556,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4515.204075201421,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4518.562862027196,
        "raw_reward": 4502.031943993295,
        "reward_service": 890.1678749880859,
        "reward_efficiency": 3611.8640690052084,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4514.4753569485265,
        "convergence_std_reward": 31.797710457266525,
        "convergence_cv": 0.007043500726684569,
        "convergence_improvement_rate": 0.006699675517324158,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1365667366536914,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3070653478304545,
        "value_loss": -0.10770050436258316,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 157,
      "reward": 4501.345279616549,
      "timestamp": 45.490150690078735,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4516.02862320719,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4515.291521369173,
        "raw_reward": 4501.345279616549,
        "reward_service": 736.9205878660449,
        "reward_efficiency": 3764.4246917505047,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.193516513511,
        "convergence_std_reward": 31.443224127339054,
        "convergence_cv": 0.006963870764861151,
        "convergence_improvement_rate": 0.006925981251654447,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13588390297042294,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27409350872039795,
        "value_loss": -0.0910891592502594,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 158,
      "reward": 4495.509994262262,
      "timestamp": 45.79444670677185,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4516.615710925564,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4511.533031218861,
        "raw_reward": 4495.509994262262,
        "reward_service": 720.859704751924,
        "reward_efficiency": 3774.650289510338,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.995083972289,
        "convergence_std_reward": 30.864283589206455,
        "convergence_cv": 0.0068344369325703735,
        "convergence_improvement_rate": 0.007066602517805309,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13520448345557082,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2980596621831258,
        "value_loss": -0.08642676224311192,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 159,
      "reward": 4205.76213173574,
      "timestamp": 46.093236207962036,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4516.718655974964,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4478.149636317174,
        "raw_reward": 4205.76213173574,
        "reward_service": 694.2979485900389,
        "reward_efficiency": 3511.464183145701,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4516.14706858981,
        "convergence_std_reward": 30.816040799551395,
        "convergence_cv": 0.00682352463981512,
        "convergence_improvement_rate": 0.006868948492378028,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13452846103829297,
        "exploration_entropy": 0.001,
        "policy_loss": 0.30731990933418274,
        "value_loss": -0.08300058792034785,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 160,
      "reward": 4682.687921463478,
      "timestamp": 46.33690905570984,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4516.307639090581,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4517.011910494972,
        "raw_reward": 4682.687921463478,
        "reward_service": 798.5162856603711,
        "reward_efficiency": 3884.171635803107,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.674307327676,
        "convergence_std_reward": 31.073455675207374,
        "convergence_cv": 0.006881243765694938,
        "convergence_improvement_rate": 0.00637753837313621,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1338558187331015,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2881665825843811,
        "value_loss": -0.108531321088473,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 161,
      "reward": 4137.30377134092,
      "timestamp": 46.58174133300781,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4515.342656293031,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4444.867364055702,
        "raw_reward": 4137.30377134092,
        "reward_service": 695.801136639375,
        "reward_efficiency": 3441.502634701546,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.041877426686,
        "convergence_std_reward": 31.218280403404698,
        "convergence_cv": 0.006914283687928343,
        "convergence_improvement_rate": 0.005960836664707038,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.133186539639436,
        "exploration_entropy": 0.001,
        "policy_loss": 0.30340911944707233,
        "value_loss": -0.08511543770631154,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 162,
      "reward": 4713.098814238372,
      "timestamp": 46.82719421386719,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4514.341029370578,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4495.83133959041,
        "raw_reward": 4713.098814238372,
        "reward_service": 845.8974246218946,
        "reward_efficiency": 3867.201389616477,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.75927495978,
        "convergence_std_reward": 32.24717613613219,
        "convergence_cv": 0.007144194931932769,
        "convergence_improvement_rate": 0.005511154387175208,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13252060694123882,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3022987941900889,
        "value_loss": -0.07818460961182912,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 163,
      "reward": 4636.576255321677,
      "timestamp": 47.06724238395691,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4514.429107451879,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4522.57287357935,
        "raw_reward": 4636.576255321677,
        "reward_service": 778.9080607610937,
        "reward_efficiency": 3857.6681945605837,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.555378529211,
        "convergence_std_reward": 32.280771207840885,
        "convergence_cv": 0.007151960815945479,
        "convergence_improvement_rate": 0.005425962123812631,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13185800390653263,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27537132302920025,
        "value_loss": -0.08888224263985951,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 164,
      "reward": 4537.103338052321,
      "timestamp": 47.30666208267212,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4515.143968604822,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4525.333661829215,
        "raw_reward": 4537.103338052321,
        "reward_service": 747.0323849392969,
        "reward_efficiency": 3790.070953113025,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4514.219625207013,
        "convergence_std_reward": 32.13154370922243,
        "convergence_cv": 0.007117851229435683,
        "convergence_improvement_rate": 0.005723744831594429,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13119871388699997,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3012283543745677,
        "value_loss": -0.08621018628279369,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 165,
      "reward": 4549.637666625779,
      "timestamp": 47.57874655723572,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4515.86044812472,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4529.9514227405625,
        "raw_reward": 4549.637666625779,
        "reward_service": 734.4963522081837,
        "reward_efficiency": 3815.141314417596,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4514.980848571061,
        "convergence_std_reward": 31.960058665619943,
        "convergence_cv": 0.007078669818884156,
        "convergence_improvement_rate": 0.006126948214751735,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.13054272031756498,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2799420654773712,
        "value_loss": -0.09744042654832204,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 166,
      "reward": 4297.400627000944,
      "timestamp": 47.836095571517944,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4516.132416280966,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4485.766771550036,
        "raw_reward": 4297.400627000944,
        "reward_service": 825.5657010884375,
        "reward_efficiency": 3471.8349259125066,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.4348029494,
        "convergence_std_reward": 31.88333218887134,
        "convergence_cv": 0.007060966126240096,
        "convergence_improvement_rate": 0.00629114244166051,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12989000671597714,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2990679244200389,
        "value_loss": -0.08209926635026932,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 167,
      "reward": 4254.648161583251,
      "timestamp": 48.077699422836304,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4516.247900723121,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4441.854235656348,
        "raw_reward": 4254.648161583251,
        "reward_service": 755.7084100791014,
        "reward_efficiency": 3498.9397515041487,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.700459523336,
        "convergence_std_reward": 31.518057812950012,
        "convergence_cv": 0.006979660873315979,
        "convergence_improvement_rate": 0.006511946847676525,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12924055668239726,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28498292962710065,
        "value_loss": -0.0999093900124232,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 168,
      "reward": 4391.983976240074,
      "timestamp": 48.33555793762207,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4515.430198237635,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4432.378886367255,
        "raw_reward": 4391.983976240074,
        "reward_service": 688.024573968966,
        "reward_efficiency": 3703.9594022711094,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4515.214195583067,
        "convergence_std_reward": 32.43041357010997,
        "convergence_cv": 0.007182475108674685,
        "convergence_improvement_rate": 0.006613074107297241,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12859435389898527,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2689888874689738,
        "value_loss": -0.0672123854358991,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 169,
      "reward": 4232.467724234576,
      "timestamp": 48.56778383255005,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4513.3909483389925,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4394.395765562047,
        "raw_reward": 4232.467724234576,
        "reward_service": 757.497425946875,
        "reward_efficiency": 3474.9702982877006,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4513.487424700305,
        "convergence_std_reward": 34.95172424286596,
        "convergence_cv": 0.0077438399521378406,
        "convergence_improvement_rate": 0.006293942540055947,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12795138212949034,
        "exploration_entropy": 0.001,
        "policy_loss": 0.31685224175453186,
        "value_loss": -0.08743716031312943,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 170,
      "reward": 4642.140787816478,
      "timestamp": 48.81165552139282,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4510.408728757214,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4415.577663779945,
        "raw_reward": 4642.140787816478,
        "reward_service": 832.5579948347754,
        "reward_efficiency": 3809.582792981703,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4510.695458845803,
        "convergence_std_reward": 38.71909769633765,
        "convergence_cv": 0.008583842125809375,
        "convergence_improvement_rate": 0.005619308818485035,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12731162521884287,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2839593191941579,
        "value_loss": -0.11263326803843181,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 171,
      "reward": 4450.195502408842,
      "timestamp": 49.07109212875366,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4507.696199309674,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4422.155053119436,
        "raw_reward": 4450.195502408842,
        "reward_service": 674.9686330616087,
        "reward_efficiency": 3775.2268693472324,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4507.964518821157,
        "convergence_std_reward": 40.359926550427964,
        "convergence_cv": 0.008953026666896255,
        "convergence_improvement_rate": 0.004868260131409474,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12667506709274864,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2919444938500722,
        "value_loss": -0.03303318781157335,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 172,
      "reward": 4454.50344068571,
      "timestamp": 49.30936932563782,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4505.071717311889,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4428.301246757028,
        "raw_reward": 4454.50344068571,
        "reward_service": 716.419564119375,
        "reward_efficiency": 3738.083876566336,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4505.312582617891,
        "convergence_std_reward": 41.43485994693666,
        "convergence_cv": 0.009196889047565309,
        "convergence_improvement_rate": 0.004153080365905799,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1260416917572849,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28927074869473773,
        "value_loss": -0.0917101576924324,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 173,
      "reward": 4634.525134797539,
      "timestamp": 49.552916049957275,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4502.874272978716,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4467.483785484726,
        "raw_reward": 4634.525134797539,
        "reward_service": 780.7992694691308,
        "reward_efficiency": 3853.725865328407,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4502.926328237345,
        "convergence_std_reward": 41.7957075835046,
        "convergence_cv": 0.009281899044496557,
        "convergence_improvement_rate": 0.003508361426667437,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1254114832984985,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27761950095494586,
        "value_loss": -0.11734225849310558,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 174,
      "reward": 4203.646536101149,
      "timestamp": 49.79916024208069,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4502.079334213966,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4417.354708101846,
        "raw_reward": 4203.646536101149,
        "reward_service": 719.3664168093555,
        "reward_efficiency": 3484.280119291793,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4502.002514385616,
        "convergence_std_reward": 42.296380092408036,
        "convergence_cv": 0.00939501476448646,
        "convergence_improvement_rate": 0.003424527486441512,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.124784425882006,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29157204429308575,
        "value_loss": -0.09003738810618718,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 175,
      "reward": 4626.028934673293,
      "timestamp": 50.033522605895996,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4501.400791036413,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4457.002811150421,
        "raw_reward": 4626.028934673293,
        "reward_service": 783.1548567275781,
        "reward_efficiency": 3842.874077945716,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4501.381760992744,
        "convergence_std_reward": 43.12883132048035,
        "convergence_cv": 0.00958124274066651,
        "convergence_improvement_rate": 0.0036650437715397216,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12416050375259596,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2958264748255412,
        "value_loss": -0.0807688186566035,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 176,
      "reward": 4676.318955100011,
      "timestamp": 50.28714179992676,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4500.9627559675855,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4498.672878500843,
        "raw_reward": 4676.318955100011,
        "reward_service": 836.8062588096275,
        "reward_efficiency": 3839.5126962903837,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4500.77146106334,
        "convergence_std_reward": 43.407127173497784,
        "convergence_cv": 0.009644374869734561,
        "convergence_improvement_rate": 0.0035375250617338595,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12353970123383298,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28547582030296326,
        "value_loss": -0.0923434595266978,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 177,
      "reward": 4348.62278267608,
      "timestamp": 50.52510952949524,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4500.4806837201895,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4470.163360294138,
        "raw_reward": 4348.62278267608,
        "reward_service": 649.985797085576,
        "reward_efficiency": 3698.6369855905027,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4500.466445779749,
        "convergence_std_reward": 43.43247838602665,
        "convergence_cv": 0.009650661527930036,
        "convergence_improvement_rate": 0.0034341144554913537,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12292200272766382,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28168274958928424,
        "value_loss": -0.05730256189902624,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 178,
      "reward": 4369.93514884349,
      "timestamp": 50.75270080566406,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4500.082734927489,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4451.120000118515,
        "raw_reward": 4369.93514884349,
        "reward_service": 801.1679579394532,
        "reward_efficiency": 3568.767190904038,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4499.867083992275,
        "convergence_std_reward": 43.66170460944483,
        "convergence_cv": 0.009702887617451188,
        "convergence_improvement_rate": 0.0033509841063734044,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1223073927140255,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3029793401559194,
        "value_loss": -0.11380540579557419,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 179,
      "reward": 4343.490543172353,
      "timestamp": 50.974177837371826,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4499.968277150942,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4430.6704032987445,
        "raw_reward": 4343.490543172353,
        "reward_service": 790.4300140443358,
        "reward_efficiency": 3553.0605291280176,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4499.792152402947,
        "convergence_std_reward": 43.7551971462179,
        "convergence_cv": 0.009723826271142781,
        "convergence_improvement_rate": 0.0035579986265919386,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12169585575045538,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29269909858703613,
        "value_loss": -0.07957448686162631,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 180,
      "reward": 4278.661230115172,
      "timestamp": 51.216946601867676,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4499.1887957594545,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4401.788660393866,
        "raw_reward": 4278.661230115172,
        "reward_service": 759.4774809984569,
        "reward_efficiency": 3519.183749116715,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4499.309609250433,
        "convergence_std_reward": 44.46111452093193,
        "convergence_cv": 0.009881763733156157,
        "convergence_improvement_rate": 0.003709843509655094,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1210873764717031,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27590206265449524,
        "value_loss": -0.08711851636568706,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 181,
      "reward": 4631.312027379667,
      "timestamp": 51.46069025993347,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4498.183594372848,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4445.398100121169,
        "raw_reward": 4631.312027379667,
        "reward_service": 802.464487977334,
        "reward_efficiency": 3828.847539402332,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4498.15002344631,
        "convergence_std_reward": 45.93602585176077,
        "convergence_cv": 0.010212204042177844,
        "convergence_improvement_rate": 0.003636397484711224,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.12048193958934458,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27053091923395794,
        "value_loss": -0.08501054346561432,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 182,
      "reward": 4409.491031156379,
      "timestamp": 51.699745416641235,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4497.363532645775,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4438.575757017858,
        "raw_reward": 4409.491031156379,
        "reward_service": 680.1663462601953,
        "reward_efficiency": 3729.324684896183,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4497.376337865236,
        "convergence_std_reward": 46.519396831982085,
        "convergence_cv": 0.010343674475341636,
        "convergence_improvement_rate": 0.003608366076253522,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11987952989139786,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2953151861826579,
        "value_loss": -0.08801642060279846,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 183,
      "reward": 4119.607829097706,
      "timestamp": 51.93391537666321,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4495.622009298708,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4377.97185071303,
        "raw_reward": 4119.607829097706,
        "reward_service": 664.3430041152343,
        "reward_efficiency": 3455.2648249824715,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4496.053806796768,
        "convergence_std_reward": 47.71952687032864,
        "convergence_cv": 0.010613646749109218,
        "convergence_improvement_rate": 0.003276962957283068,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11928013224194087,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2767601013183594,
        "value_loss": -0.10227521260579427,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 184,
      "reward": 4493.660509219091,
      "timestamp": 52.175546169281006,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4493.244831167547,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4399.952695829182,
        "raw_reward": 4493.660509219091,
        "reward_service": 689.0729335291894,
        "reward_efficiency": 3804.587575689901,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4493.617363006915,
        "convergence_std_reward": 50.22078544488997,
        "convergence_cv": 0.011176026214053216,
        "convergence_improvement_rate": 0.002573773079904591,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11868373158073116,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28405870993932086,
        "value_loss": -0.11588094880183537,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 185,
      "reward": 4683.960746371696,
      "timestamp": 52.41243529319763,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4491.028802063691,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4453.91422543226,
        "raw_reward": 4683.960746371696,
        "reward_service": 835.390387272588,
        "reward_efficiency": 3848.570359099107,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4491.133293639765,
        "convergence_std_reward": 51.05131173996176,
        "convergence_cv": 0.01136713350553621,
        "convergence_improvement_rate": 0.0015399236136897912,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1180903129228275,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29747583468755084,
        "value_loss": -0.11477956175804138,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 186,
      "reward": 4207.874467696544,
      "timestamp": 52.643698930740356,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4489.1540894211885,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4407.1666714624735,
        "raw_reward": 4207.874467696544,
        "reward_service": 752.1880770855469,
        "reward_efficiency": 3455.686390610997,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4489.329159003549,
        "convergence_std_reward": 51.1899324199306,
        "convergence_cv": 0.011402579451601788,
        "convergence_improvement_rate": 0.00048809546533883894,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11749986135821336,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27619602282842,
        "value_loss": -0.07647447288036346,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 187,
      "reward": 4391.643554009083,
      "timestamp": 52.88445854187012,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4486.775199457346,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4404.217279146329,
        "raw_reward": 4391.643554009083,
        "reward_service": 675.0471706736231,
        "reward_efficiency": 3716.596383335461,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4487.138837494718,
        "convergence_std_reward": 52.32184687416639,
        "convergence_cv": 0.01166040293582246,
        "convergence_improvement_rate": -0.0004611225520546642,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11691236205142229,
        "exploration_entropy": 0.001,
        "policy_loss": 0.26899710297584534,
        "value_loss": -0.10353836168845494,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 188,
      "reward": 4469.863370719917,
      "timestamp": 53.12166881561279,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4484.1673008758635,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4416.690036545311,
        "raw_reward": 4469.863370719917,
        "reward_service": 693.568605597119,
        "reward_efficiency": 3776.294765122798,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4484.475731612545,
        "convergence_std_reward": 52.917505919892754,
        "convergence_cv": 0.01180015437409101,
        "convergence_improvement_rate": -0.001608330034370507,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11632780024116518,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2983790834744771,
        "value_loss": -0.11442390084266663,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 189,
      "reward": 4170.5361660850485,
      "timestamp": 53.36128759384155,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4481.634129648258,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4369.920801157861,
        "raw_reward": 4170.5361660850485,
        "reward_service": 730.8231608189843,
        "reward_efficiency": 3439.713005266064,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4482.381396957687,
        "convergence_std_reward": 53.974274304677586,
        "convergence_cv": 0.012041428322300146,
        "convergence_improvement_rate": -0.002631209332102882,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11574616123995936,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27791455388069153,
        "value_loss": -0.10319655636946361,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 190,
      "reward": 4539.731161162506,
      "timestamp": 53.601298570632935,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4478.613166404993,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4402.184769558744,
        "raw_reward": 4539.731161162506,
        "reward_service": 711.2178723217284,
        "reward_efficiency": 3828.5132888407775,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4479.209376806779,
        "convergence_std_reward": 55.32711396823033,
        "convergence_cv": 0.012351982082979327,
        "convergence_improvement_rate": -0.0038737198627760545,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11516743043375956,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2974755863348643,
        "value_loss": -0.12034457921981812,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 191,
      "reward": 4141.853979293263,
      "timestamp": 53.848247051239014,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4475.193188427597,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4352.721919408303,
        "raw_reward": 4141.853979293263,
        "reward_service": 650.8120121713769,
        "reward_efficiency": 3491.041967121887,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4476.124857413326,
        "convergence_std_reward": 55.986034055580085,
        "convergence_cv": 0.012507701603286694,
        "convergence_improvement_rate": -0.005122940256104149,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11459159328159076,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28782081604003906,
        "value_loss": -0.10064415136973064,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 192,
      "reward": 4518.693990486224,
      "timestamp": 54.0915253162384,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4471.358612022661,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4384.256612913108,
        "raw_reward": 4518.693990486224,
        "reward_service": 714.848674858125,
        "reward_efficiency": 3803.845315628098,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4472.195667496612,
        "convergence_std_reward": 57.03268466941623,
        "convergence_cv": 0.01275272571008532,
        "convergence_improvement_rate": -0.00673099777943098,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1140186353151828,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2852926254272461,
        "value_loss": -0.08920918405056,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 193,
      "reward": 4316.931141785,
      "timestamp": 54.32403540611267,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4467.981734767536,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4371.464773398768,
        "raw_reward": 4316.931141785,
        "reward_service": 795.9531274746093,
        "reward_efficiency": 3520.978014310391,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4468.675092827182,
        "convergence_std_reward": 57.10958700916513,
        "convergence_cv": 0.012779981946065761,
        "convergence_improvement_rate": -0.008091061591634995,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11344854213860689,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2849460244178772,
        "value_loss": -0.1138991117477417,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 194,
      "reward": 4751.959830102474,
      "timestamp": 54.56657671928406,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4465.592349120402,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4443.758834172472,
        "raw_reward": 4751.959830102474,
        "reward_service": 857.3417627155762,
        "reward_efficiency": 3894.6180673868985,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4465.921289048927,
        "convergence_std_reward": 57.555113144381735,
        "convergence_cv": 0.01288762372178727,
        "convergence_improvement_rate": -0.009157426295468259,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11288129942791385,
        "exploration_entropy": 0.001,
        "policy_loss": 0.26998209953308105,
        "value_loss": -0.12354616324106853,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 195,
      "reward": 4505.038852600851,
      "timestamp": 54.82042384147644,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4464.142593986732,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4455.4020376738645,
        "raw_reward": 4505.038852600851,
        "reward_service": 712.9892146108402,
        "reward_efficiency": 3792.0496379900114,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4464.329694976534,
        "convergence_std_reward": 56.956619809042785,
        "convergence_cv": 0.012758157148011035,
        "convergence_improvement_rate": -0.009863595749961328,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11231689293077428,
        "exploration_entropy": 0.001,
        "policy_loss": 0.26183801889419556,
        "value_loss": -0.09469267229239146,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 196,
      "reward": 4607.316367526431,
      "timestamp": 55.05874943733215,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4462.927358042489,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4484.265760345853,
        "raw_reward": 4607.316367526431,
        "reward_service": 759.3237319472265,
        "reward_efficiency": 3847.9926355792054,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4463.124854482428,
        "convergence_std_reward": 56.33000174008443,
        "convergence_cv": 0.012621202313780397,
        "convergence_improvement_rate": -0.010336595545967614,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1117553084661204,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3030875027179718,
        "value_loss": -0.08693752686182658,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 197,
      "reward": 4644.562376661144,
      "timestamp": 55.29224181175232,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4461.966446399046,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4514.722117445758,
        "raw_reward": 4644.562376661144,
        "reward_service": 815.4455878170604,
        "reward_efficiency": 3829.116788844084,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4462.027424783459,
        "convergence_std_reward": 55.21694380797323,
        "convergence_cv": 0.012374855318298026,
        "convergence_improvement_rate": -0.01083263971863404,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1111965319237898,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2988891700903575,
        "value_loss": -0.10066931694746017,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 198,
      "reward": 4573.512328202427,
      "timestamp": 55.535228967666626,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4461.2271880100225,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4525.892257489525,
        "raw_reward": 4573.512328202427,
        "reward_service": 753.933871052334,
        "reward_efficiency": 3819.5784571500926,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4461.300206017202,
        "convergence_std_reward": 54.230481996857584,
        "convergence_cv": 0.0121557571767338,
        "convergence_improvement_rate": -0.011202013030302537,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11064054926417084,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2682514389355977,
        "value_loss": -0.08158626407384872,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 199,
      "reward": 4640.489460804067,
      "timestamp": 55.77298331260681,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4460.728804516079,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4547.665726119289,
        "raw_reward": 4640.489460804067,
        "reward_service": 802.052756949219,
        "reward_efficiency": 3838.436703854848,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4460.670549415228,
        "convergence_std_reward": 53.22415568294605,
        "convergence_cv": 0.011931873267332749,
        "convergence_improvement_rate": -0.011501093635354443,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.11008734651784999,
        "exploration_entropy": 0.001,
        "policy_loss": 0.31428709626197815,
        "value_loss": -0.10230896125237147,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 200,
      "reward": 4554.469174949829,
      "timestamp": 56.01535081863403,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4460.5434684866805,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4548.958381397091,
        "raw_reward": 4554.469174949829,
        "reward_service": 789.6171314702832,
        "reward_efficiency": 3764.8520434795455,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4460.701639203372,
        "convergence_std_reward": 53.25131619897788,
        "convergence_cv": 0.011937878949574383,
        "convergence_improvement_rate": -0.011607076927085633,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10953690978526073,
        "exploration_entropy": 0.001,
        "policy_loss": 0.28994020819664,
        "value_loss": -0.09585425754388173,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 201,
      "reward": 4574.564614069937,
      "timestamp": 56.93192100524902,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4460.452042194534,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4553.823565604932,
        "raw_reward": 4574.564614069937,
        "reward_service": 779.942466284424,
        "reward_efficiency": 3794.6221477855133,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4460.368474806811,
        "convergence_std_reward": 52.61252484241317,
        "convergence_cv": 0.011795555712399286,
        "convergence_improvement_rate": -0.0118048113994697,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10898922523633443,
        "exploration_entropy": 0.001,
        "policy_loss": 0.26178330183029175,
        "value_loss": -0.11722658574581146,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 202,
      "reward": 4300.886803588125,
      "timestamp": 57.169525384902954,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4461.20214421187,
        "best_reward": 4887.368889678801,
        "step_count": 9,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4529.369451525524,
        "raw_reward": 4300.886803588125,
        "reward_service": 754.7306543853514,
        "reward_efficiency": 3546.1561492027727,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4460.9153769633795,
        "convergence_std_reward": 53.419849984617066,
        "convergence_cv": 0.011975087055110394,
        "convergence_improvement_rate": -0.011640279784721352,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10844427911015277,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27660099665323895,
        "value_loss": -0.11311988532543182,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 203,
      "reward": 4658.53306157212,
      "timestamp": 57.41496419906616,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4461.868437059087,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4553.910537434377,
        "raw_reward": 4658.53306157212,
        "reward_service": 820.7745410589941,
        "reward_efficiency": 3837.758520513125,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4461.544189040304,
        "convergence_std_reward": 54.1122913254261,
        "convergence_cv": 0.012128601451119072,
        "convergence_improvement_rate": -0.011445307299315118,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.107902057714602,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2728589375813802,
        "value_loss": -0.12706071635087332,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 204,
      "reward": 4706.890035035638,
      "timestamp": 57.65674352645874,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4462.886829688357,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4582.976641978617,
        "raw_reward": 4706.890035035638,
        "reward_service": 820.8126336510156,
        "reward_efficiency": 3886.077401384623,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4462.42026447181,
        "convergence_std_reward": 55.366086581347226,
        "convergence_cv": 0.012407187871154171,
        "convergence_improvement_rate": -0.011326839860099462,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10736254742602899,
        "exploration_entropy": 0.001,
        "policy_loss": 0.303087184826533,
        "value_loss": -0.12417255342006683,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 205,
      "reward": 4646.153178488947,
      "timestamp": 57.898131370544434,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4464.2541461144165,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4594.980183915581,
        "raw_reward": 4646.153178488947,
        "reward_service": 803.3639261568164,
        "reward_efficiency": 3842.7892523321316,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4463.741204477462,
        "convergence_std_reward": 57.475566224886094,
        "convergence_cv": 0.012876097334503591,
        "convergence_improvement_rate": -0.01108065398122662,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10682573468889885,
        "exploration_entropy": 0.001,
        "policy_loss": 0.29139209787050885,
        "value_loss": -0.11875784397125244,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 206,
      "reward": 4579.411743144019,
      "timestamp": 58.14204716682434,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4465.714620297233,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4592.022180168984,
        "raw_reward": 4579.411743144019,
        "reward_service": 719.830455684004,
        "reward_efficiency": 3859.581287460015,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4465.196838560268,
        "convergence_std_reward": 59.79276941363921,
        "convergence_cv": 0.013390847386006492,
        "convergence_improvement_rate": -0.010915668929815067,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10629160601545436,
        "exploration_entropy": 0.001,
        "policy_loss": 0.2674004137516022,
        "value_loss": -0.11344827214876811,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 207,
      "reward": 4789.701093637828,
      "timestamp": 58.38809895515442,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4467.613599651827,
        "best_reward": 4887.368889678801,
        "step_count": 10,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4629.581173728066,
        "raw_reward": 4789.701093637828,
        "reward_service": 840.934206164033,
        "reward_efficiency": 3948.7668874737956,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4466.880921418983,
        "convergence_std_reward": 62.393356580561694,
        "convergence_cv": 0.013967991911622606,
        "convergence_improvement_rate": -0.010700005418999892,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.10576014798537708,
        "exploration_entropy": 0.001,
        "policy_loss": 0.27018339435259503,
        "value_loss": -0.13224850098292032,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    },
    {
      "episode": 208,
      "reward": 4541.032864720869,
      "timestamp": 58.640867710113525,
      "metrics": {
        "episode_success": true,
        "current_avg_reward": 4469.761859135204,
        "best_reward": 4887.368889678801,
        "step_count": 11,
        "completion_rate": 1.0,
        "efficiency": 1.0,
        "smoothed_reward": 4612.756995016698,
        "raw_reward": 4541.032864720869,
        "reward_service": 703.1409498271255,
        "reward_efficiency": 3837.8919148937434,
        "reward_cost": 0.0,
        "convergence_status": "converged",
        "convergence_message": "训练已收敛",
        "convergence_confidence": 0.8,
        "convergence_mean_reward": 4469.097947684259,
        "convergence_std_reward": 65.91219928347019,
        "convergence_cv": 0.014748434707640215,
        "convergence_improvement_rate": -0.010384673901544277,
        "current_learning_rate": 0.00023999999999999998,
        "exploration_epsilon": 0.1052313472454502,
        "exploration_entropy": 0.001,
        "policy_loss": 0.3146798511346181,
        "value_loss": -0.10671522965033849,
        "learning_rate": 0.00023999999999999998,
        "entropy_coefficient": 0.001
      }
    }
  ],
  "recommendations": [
    "训练数据不足，建议增加训练轮数",
    "奖励稳定性良好，训练收敛效果佳"
  ],
  "optimization_history": {
    "learning_rate_history": [
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.0003,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998,
      0.00023999999999999998
    ],
    "exploration_history": [
      {
        "epsilon": 0.2985,
        "entropy_coef": 0.0098
      },
      {
        "epsilon": 0.2970075,
        "entropy_coef": 0.009604
      },
      {
        "epsilon": 0.29552246249999997,
        "entropy_coef": 0.009411919999999999
      },
      {
        "epsilon": 0.2940448501875,
        "entropy_coef": 0.009223681599999999
      },
      {
        "epsilon": 0.29257462593656247,
        "entropy_coef": 0.009039207967999998
      },
      {
        "epsilon": 0.2911117528068797,
        "entropy_coef": 0.008858423808639998
      },
      {
        "epsilon": 0.28965619404284526,
        "entropy_coef": 0.008681255332467197
      },
      {
        "epsilon": 0.28820791307263105,
        "entropy_coef": 0.008507630225817853
      },
      {
        "epsilon": 0.28676687350726787,
        "entropy_coef": 0.008337477621301496
      },
      {
        "epsilon": 0.2853330391397315,
        "entropy_coef": 0.008170728068875466
      },
      {
        "epsilon": 0.28390637394403284,
        "entropy_coef": 0.008007313507497957
      },
      {
        "epsilon": 0.2824868420743127,
        "entropy_coef": 0.007847167237347998
      },
      {
        "epsilon": 0.2810744078639411,
        "entropy_coef": 0.007690223892601038
      },
      {
        "epsilon": 0.2796690358246214,
        "entropy_coef": 0.007536419414749017
      },
      {
        "epsilon": 0.27827069064549825,
        "entropy_coef": 0.007385691026454037
      },
      {
        "epsilon": 0.27687933719227076,
        "entropy_coef": 0.0072379772059249555
      },
      {
        "epsilon": 0.2754949405063094,
        "entropy_coef": 0.007093217661806457
      },
      {
        "epsilon": 0.2741174658037779,
        "entropy_coef": 0.006951353308570328
      },
      {
        "epsilon": 0.272746878474759,
        "entropy_coef": 0.006812326242398921
      },
      {
        "epsilon": 0.2713831440823852,
        "entropy_coef": 0.006676079717550942
      },
      {
        "epsilon": 0.27002622836197326,
        "entropy_coef": 0.006542558123199923
      },
      {
        "epsilon": 0.2686760972201634,
        "entropy_coef": 0.0064117069607359245
      },
      {
        "epsilon": 0.26733271673406256,
        "entropy_coef": 0.006283472821521206
      },
      {
        "epsilon": 0.26599605315039226,
        "entropy_coef": 0.006157803365090782
      },
      {
        "epsilon": 0.2646660728846403,
        "entropy_coef": 0.006034647297788966
      },
      {
        "epsilon": 0.26334274252021705,
        "entropy_coef": 0.005913954351833187
      },
      {
        "epsilon": 0.26202602880761594,
        "entropy_coef": 0.0057956752647965225
      },
      {
        "epsilon": 0.26071589866357786,
        "entropy_coef": 0.005679761759500592
      },
      {
        "epsilon": 0.25941231917026,
        "entropy_coef": 0.00556616652431058
      },
      {
        "epsilon": 0.2581152575744087,
        "entropy_coef": 0.005454843193824369
      },
      {
        "epsilon": 0.2568246812865366,
        "entropy_coef": 0.005345746329947882
      },
      {
        "epsilon": 0.25554055788010394,
        "entropy_coef": 0.005238831403348924
      },
      {
        "epsilon": 0.2542628550907034,
        "entropy_coef": 0.005134054775281945
      },
      {
        "epsilon": 0.2529915408152499,
        "entropy_coef": 0.005031373679776306
      },
      {
        "epsilon": 0.25172658311117363,
        "entropy_coef": 0.00493074620618078
      },
      {
        "epsilon": 0.25046795019561774,
        "entropy_coef": 0.004832131282057164
      },
      {
        "epsilon": 0.24921561044463966,
        "entropy_coef": 0.004735488656416021
      },
      {
        "epsilon": 0.24796953239241645,
        "entropy_coef": 0.004640778883287701
      },
      {
        "epsilon": 0.24672968473045437,
        "entropy_coef": 0.0045479633056219465
      },
      {
        "epsilon": 0.24549603630680208,
        "entropy_coef": 0.004457004039509507
      },
      {
        "epsilon": 0.24426855612526807,
        "entropy_coef": 0.004367863958719317
      },
      {
        "epsilon": 0.24304721334464174,
        "entropy_coef": 0.004280506679544931
      },
      {
        "epsilon": 0.24183197727791853,
        "entropy_coef": 0.004194896545954032
      },
      {
        "epsilon": 0.24062281739152894,
        "entropy_coef": 0.004110998615034951
      },
      {
        "epsilon": 0.2394197033045713,
        "entropy_coef": 0.004028778642734252
      },
      {
        "epsilon": 0.23822260478804844,
        "entropy_coef": 0.003948203069879567
      },
      {
        "epsilon": 0.2370314917641082,
        "entropy_coef": 0.003869239008481976
      },
      {
        "epsilon": 0.23584633430528767,
        "entropy_coef": 0.0037918542283123364
      },
      {
        "epsilon": 0.23466710263376123,
        "entropy_coef": 0.0037160171437460895
      },
      {
        "epsilon": 0.23349376712059242,
        "entropy_coef": 0.003641696800871168
      },
      {
        "epsilon": 0.23232629828498946,
        "entropy_coef": 0.0035688628648537445
      },
      {
        "epsilon": 0.2311646667935645,
        "entropy_coef": 0.0034974856075566697
      },
      {
        "epsilon": 0.23000884345959668,
        "entropy_coef": 0.0034275358954055364
      },
      {
        "epsilon": 0.2288587992422987,
        "entropy_coef": 0.0033589851774974257
      },
      {
        "epsilon": 0.22771450524608722,
        "entropy_coef": 0.0032918054739474773
      },
      {
        "epsilon": 0.2265759327198568,
        "entropy_coef": 0.003225969364468528
      },
      {
        "epsilon": 0.2254430530562575,
        "entropy_coef": 0.0031614499771791572
      },
      {
        "epsilon": 0.22431583779097622,
        "entropy_coef": 0.003098220977635574
      },
      {
        "epsilon": 0.22319425860202133,
        "entropy_coef": 0.0030362565580828627
      },
      {
        "epsilon": 0.22207828730901122,
        "entropy_coef": 0.0029755314269212054
      },
      {
        "epsilon": 0.22096789587246615,
        "entropy_coef": 0.0029160207983827814
      },
      {
        "epsilon": 0.21986305639310383,
        "entropy_coef": 0.0028577003824151255
      },
      {
        "epsilon": 0.2187637411111383,
        "entropy_coef": 0.002800546374766823
      },
      {
        "epsilon": 0.2176699224055826,
        "entropy_coef": 0.0027445354472714865
      },
      {
        "epsilon": 0.2165815727935547,
        "entropy_coef": 0.0026896447383260567
      },
      {
        "epsilon": 0.21549866492958691,
        "entropy_coef": 0.0026358518435595354
      },
      {
        "epsilon": 0.21442117160493898,
        "entropy_coef": 0.0025831348066883445
      },
      {
        "epsilon": 0.21334906574691428,
        "entropy_coef": 0.0025314721105545775
      },
      {
        "epsilon": 0.2122823204181797,
        "entropy_coef": 0.002480842668343486
      },
      {
        "epsilon": 0.21122090881608882,
        "entropy_coef": 0.002431225814976616
      },
      {
        "epsilon": 0.21016480427200837,
        "entropy_coef": 0.0023826012986770837
      },
      {
        "epsilon": 0.20911398025064834,
        "entropy_coef": 0.002334949272703542
      },
      {
        "epsilon": 0.2080684103493951,
        "entropy_coef": 0.002288250287249471
      },
      {
        "epsilon": 0.20702806829764814,
        "entropy_coef": 0.002242485281504482
      },
      {
        "epsilon": 0.2059929279561599,
        "entropy_coef": 0.0021976355758743923
      },
      {
        "epsilon": 0.2049629633163791,
        "entropy_coef": 0.0021536828643569043
      },
      {
        "epsilon": 0.2039381484997972,
        "entropy_coef": 0.002110609207069766
      },
      {
        "epsilon": 0.2029184577572982,
        "entropy_coef": 0.0020683970229283706
      },
      {
        "epsilon": 0.2019038654685117,
        "entropy_coef": 0.002027029082469803
      },
      {
        "epsilon": 0.20089434614116913,
        "entropy_coef": 0.001986488500820407
      },
      {
        "epsilon": 0.1998898744104633,
        "entropy_coef": 0.001946758730803999
      },
      {
        "epsilon": 0.19889042503841098,
        "entropy_coef": 0.001907823556187919
      },
      {
        "epsilon": 0.19789597291321892,
        "entropy_coef": 0.0018696670850641606
      },
      {
        "epsilon": 0.19690649304865282,
        "entropy_coef": 0.0018322737433628773
      },
      {
        "epsilon": 0.19592196058340955,
        "entropy_coef": 0.0017956282684956197
      },
      {
        "epsilon": 0.1949423507804925,
        "entropy_coef": 0.0017597157031257072
      },
      {
        "epsilon": 0.19396763902659003,
        "entropy_coef": 0.001724521389063193
      },
      {
        "epsilon": 0.1929978008314571,
        "entropy_coef": 0.0016900309612819292
      },
      {
        "epsilon": 0.1920328118272998,
        "entropy_coef": 0.0016562303420562905
      },
      {
        "epsilon": 0.19107264776816332,
        "entropy_coef": 0.0016231057352151647
      },
      {
        "epsilon": 0.1901172845293225,
        "entropy_coef": 0.0015906436205108613
      },
      {
        "epsilon": 0.18916669810667588,
        "entropy_coef": 0.0015588307481006441
      },
      {
        "epsilon": 0.1882208646161425,
        "entropy_coef": 0.0015276541331386312
      },
      {
        "epsilon": 0.1872797602930618,
        "entropy_coef": 0.0014971010504758585
      },
      {
        "epsilon": 0.18634336149159647,
        "entropy_coef": 0.0014671590294663413
      },
      {
        "epsilon": 0.18541164468413848,
        "entropy_coef": 0.0014378158488770143
      },
      {
        "epsilon": 0.18448458646071778,
        "entropy_coef": 0.001409059531899474
      },
      {
        "epsilon": 0.18356216352841417,
        "entropy_coef": 0.0013808783412614844
      },
      {
        "epsilon": 0.1826443527107721,
        "entropy_coef": 0.0013532607744362547
      },
      {
        "epsilon": 0.18173113094721824,
        "entropy_coef": 0.0013261955589475296
      },
      {
        "epsilon": 0.18082247529248216,
        "entropy_coef": 0.001299671647768579
      },
      {
        "epsilon": 0.17991836291601976,
        "entropy_coef": 0.0012736782148132075
      },
      {
        "epsilon": 0.17901877110143966,
        "entropy_coef": 0.0012482046505169432
      },
      {
        "epsilon": 0.17812367724593245,
        "entropy_coef": 0.0012232405575066043
      },
      {
        "epsilon": 0.17723305885970278,
        "entropy_coef": 0.001198775746356472
      },
      {
        "epsilon": 0.17634689356540426,
        "entropy_coef": 0.0011748002314293426
      },
      {
        "epsilon": 0.17546515909757723,
        "entropy_coef": 0.0011513042268007558
      },
      {
        "epsilon": 0.17458783330208935,
        "entropy_coef": 0.0011282781422647407
      },
      {
        "epsilon": 0.1737148941355789,
        "entropy_coef": 0.0011057125794194458
      },
      {
        "epsilon": 0.172846319664901,
        "entropy_coef": 0.0010835983278310569
      },
      {
        "epsilon": 0.1719820880665765,
        "entropy_coef": 0.0010619263612744356
      },
      {
        "epsilon": 0.17112217762624363,
        "entropy_coef": 0.0010406878340489469
      },
      {
        "epsilon": 0.1702665667381124,
        "entropy_coef": 0.001019874077367968
      },
      {
        "epsilon": 0.16941523390442184,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16856815773489972,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16772531694622522,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1668866903614941,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1660522569096866,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16522199562513817,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1643958856470125,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16357390621877743,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16275603668768354,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16194225650424512,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1611325452217239,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.16032688249561527,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1595252480831372,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15872762184272152,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15793398373350792,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15714431381484037,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15635859224576618,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15557679928453735,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15479891528811468,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15402492071167412,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15325479610811574,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15248852212757516,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.15172607951693728,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1509674491193526,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1502126118737558,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14946154881438703,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14871424107031508,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14797066986496352,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14723081651563868,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1464946624330605,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1457621891208952,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14503337817529072,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14430821128441426,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14358667022799218,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14286873687685223,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14215439319246798,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14144362122650564,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1407364031203731,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.14003272110477125,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1393325574992474,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13863589471175114,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13794271523819238,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1372530016620014,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1365667366536914,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13588390297042294,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13520448345557082,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13452846103829297,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1338558187331015,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.133186539639436,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13252060694123882,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13185800390653263,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13119871388699997,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.13054272031756498,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12989000671597714,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12924055668239726,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12859435389898527,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12795138212949034,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12731162521884287,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12667506709274864,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1260416917572849,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1254114832984985,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.124784425882006,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12416050375259596,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12353970123383298,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12292200272766382,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1223073927140255,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12169585575045538,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1210873764717031,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.12048193958934458,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11987952989139786,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11928013224194087,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11868373158073116,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1180903129228275,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11749986135821336,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11691236205142229,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11632780024116518,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11574616123995936,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11516743043375956,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11459159328159076,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1140186353151828,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11344854213860689,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11288129942791385,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11231689293077428,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1117553084661204,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1111965319237898,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11064054926417084,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.11008734651784999,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10953690978526073,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10898922523633443,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10844427911015277,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.107902057714602,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10736254742602899,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10682573468889885,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10629160601545436,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.10576014798537708,
        "entropy_coef": 0.001
      },
      {
        "epsilon": 0.1052313472454502,
        "entropy_coef": 0.001
      }
    ],
    "smoothed_rewards": [
      4197.140301279942,
      4197.140301279942,
      4234.1207035305415,
      4267.403065556082,
      4329.399647968354,
      4385.196572139399,
      4363.584410681924,
      4344.133465370197,
      4373.090747022636,
      4399.152300509831,
      4399.034307165933,
      4398.928113156425,
      4377.291367272135,
      4357.818295976275,
      4378.881716908592,
      4397.838795747677,
      4420.471420958985,
      4440.8407836491615,
      4470.384152063893,
      4496.973183637152,
      4522.170972312338,
      4544.848982120005,
      4555.133780719976,
      4564.390099459951,
      4570.201047652337,
      4527.417762907785,
      4533.464591116255,
      4538.906736503878,
      4510.176355990003,
      4484.319013527515,
      4476.49059884579,
      4469.445025632238,
      4449.7096428856585,
      4431.947798413737,
      4451.528555498411,
      4469.151236874618,
      4470.904895834967,
      4472.483188899281,
      4473.2710703820285,
      4473.980163716501,
      4497.435966191448,
      4518.5461884189,
      4524.32319609944,
      4529.522503011926,
      4529.805767765323,
      4496.023947829615,
      4504.225450014549,
      4511.606801980989,
      4467.400083627076,
      4427.614037108555,
      4400.231468439315,
      4375.587156637,
      4395.700507185244,
      4413.802522678663,
      4398.477864090088,
      4384.68567136037,
      4406.307269287883,
      4425.766707422645,
      4447.624915440059,
      4467.297302655732,
      4462.639397242664,
      4458.447282370902,
      4437.1554776375115,
      4417.99285337746,
      4398.129082570744,
      4380.251688844699,
      4421.907388815676,
      4459.397518789556,
      4447.71909790848,
      4437.208519115511,
      4423.251534790331,
      4410.690248897669,
      4416.080733925869,
      4420.932170451249,
      4428.590700315561,
      4435.483377193441,
      4425.545101079296,
      4416.600652576565,
      4429.301771543117,
      4440.732778613014,
      4448.810558872965,
      4456.080561106921,
      4436.09131101768,
      4418.100985937363,
      4386.799665440434,
      4358.628476993198,
      4380.806339878271,
      4400.766416474836,
      4416.457016338722,
      4430.578556216219,
      4438.108396306945,
      4444.885252388599,
      4450.378979729142,
      4455.3233343356305,
      4430.2698010180075,
      4407.721621032147,
      4429.650911322597,
      4449.387272584001,
      4452.484108700668,
      4455.2712612056675,
      4458.428095657995,
      4461.26924666509,
      4473.196422636317,
      4483.930881010421,
      4479.532411632448,
      4475.573789192273,
      4469.895132350492,
      4464.784341192889,
      4480.147240334656,
      4493.973849562247,
      4512.263271879172,
      4528.723751964405,
      4517.480904582558,
      4507.362341938895,
      4485.690883191888,
      4466.186570319582,
      4465.281315508594,
      4464.466586178705,
      4437.158672502538,
      4412.581550193988,
      4439.406975192004,
      4463.549857690218,
      4472.7458440097325,
      4481.022231697295,
      4490.629015060172,
      4499.275120086761,
      4509.094711536522,
      4517.932343841307,
      4529.114878412693,
      4539.179159526941,
      4542.014337830879,
      4515.29456452651,
      4504.067417478327,
      4493.962985134963,
      4499.937698111801,
      4505.314939790956,
      4515.412358751261,
      4524.500035815537,
      4522.288016528828,
      4520.29719917079,
      4520.435850159241,
      4520.560636048847,
      4523.67136771813,
      4526.4710262204835,
      4530.8527264953,
      4534.796256742635,
      4530.591845989217,
      4526.807876311142,
      4528.734012077988,
      4544.413492560128,
      4542.604240721,
      4502.449030348582,
      4498.3432307283565,
      4494.648011070154,
      4502.371445663982,
      4509.322536798427,
      4503.493572758863,
      4498.247505123255,
      4500.702806471174,
      4502.912577684301,
      4507.39073316125,
      4511.421073090504,
      4513.8013911398575,
      4515.943677384275,
      4513.202668656308,
      4481.877395985821,
      4479.428052321122,
      4477.223643022893,
      4462.635887016424,
      4449.506906610602,
      4412.5639393912,
      4379.315268893738,
      4393.214506483403,
      4405.723820314102,
      4419.098745409732,
      4431.136177995799,
      4401.078132740105,
      4374.0258920099795,
      4393.499471827545,
      4411.025693663353,
      4420.853067314107,
      4429.697703599785,
      4402.510489339572,
      4378.041996505381,
      4404.103824171971,
      4427.559469071902,
      4423.843763233019,
      4420.499627978024,
      4428.934504641487,
      4436.525893638604,
      4455.569025135766,
      4472.707843483213,
      4480.020418611067,
      4486.6017362261355,
      4492.49434294896,
      4497.797688999503,
      4515.285229672122,
      4531.024016277479,
      4522.373678415806,
      4514.588374340301,
      4526.781616964751,
      4537.755535326756,
      4539.447430204063,
      4540.970135593638,
      4532.723770174351,
      4525.302041296993,
      4508.881546780001,
      4494.10310171471,
      4506.876413035619,
      4518.3723932244375,
      4500.592591920155,
      4484.590770746302,
      4486.371578678389,
      4487.974305817267,
      4475.6137034976455,
      4464.489161409986,
      4483.958982739837,
      4501.4818219367025,
      4503.267662133964,
      4504.8749183115,
      4519.154309050947,
      4532.005760716449,
      4527.290236457103,
      4523.046264623691,
      4521.771855196579,
      4520.624886712178,
      4505.501927098566,
      4491.891263446315,
      4488.282862379519,
      4485.035301419403,
      4491.94008631392,
      4498.154392718985,
      4493.097935354289,
      4464.975336264274,
      4456.880688445613,
      4449.595505408817,
      4477.752333989043,
      4503.093479711246,
      4514.350265086105,
      4524.481371923478,
      4539.514962420367,
      4553.045193867567,
      4554.665920019797,
      4556.1245735568045,
      4556.613991765317,
      4557.054468152978,
      4558.795852219702,
      4520.11823242004,
      4480.846998783908,
      4445.502888511389,
      4472.149289636676,
      4496.131050649435,
      4500.836178986698,
      4505.070794490234,
      4507.771481288228,
      4510.202099406421,
      4480.037978784085,
      4452.890270223981,
      4446.079943251225,
      4439.950648975744,
      4454.443555554753,
      4467.487171475862,
      4475.001066422784,
      4481.763571875013,
      4482.810479578318,
      4483.752696511293,
      4493.7551315792125,
      4502.75732314034,
      4508.399623121641,
      4537.558177351286,
      4539.154956076289,
      4540.592056928792,
      4523.045881783804,
      4507.254324153315,
      4524.558857909244,
      4540.132938289579,
      4541.176817377829,
      4499.464940650893,
      4518.733210879565,
      4536.07465408537,
      4537.949999321596,
      4539.637810034199,
      4547.150589401953,
      4553.912090832931,
      4561.047946762908,
      4567.470217099886,
      4546.36792855654,
      4527.375868867529,
      4528.983561425862,
      4530.430484728362,
      4522.374592698728,
      4515.124289872057,
      4531.3552678714,
      4545.963148070809,
      4548.341004149462,
      4550.481074620249,
      4557.564057972619,
      4563.9387429897515,
      4562.271152518126,
      4535.28994583496,
      4557.612872472296,
      4577.703506445899,
      4556.090319271673,
      4536.63845081487,
      4517.251311540642,
      4499.8028861938365,
      4508.97117016934,
      4517.222625747294,
      4518.288855731044,
      4519.248462716419,
      4520.928474326666,
      4522.440484775889,
      4520.39963069763,
      4518.562862027196,
      4516.8411037861315,
      4515.291521369173,
      4513.313368658482,
      4511.533031218861,
      4508.414914604,
      4478.149636317174,
      4498.603464831805,
      4517.011910494972,
      4479.041096579566,
      4444.867364055702,
      4471.690509073969,
      4495.83133959041,
      4509.905831163536,
      4522.57287357935,
      4524.025920026647,
      4525.333661829215,
      4527.764062308872,
      4529.9514227405625,
      4506.696343166601,
      4485.766771550036,
      4462.654910553358,
      4441.854235656348,
      4436.86720971472,
      4432.378886367255,
      4412.387770153988,
      4394.395765562047,
      4390.403983331442,
      4415.577663779945,
      4419.0394476428355,
      4422.155053119436,
      4425.389891876063,
      4428.301246757028,
      4448.923635561079,
      4467.483785484726,
      4441.100060546368,
      4417.354708101846,
      4438.222130758991,
      4457.002811150421,
      4478.93442554538,
      4498.672878500843,
      4483.667868918366,
      4470.163360294138,
      4460.140539149073,
      4451.120000118515,
      4440.357054423899,
      4430.6704032987445,
      4415.469485980388,
      4401.788660393866,
      4424.740997092446,
      4445.398100121169,
      4441.80739322469,
      4438.575757017858,
      4406.678964225844,
      4377.97185071303,
      4389.540716563636,
      4399.952695829182,
      4428.353500883433,
      4453.91422543226,
      4429.310249658688,
      4407.1666714624735,
      4405.614359717134,
      4404.217279146329,
      4410.781888303688,
      4416.690036545311,
      4392.074649499285,
      4369.920801157861,
      4386.901837158326,
      4402.184769558744,
      4376.151690532196,
      4352.721919408303,
      4369.319126516095,
      4384.256612913108,
      4377.524065800298,
      4371.464773398768,
      4409.514279069139,
      4443.758834172472,
      4449.88683601531,
      4455.4020376738645,
      4470.5934706591215,
      4484.265760345853,
      4500.295421977382,
      4514.722117445758,
      4520.601138521425,
      4525.892257489525,
      4537.351977820979,
      4547.665726119289,
      4548.346071002343,
      4548.958381397091,
      4551.519004664376,
      4553.823565604932,
      4554.756412407456,
      4529.369451525524,
      4542.285812530184,
      4553.910537434377,
      4569.2084871945035,
      4582.976641978617,
      4589.294295629651,
      4594.980183915581,
      4593.423339838425,
      4592.022180168984,
      4611.790071515869,
      4629.581173728066,
      4620.726342827345,
      4612.756995016698,
      4611.840043083531
    ]
  }
}