#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MAPPOè®­ç»ƒå¯åŠ¨è„šæœ¬
ä½œè€…: Dionysus
è”ç³»æ–¹å¼: wechat:gzw1546484791

å¯åŠ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒéªŒè¯æ™ºèƒ½ä½“ååŒå­¦ä¹ æ•ˆæœ
é›†æˆè®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡è°ƒåº¦ã€æ¢ç´¢ç­–ç•¥ã€å¥–åŠ±å¹³æ»‘å’Œæ”¶æ•›æ£€æµ‹
"""

import sys
import os

# Force CPU to avoid OOM
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import time
import json
import traceback
from typing import Dict, Any, List, Optional, Tuple
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
from sklearn.linear_model import LinearRegression


class CustomJSONEncoder(json.JSONEncoder):
    """
    è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†ä¸å¯åºåˆ—åŒ–çš„ç±»å‹
    """
    def default(self, obj):
        if isinstance(obj, bool):
            return bool(obj)  # ç¡®ä¿å¸ƒå°”å€¼æ­£ç¡®åºåˆ—åŒ–
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, torch.Tensor):
            return obj.detach().cpu().numpy().tolist()
        elif hasattr(obj, '__dict__'):
            return obj.__dict__
        return super().default(obj)


class AdaptiveLearningRateScheduler:
    """
    è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨
    æ ¹æ®è®­ç»ƒè¿›åº¦å’Œæ€§èƒ½åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
    """
    
    def __init__(self, initial_lr: float = 3e-4, min_lr: float = 1e-5, 
                 patience: int = 100, decay_factor: float = 0.8):
        """
        åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        
        Args:
            initial_lr: åˆå§‹å­¦ä¹ ç‡
            min_lr: æœ€å°å­¦ä¹ ç‡
            patience: æ€§èƒ½åœæ»å®¹å¿è½®æ•°
            decay_factor: è¡°å‡å› å­
        """
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.min_lr = min_lr
        self.patience = patience
        self.decay_factor = decay_factor
        self.best_reward = float('-inf')
        self.patience_counter = 0
        self.lr_history = []
        
    def update(self, current_reward: float, episode: int) -> float:
        """
        æ›´æ–°å­¦ä¹ ç‡
        
        Args:
            current_reward: å½“å‰å¥–åŠ±
            episode: å½“å‰è½®æ•°
            
        Returns:
            æ›´æ–°åçš„å­¦ä¹ ç‡
        """
        # è®°å½•å†å²
        self.lr_history.append(self.current_lr)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½æå‡
        if current_reward > self.best_reward:
            self.best_reward = current_reward
            self.patience_counter = 0
        else:
            self.patience_counter += 1
        
        # å¦‚æœæ€§èƒ½åœæ»ï¼Œé™ä½å­¦ä¹ ç‡
        if self.patience_counter >= self.patience and self.current_lr > self.min_lr:
            self.current_lr = max(self.current_lr * self.decay_factor, self.min_lr)
            self.patience_counter = 0
            
        return self.current_lr


class ExplorationScheduler:
    """
    æ¢ç´¢ç­–ç•¥è°ƒåº¦å™¨
    åŠ¨æ€è°ƒæ•´æ¢ç´¢å‚æ•°ä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
    """
    
    def __init__(self, initial_epsilon: float = 0.3, min_epsilon: float = 0.05,
                 decay_rate: float = 0.995, entropy_coef_initial: float = 0.01):
        """
        åˆå§‹åŒ–æ¢ç´¢è°ƒåº¦å™¨
        
        Args:
            initial_epsilon: åˆå§‹æ¢ç´¢ç‡
            min_epsilon: æœ€å°æ¢ç´¢ç‡
            decay_rate: è¡°å‡ç‡
            entropy_coef_initial: åˆå§‹ç†µç³»æ•°
        """
        self.initial_epsilon = initial_epsilon
        self.current_epsilon = initial_epsilon
        self.min_epsilon = min_epsilon
        self.decay_rate = decay_rate
        self.entropy_coef = entropy_coef_initial
        self.exploration_history = []
        
    def update(self, episode: int, performance_variance: float) -> Dict[str, float]:
        """
        æ›´æ–°æ¢ç´¢å‚æ•°
        
        Args:
            episode: å½“å‰è½®æ•°
            performance_variance: æ€§èƒ½æ–¹å·®
            
        Returns:
            æ›´æ–°åçš„æ¢ç´¢å‚æ•°
        """
        # åŸºäºè½®æ•°çš„è¡°å‡
        self.current_epsilon = max(
            self.current_epsilon * self.decay_rate,
            self.min_epsilon
        )
        
        # åŸºäºæ€§èƒ½æ–¹å·®è°ƒæ•´ç†µç³»æ•°
        if performance_variance > 0.2:  # é«˜æ–¹å·®æ—¶å¢åŠ æ¢ç´¢
            self.entropy_coef = min(self.entropy_coef * 1.05, 0.05)
        else:  # ä½æ–¹å·®æ—¶å‡å°‘æ¢ç´¢
            self.entropy_coef = max(self.entropy_coef * 0.98, 0.001)
        
        exploration_params = {
            'epsilon': self.current_epsilon,
            'entropy_coef': self.entropy_coef
        }
        
        self.exploration_history.append(exploration_params.copy())
        return exploration_params
    
    def get_current_params(self) -> Dict[str, float]:
        """
        è·å–å½“å‰æ¢ç´¢å‚æ•°
        
        Returns:
            å½“å‰æ¢ç´¢å‚æ•°å­—å…¸
        """
        return {
            'epsilon': self.current_epsilon,
            'entropy_coef': self.entropy_coef
        }


class RewardSmoother:
    """
    å¥–åŠ±å¹³æ»‘å™¨
    ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å’Œå¼‚å¸¸å€¼æ£€æµ‹æ¥å¹³æ»‘å¥–åŠ±ä¿¡å·
    """
    
    def __init__(self, alpha: float = 0.1, outlier_threshold: float = 2.0):
        """
        åˆå§‹åŒ–å¥–åŠ±å¹³æ»‘å™¨
        
        Args:
            alpha: æŒ‡æ•°ç§»åŠ¨å¹³å‡ç³»æ•°
            outlier_threshold: å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰
        """
        self.alpha = alpha
        self.outlier_threshold = outlier_threshold
        self.ema_reward = None
        self.reward_history = []
        self.smoothed_history = []
        
    def smooth(self, reward: float) -> float:
        """
        å¹³æ»‘å¥–åŠ±å€¼
        
        Args:
            reward: åŸå§‹å¥–åŠ±
            
        Returns:
            å¹³æ»‘åçš„å¥–åŠ±
        """
        self.reward_history.append(reward)
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        if len(self.reward_history) > 10:
            recent_rewards = self.reward_history[-10:]
            mean_reward = np.mean(recent_rewards)
            std_reward = np.std(recent_rewards)
            
            # å¦‚æœæ˜¯å¼‚å¸¸å€¼ï¼Œä½¿ç”¨å‡å€¼æ›¿ä»£
            if abs(reward - mean_reward) > self.outlier_threshold * std_reward:
                reward = mean_reward
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        if self.ema_reward is None:
            self.ema_reward = reward
        else:
            self.ema_reward = self.alpha * reward + (1 - self.alpha) * self.ema_reward
        
        self.smoothed_history.append(self.ema_reward)
        return self.ema_reward


class ConvergenceDetector:
    """
    æ”¶æ•›æ£€æµ‹å™¨
    æ£€æµ‹è®­ç»ƒæ˜¯å¦æ”¶æ•›æˆ–é™·å…¥å±€éƒ¨æœ€ä¼˜
    """
    
    def __init__(self, window_size: int = 50, stability_threshold: float = 0.05,
                 improvement_threshold: float = 0.01):
        """
        åˆå§‹åŒ–æ”¶æ•›æ£€æµ‹å™¨
        
        Args:
            window_size: æ£€æµ‹çª—å£å¤§å°ï¼ˆå‡å°‘åˆ°50ä»¥æ›´å¿«æ£€æµ‹æ”¶æ•›ï¼‰
            stability_threshold: ç¨³å®šæ€§é˜ˆå€¼
            improvement_threshold: æ”¹è¿›é˜ˆå€¼
        """
        self.window_size = window_size
        self.stability_threshold = stability_threshold
        self.improvement_threshold = improvement_threshold
        self.reward_history = []
        
    def check_convergence(self, reward: float) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ”¶æ•›çŠ¶æ€
        
        Args:
            reward: å½“å‰å¥–åŠ±
            
        Returns:
            æ”¶æ•›çŠ¶æ€ä¿¡æ¯
        """
        self.reward_history.append(reward)
        
        if len(self.reward_history) < self.window_size:
            return {
                'status': 'insufficient_data',
                'message': f'éœ€è¦è‡³å°‘{self.window_size}è½®æ•°æ®',
                'confidence': 0.0
            }
        
        # è·å–æœ€è¿‘çš„å¥–åŠ±çª—å£
        recent_rewards = self.reward_history[-self.window_size:]
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean_reward = np.mean(recent_rewards)
        std_reward = np.std(recent_rewards)
        cv = std_reward / abs(mean_reward) if mean_reward != 0 else float('inf')
        
        # è®¡ç®—è¶‹åŠ¿
        if len(self.reward_history) >= 2 * self.window_size:
            early_rewards = self.reward_history[-2*self.window_size:-self.window_size]
            early_mean = np.mean(early_rewards)
            improvement_rate = (mean_reward - early_mean) / abs(early_mean) if early_mean != 0 else 0
        else:
            improvement_rate = 0
        
        # åˆ¤æ–­æ”¶æ•›çŠ¶æ€
        if cv < self.stability_threshold:
            if improvement_rate > self.improvement_threshold:
                status = 'converging_with_improvement'
                message = 'è®­ç»ƒæ”¶æ•›ä¸”æ€§èƒ½æŒç»­æ”¹è¿›'
                confidence = 0.9
            else:
                status = 'converged'
                message = 'è®­ç»ƒå·²æ”¶æ•›'
                confidence = 0.8
        elif improvement_rate < -self.improvement_threshold:
            status = 'degrading'
            message = 'æ€§èƒ½ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°'
            confidence = 0.7
        elif abs(improvement_rate) < self.improvement_threshold and len(self.reward_history) > 3 * self.window_size:
            status = 'local_optimum'
            message = 'å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜'
            confidence = 0.6
        else:
            status = 'training'
            message = 'è®­ç»ƒè¿›è¡Œä¸­'
            confidence = 0.5
        
        return {
            'status': status,
            'message': message,
            'confidence': confidence,
            'statistics': {
                'mean_reward': mean_reward,
                'std_reward': std_reward,
                'coefficient_of_variation': cv,
                'improvement_rate': improvement_rate
            }
        }


# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from truck_routing import TruckSchedulingEnv, train_marl
import config


class TrainingManager:
    """
    è®­ç»ƒç®¡ç†å™¨
    ç®¡ç†MAPPOè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç›‘æ§ã€æ—¥å¿—è®°å½•ã€æ€§èƒ½è¯„ä¼°å’Œç¨³å®šæ€§ä¼˜åŒ–
    """
    
    def __init__(self, env: TruckSchedulingEnv, enable_optimization: bool = True):
        """
        åˆå§‹åŒ–è®­ç»ƒç®¡ç†å™¨
        
        Args:
            env: è®­ç»ƒç¯å¢ƒ
            enable_optimization: æ˜¯å¦å¯ç”¨è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–
        """
        self.env = env
        self.start_time = time.time()
        self.training_log = []
        self.performance_metrics = {}
        self.enable_optimization = enable_optimization
        
        # åˆå§‹åŒ–ç¨³å®šæ€§ä¼˜åŒ–ç»„ä»¶
        if self.enable_optimization:
            self.lr_scheduler = AdaptiveLearningRateScheduler(
                initial_lr=config.LEARNING_RATE,
                min_lr=config.LEARNING_RATE * 0.1,
                patience=100,
                decay_factor=0.8
            )
            self.exploration_scheduler = ExplorationScheduler(
                initial_epsilon=0.3,
                min_epsilon=0.05,
                decay_rate=0.995
            )
            self.reward_smoother = RewardSmoother(alpha=0.1, outlier_threshold=2.0)
            self.convergence_detector = ConvergenceDetector(
                window_size=100,
                stability_threshold=0.05,
                improvement_threshold=0.01
            )
            print("âœ… è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–å·²å¯ç”¨")
        
    def log_training_progress(self, episode: int, reward: float, metrics: Dict[str, Any]):
        """
        è®°å½•è®­ç»ƒè¿›åº¦å¹¶åº”ç”¨ç¨³å®šæ€§ä¼˜åŒ–
        
        Args:
            episode: è®­ç»ƒè½®æ•°
            reward: å¥–åŠ±å€¼
            metrics: æ€§èƒ½æŒ‡æ ‡
        """
        # åº”ç”¨å¥–åŠ±å¹³æ»‘
        if self.enable_optimization:
            smoothed_reward = self.reward_smoother.smooth(reward)
            
            # æ›´æ–°å­¦ä¹ ç‡
            new_lr = self.lr_scheduler.update(smoothed_reward, episode)
            
            # è®¡ç®—æ€§èƒ½æ–¹å·®ç”¨äºæ¢ç´¢è°ƒåº¦
            recent_rewards = [entry['reward'] for entry in self.training_log[-20:]]
            if len(recent_rewards) > 5:
                performance_variance = np.std(recent_rewards) / (np.mean(recent_rewards) + 1e-8)
            else:
                performance_variance = 0.2
            
            # æ›´æ–°æ¢ç´¢å‚æ•°
            exploration_params = self.exploration_scheduler.update(episode, performance_variance)
            
            # æ£€æŸ¥æ”¶æ•›çŠ¶æ€
            convergence_info = self.convergence_detector.check_convergence(smoothed_reward)
            
            # æ›´æ–°metrics
            metrics.update({
                'smoothed_reward': smoothed_reward,
                'learning_rate': new_lr,
                'exploration_epsilon': exploration_params['epsilon'],
                'entropy_coefficient': exploration_params['entropy_coef'],
                'convergence_status': convergence_info['status'],
                'convergence_confidence': convergence_info['confidence']
            })
        
        log_entry = {
            'episode': episode,
            'reward': reward,
            'timestamp': time.time() - self.start_time,
            'metrics': metrics
        }
        self.training_log.append(log_entry)
        
        # è¿›åº¦ä¿¡æ¯ç”±tqdmè¿›åº¦æ¡æ˜¾ç¤ºï¼Œæ— éœ€é¢å¤–printè¾“å‡º
    
    def get_current_training_params(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰è®­ç»ƒå‚æ•°ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ï¼‰
        
        Returns:
            å½“å‰è®­ç»ƒå‚æ•°
        """
        if not self.enable_optimization:
            return {}
        
        return {
            'learning_rate': self.lr_scheduler.current_lr,
            'epsilon': self.exploration_scheduler.current_epsilon,
            'entropy_coef': self.exploration_scheduler.entropy_coef
        }
    
    def evaluate_training_performance(self) -> Dict[str, Any]:
        """
        è¯„ä¼°è®­ç»ƒæ€§èƒ½
        
        Returns:
            æ€§èƒ½è¯„ä¼°ç»“æœ
        """
        if not self.training_log:
            return {"status": "no_data", "message": "æ²¡æœ‰è®­ç»ƒæ•°æ®"}
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        rewards = [entry['reward'] for entry in self.training_log]
        recent_rewards = rewards[-100:] if len(rewards) >= 100 else rewards
        
        performance = {
            'total_episodes': len(self.training_log),
            'training_time': time.time() - self.start_time,
            'average_reward': np.mean(rewards),
            'recent_average_reward': np.mean(recent_rewards),
            'best_reward': max(rewards),
            'reward_improvement': recent_rewards[-1] - rewards[0] if len(rewards) > 1 else 0,
            'convergence_status': self._check_convergence(rewards)
        }
        
        # æ·»åŠ ç¨³å®šæ€§ä¼˜åŒ–ç›¸å…³æŒ‡æ ‡
        if self.enable_optimization and len(self.training_log) > 0:
            latest_metrics = self.training_log[-1]['metrics']
            performance.update({
                'final_learning_rate': latest_metrics.get('learning_rate', config.LEARNING_RATE),
                'final_exploration_rate': latest_metrics.get('exploration_epsilon', 0.1),
                'reward_stability': np.std(recent_rewards) / (np.mean(recent_rewards) + 1e-8),
                'optimization_enabled': True
            })
        
        return performance
    
    def _check_convergence(self, rewards: List[float]) -> str:
        """
        æ£€æŸ¥è®­ç»ƒæ”¶æ•›çŠ¶æ€
        
        Args:
            rewards: å¥–åŠ±å†å²
            
        Returns:
            æ”¶æ•›çŠ¶æ€æè¿°
        """
        if self.enable_optimization and hasattr(self, 'convergence_detector'):
            if len(rewards) > 0:
                convergence_info = self.convergence_detector.check_convergence(rewards[-1])
                return convergence_info['message']
        
        # å›é€€åˆ°åŸå§‹æ£€æŸ¥é€»è¾‘
        if len(rewards) < 100:
            return "è®­ç»ƒæ•°æ®ä¸è¶³"
        
        # æ£€æŸ¥æœ€è¿‘100è½®çš„å¥–åŠ±ç¨³å®šæ€§
        recent_rewards = rewards[-100:]
        reward_std = np.std(recent_rewards)
        reward_mean = np.mean(recent_rewards)
        
        if reward_std / abs(reward_mean) < 0.1:  # å˜å¼‚ç³»æ•°å°äº10%
            return "å·²æ”¶æ•›"
        elif len(rewards) >= 500:
            # æ£€æŸ¥è¶‹åŠ¿
            early_avg = np.mean(rewards[:100])
            late_avg = np.mean(rewards[-100:])
            if late_avg > early_avg * 1.1:
                return "æŒç»­æ”¹è¿›ä¸­"
            else:
                return "å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜"
        else:
            return "è®­ç»ƒä¸­"
    
    def save_training_report(self, filename: str = "training_report.json"):
        """
        ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        
        Args:
            filename: æŠ¥å‘Šæ–‡ä»¶å
        """
        performance = self.evaluate_training_performance()
        
        report = {
            'training_summary': performance,
            'environment_config': {
                'num_trucks': self.env.num_trucks,
                'num_lockers': self.env.num_lockers,
                'truck_capacity': self.env.truck_capacity,
                'state_dimension': self.env.state_dim
            },
            'training_config': {
                'total_timesteps': config.TOTAL_TIMESTEPS,
                'learning_rate': config.LEARNING_RATE,
                'batch_size': config.BATCH_SIZE,
                'gamma': config.GAMMA,
                'optimization_enabled': self.enable_optimization
            },
            'training_log': self.training_log,  # ä¿å­˜å®Œæ•´çš„è®­ç»ƒæ—¥å¿—
            'recommendations': self._generate_recommendations(performance)
        }
        
        # æ·»åŠ ä¼˜åŒ–å†å²æ•°æ®
        if self.enable_optimization:
            report['optimization_history'] = {
                'learning_rate_history': getattr(self.lr_scheduler, 'lr_history', []),
                'exploration_history': getattr(self.exploration_scheduler, 'exploration_history', []),
                'smoothed_rewards': getattr(self.reward_smoother, 'smoothed_history', [])
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)
        
        print(f"ğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    
    def _generate_recommendations(self, performance: Dict[str, Any]) -> List[str]:
        """
        æ ¹æ®æ€§èƒ½ç”Ÿæˆå»ºè®®
        
        Args:
            performance: æ€§èƒ½è¯„ä¼°ç»“æœ
            
        Returns:
            å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        # å®‰å…¨è·å–æ”¶æ•›çŠ¶æ€
        convergence_status = performance.get('convergence_status', 'æœªçŸ¥')
        if convergence_status == "å·²æ”¶æ•›":
            recommendations.append("è®­ç»ƒå·²æ”¶æ•›ï¼Œå¯ä»¥è¿›è¡Œæ¨¡å‹éƒ¨ç½²æµ‹è¯•")
        elif convergence_status == "æŒç»­æ”¹è¿›ä¸­":
            recommendations.append("è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒä»¥è·å¾—æ›´å¥½æ€§èƒ½")
        elif convergence_status == "å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜":
            if self.enable_optimization:
                recommendations.append("å·²å¯ç”¨è‡ªé€‚åº”ä¼˜åŒ–ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡å’Œæ¢ç´¢ç­–ç•¥")
            else:
                recommendations.append("å»ºè®®å¯ç”¨è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–æˆ–æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡")
        else:
            recommendations.append("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œå»ºè®®å¢åŠ è®­ç»ƒè½®æ•°")
        
        # å®‰å…¨è·å–å¥–åŠ±æ•°æ®
        recent_avg = performance.get('recent_average_reward', 0)
        avg_reward = performance.get('average_reward', 0)
        
        if recent_avg > avg_reward * 1.1:
            recommendations.append("è¿‘æœŸè¡¨ç°ä¼˜ç§€ï¼Œè®­ç»ƒæ•ˆæœè‰¯å¥½")
        elif recent_avg < avg_reward * 0.9:
            recommendations.append("è¿‘æœŸè¡¨ç°ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒå‚æ•°")
        
        # ç¨³å®šæ€§ç›¸å…³å»ºè®®
        if self.enable_optimization:
            reward_stability = performance.get('reward_stability', 0)
            if reward_stability > 0.3:
                recommendations.append("å¥–åŠ±æ³¢åŠ¨è¾ƒå¤§ï¼Œå·²å¯ç”¨å¥–åŠ±å¹³æ»‘æœºåˆ¶")
            elif reward_stability < 0.1:
                recommendations.append("å¥–åŠ±ç¨³å®šæ€§è‰¯å¥½ï¼Œè®­ç»ƒæ”¶æ•›æ•ˆæœä½³")
        
        return recommendations

    def generate_training_plots(self, save_dir: str = "."):
        """
        ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–å›¾è¡¨
        
        Args:
            save_dir: å›¾è¡¨ä¿å­˜ç›®å½•
        """
        if not self.training_log:
            print("âš ï¸ æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            return
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)
        
        # æå–æ•°æ®
        episodes = [entry['episode'] for entry in self.training_log]
        rewards = [entry['reward'] for entry in self.training_log]
        timestamps = [entry['timestamp'] for entry in self.training_log]
        
        # è®¾ç½®è‹±æ–‡å­—ä½“ï¼Œé¿å…å­—ä½“æ˜¾ç¤ºé—®é¢˜
        plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå•ä¸ªå›¾è¡¨ï¼Œåªæ˜¾ç¤ºå¹³å‡å¥–åŠ±æ›²çº¿
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        fig.suptitle('Average Reward Convergence Curve', fontsize=16, fontweight='bold')
        
        # è£å‰ªæ‰æœ€å50ä¸ªepisodeçš„æ•°æ®
        if len(rewards) > 50:
            trimmed_rewards = rewards[:-50]
            trimmed_episodes = episodes[:-50]
        else:
            trimmed_rewards = rewards
            trimmed_episodes = episodes
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡å¥–åŠ±
        if len(trimmed_rewards) > 30:
            window_size = 30  # ä½¿ç”¨30ä¸ªepisodeçš„ç§»åŠ¨å¹³å‡
            moving_avg = np.convolve(trimmed_rewards, np.ones(window_size)/window_size, mode='valid')
            moving_episodes = trimmed_episodes[window_size-1:]
            
            # å‡å»æœ€ä½å€¼è¿›è¡Œå½’ä¸€åŒ–
            min_avg_reward = np.min(moving_avg)
            normalized_avg = moving_avg - min_avg_reward
            
            # ç»˜åˆ¶å½’ä¸€åŒ–åçš„å¹³å‡å¥–åŠ±æ›²çº¿
            ax.plot(moving_episodes, normalized_avg, 'b-', linewidth=3, label=f'Average Reward (30-episode window)')
            
            # è®¡ç®—å¹¶ç»˜åˆ¶è¶‹åŠ¿çº¿
            if len(moving_episodes) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
                # ä½¿ç”¨çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿çº¿
                X = np.array(moving_episodes).reshape(-1, 1)
                y = np.array(normalized_avg)
                
                lr_model = LinearRegression()
                lr_model.fit(X, y)
                
                # è®¡ç®—è¶‹åŠ¿çº¿çš„é¢„æµ‹å€¼
                trend_line = lr_model.predict(X)
                
                # ç»˜åˆ¶è¶‹åŠ¿çº¿
                ax.plot(moving_episodes, trend_line, 'r--', linewidth=2, alpha=0.8, 
                       label=f'Trend Line (slope: {lr_model.coef_[0]:.3f})')
                
                # è®¡ç®—è¶‹åŠ¿å¼ºåº¦ (RÂ²)
                from sklearn.metrics import r2_score
                r2 = r2_score(y, trend_line)
                
                # åœ¨å›¾ä¸Šæ˜¾ç¤ºè¶‹åŠ¿ä¿¡æ¯
                trend_direction = "Up" if lr_model.coef_[0] > 0 else "Down" if lr_model.coef_[0] < 0 else "Flat"
                ax.text(0.02, 0.85, f'Trend: {trend_direction}\nSlope: {lr_model.coef_[0]:.4f}\nR^2: {r2:.3f}', 
                       transform=ax.transAxes, fontsize=10, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            
            ax.set_xlabel('Training Episodes', fontsize=12)
            ax.set_ylabel('Normalized Average Reward (minus minimum)', fontsize=12)
            ax.set_title(f'Average Reward Curve with Trend (Min: {min_avg_reward:.2f}, Max: {np.max(moving_avg):.2f})', fontsize=14)
            ax.grid(True, alpha=0.3)
            ax.legend(fontsize=12)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            trimmed_info = f" (trimmed last 50)" if len(rewards) > 50 else ""
            ax.text(0.02, 0.98, f'Episodes: {len(trimmed_episodes)}{trimmed_info}\nMin Avg: {min_avg_reward:.2f}\nMax Avg: {np.max(moving_avg):.2f}\nRange: {np.max(moving_avg) - min_avg_reward:.2f}', 
                   transform=ax.transAxes, fontsize=10, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        else:
            ax.text(0.5, 0.5, 'Insufficient Data\n(Need >30 episodes for average)', ha='center', va='center', 
                   transform=ax.transAxes, fontsize=14)
            ax.set_title('Average Reward Curve')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(save_dir, 'training_analysis.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è®­ç»ƒåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
        
        # ç”ŸæˆæŸå¤±æ›²çº¿å›¾ï¼ˆå¦‚æœæœ‰æŸå¤±æ•°æ®ï¼‰
        self._generate_loss_plots(save_dir)
        
        # ç”Ÿæˆå¥–åŠ±åˆ†è§£æ›²çº¿å›¾
        self._generate_component_plots(save_dir)

        # ç”Ÿæˆå­¦ä¹ è¿‡ç¨‹ç›‘æ§å›¾ï¼ˆå­¦ä¹ ç‡ã€æ¢ç´¢ç‡ã€ç†µï¼‰
        self._generate_learning_process_plots(save_dir)

    def _generate_learning_process_plots(self, save_dir: str):
        """
        ç”Ÿæˆå­¦ä¹ è¿‡ç¨‹ç›‘æ§å›¾ï¼ˆå­¦ä¹ ç‡ã€æ¢ç´¢ç‡ã€ç†µï¼‰
        """
        # æå–æ•°æ®
        episodes = []
        learning_rates = []
        exploration_epsilons = []
        entropies = []
        
        for entry in self.training_log:
            if 'metrics' in entry and entry['metrics']:
                metrics = entry['metrics']
                # æ”¶é›†å­˜åœ¨çš„æŒ‡æ ‡
                if 'current_learning_rate' in metrics:
                    episodes.append(entry['episode'])
                    learning_rates.append(metrics['current_learning_rate'])
                    exploration_epsilons.append(metrics.get('exploration_epsilon', 0))
                    entropies.append(metrics.get('exploration_entropy', 0))
        
        if not episodes:
            return

        # åˆ›å»ºå›¾è¡¨ - 3ä¸ªå­å›¾
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)
        fig.suptitle('Learning Process Analysis', fontsize=16, fontweight='bold')
        
        # 1. Learning Rate
        ax1.plot(episodes, learning_rates, 'b-', linewidth=2)
        ax1.set_ylabel('Learning Rate')
        ax1.set_title('Learning Rate Schedule')
        ax1.grid(True, alpha=0.3)
        
        # 2. Exploration Rate (Epsilon)
        ax2.plot(episodes, exploration_epsilons, 'g-', linewidth=2)
        ax2.set_ylabel('Exploration Rate (Epsilon)')
        ax2.set_title('Exploration Decay')
        ax2.grid(True, alpha=0.3)
        
        # 3. Policy Entropy
        ax3.plot(episodes, entropies, 'r-', linewidth=2)
        ax3.set_ylabel('Policy Entropy')
        ax3.set_title('Policy Entropy (Randomness)')
        ax3.set_xlabel('Episode')
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = os.path.join(save_dir, 'learning_process.png')
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"ğŸ“Š å­¦ä¹ è¿‡ç¨‹ç›‘æ§å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")

    
    def _generate_component_plots(self, save_dir: str):
        """
        ç”Ÿæˆå¥–åŠ±åˆ†è§£æ›²çº¿å›¾
        """
        # æå–æ•°æ®
        episodes = []
        service_rewards = []
        efficiency_rewards = []
        cost_penalties = []
        
        for entry in self.training_log:
            if 'metrics' in entry and entry['metrics']:
                metrics = entry['metrics']
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†è§£å¥–åŠ±æ•°æ®
                if 'reward_service' in metrics:
                    episodes.append(entry['episode'])
                    service_rewards.append(metrics['reward_service'])
                    efficiency_rewards.append(metrics.get('reward_efficiency', 0))
                    cost_penalties.append(metrics.get('reward_cost', 0))
        
        if not episodes:
            return

        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)
        fig.suptitle('Reward Components Analysis', fontsize=16, fontweight='bold')
        
        window_size = min(50, len(episodes) // 10) if len(episodes) > 50 else 1
        
        # 1. Service Reward
        ax1.plot(episodes, service_rewards, 'g-', alpha=0.3, label='Raw')
        if len(episodes) > window_size:
            ma = np.convolve(service_rewards, np.ones(window_size)/window_size, mode='valid')
            ax1.plot(episodes[window_size-1:], ma, 'g-', linewidth=2, label=f'MA({window_size})')
        ax1.set_ylabel('Service Reward')
        ax1.legend(loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # 2. Efficiency Reward
        ax2.plot(episodes, efficiency_rewards, 'b-', alpha=0.3, label='Raw')
        if len(episodes) > window_size:
            ma = np.convolve(efficiency_rewards, np.ones(window_size)/window_size, mode='valid')
            ax2.plot(episodes[window_size-1:], ma, 'b-', linewidth=2, label=f'MA({window_size})')
        ax2.set_ylabel('Efficiency Reward')
        ax2.legend(loc='upper left')
        ax2.grid(True, alpha=0.3)
        
        # 3. Cost Penalty
        ax3.plot(episodes, cost_penalties, 'r-', alpha=0.3, label='Raw')
        if len(episodes) > window_size:
            ma = np.convolve(cost_penalties, np.ones(window_size)/window_size, mode='valid')
            ax3.plot(episodes[window_size-1:], ma, 'r-', linewidth=2, label=f'MA({window_size})')
        ax3.set_ylabel('Cost Penalty')
        ax3.set_xlabel('Episode')
        ax3.legend(loc='upper left')
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = os.path.join(save_dir, 'reward_components.png')
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"ğŸ“Š å¥–åŠ±åˆ†è§£å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")

    def _generate_loss_plots(self, save_dir: str):
        """
        ç”ŸæˆæŸå¤±å‡½æ•°æ›²çº¿å›¾
        
        Args:
            save_dir: å›¾è¡¨ä¿å­˜ç›®å½•
        """
        # æå–æŸå¤±æ•°æ®
        policy_losses = []
        value_losses = []
        episodes_with_loss = []
        
        for entry in self.training_log:
            if 'metrics' in entry and entry['metrics']:
                metrics = entry['metrics']
                if 'policy_loss' in metrics and 'value_loss' in metrics:
                    policy_losses.append(metrics['policy_loss'])
                    value_losses.append(metrics['value_loss'])
                    episodes_with_loss.append(entry['episode'])
        
        if not policy_losses:
            print("âš ï¸ æ²¡æœ‰æŸå¤±æ•°æ®ï¼Œè·³è¿‡æŸå¤±æ›²çº¿å›¾ç”Ÿæˆ")
            return
        
        # åˆ›å»ºæŸå¤±æ›²çº¿å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        fig.suptitle('MAPPO Loss Function Analysis', fontsize=14, fontweight='bold')
        
        # ç­–ç•¥æŸå¤±
        ax1.plot(episodes_with_loss, policy_losses, 'b-', linewidth=1, alpha=0.7)
        if len(policy_losses) > 10:
            window_size = min(20, len(policy_losses) // 5)
            moving_avg = np.convolve(policy_losses, np.ones(window_size)/window_size, mode='valid')
            moving_episodes = episodes_with_loss[window_size-1:]
            ax1.plot(moving_episodes, moving_avg, 'r-', linewidth=2, label=f'Moving Average({window_size})')
            ax1.legend()
        
        ax1.set_xlabel('Training Episodes')
        ax1.set_ylabel('Policy Loss')
        ax1.set_title('Policy Loss Curve')
        ax1.grid(True, alpha=0.3)
        
        # ä»·å€¼æŸå¤±
        ax2.plot(episodes_with_loss, value_losses, 'g-', linewidth=1, alpha=0.7)
        if len(value_losses) > 10:
            window_size = min(20, len(value_losses) // 5)
            moving_avg = np.convolve(value_losses, np.ones(window_size)/window_size, mode='valid')
            moving_episodes = episodes_with_loss[window_size-1:]
            ax2.plot(moving_episodes, moving_avg, 'r-', linewidth=2, label=f'Moving Average({window_size})')
            ax2.legend()
        
        ax2.set_xlabel('Training Episodes')
        ax2.set_ylabel('Value Loss')
        ax2.set_title('Value Loss Curve')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜æŸå¤±å›¾è¡¨
        loss_plot_path = os.path.join(save_dir, 'loss_analysis.png')
        plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ æŸå¤±åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {loss_plot_path}")


def run_training_session(num_episodes: int = 1000, enable_optimization: bool = True) -> Dict[str, Any]:
    """
    è¿è¡Œè®­ç»ƒä¼šè¯
    
    Args:
        num_episodes: è®­ç»ƒè½®æ•°
        enable_optimization: æ˜¯å¦å¯ç”¨è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–
        
    Returns:
        è®­ç»ƒç»“æœ
    """
    print("ğŸš€ å¼€å§‹MAPPOå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        print("ğŸ”§ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...")
        env = TruckSchedulingEnv(verbose=True)
        
        # è®¾ç½®ç¯å¢ƒé…ç½®ï¼ˆä½¿ç”¨configä¸­çš„å€¼ï¼‰
        env_config = {
            'num_lockers': config.num_lockers,  # å¿«é€’æŸœæ•°é‡ï¼ˆä»configè¯»å–ï¼‰
            'num_trucks': None,  # å¡è½¦æ•°é‡ï¼ˆåŠ¨æ€è®¡ç®—ï¼‰
            'boundary': config.boundary,    # è¾¹ç•ŒèŒƒå›´ï¼ˆä»configè¯»å–ï¼‰
            'demand_variance': config.demand_variance,  # éœ€æ±‚æ–¹å·®ï¼ˆä»configè¯»å–ï¼‰
            'time_pressure': config.time_pressure     # æ—¶é—´å‹åŠ›ï¼ˆä»configè¯»å–ï¼‰
        }
        
        # åº”ç”¨ç¯å¢ƒé…ç½®
        if hasattr(env, 'update_curriculum_config'):
            env.update_curriculum_config(env_config)
        
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨ï¼ˆå¯ç”¨ä¼˜åŒ–åŠŸèƒ½ï¼‰
        training_manager = TrainingManager(env, enable_optimization=enable_optimization)
        
        print(f"ğŸ“Š ç¯å¢ƒé…ç½®:")
        print(f"   - å¡è½¦æ•°é‡: {env.num_trucks}")
        print(f"   - å¿«é€’æŸœæ•°é‡: {env.num_lockers}")
        print(f"   - è¾¹ç•ŒèŒƒå›´: Â±{env_config['boundary']}")
        print(f"   - å¡è½¦å®¹é‡: {env.truck_capacity}")
        print(f"   - çŠ¶æ€ç»´åº¦: {env.state_dim}")
        print(f"   - æ— äººæœºèˆªç¨‹: {config.DRONE_MAX_RANGE} (å•ç¨‹{config.DRONE_MAX_RANGE//2})")
        
        print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
        print(f"   - è®­ç»ƒè½®æ•°: {num_episodes}")
        print(f"   - åˆå§‹å­¦ä¹ ç‡: {config.LEARNING_RATE}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE}")
        print(f"   - æŠ˜æ‰£å› å­: {config.GAMMA}")
        print(f"   - ç¨³å®šæ€§ä¼˜åŒ–: {'âœ… å·²å¯ç”¨' if enable_optimization else 'âŒ æœªå¯ç”¨'}")
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸƒ å¼€å§‹è®­ç»ƒ...")
        trained_policy = train_marl(env, num_episodes=num_episodes, training_manager=training_manager, curriculum_manager=None)
        
        # ä¿å­˜æ¨¡å‹
        model_path = "trained_mappo_policy.pth"
        if hasattr(trained_policy, 'policy_net') and hasattr(trained_policy, 'value_net'):
            # ä¿å­˜ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œçš„çŠ¶æ€å­—å…¸
            model_state = {
                'policy_net_state_dict': trained_policy.policy_net.state_dict(),
                'value_net_state_dict': trained_policy.value_net.state_dict(),
                'num_trucks': trained_policy.num_trucks,
                'policy_optimizer_state_dict': trained_policy.policy_optimizer.state_dict(),
                'value_optimizer_state_dict': trained_policy.value_optimizer.state_dict(),
                'best_performance': trained_policy.best_performance,
                'training_metadata': {
                    'clip_ratio': trained_policy.clip_ratio,
                    'entropy_coef': trained_policy.entropy_coef,
                    'value_loss_coef': trained_policy.value_loss_coef,
                    'max_grad_norm': trained_policy.max_grad_norm
                }
            }
            torch.save(model_state, model_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        else:
            print("âš ï¸ æ— æ³•ä¿å­˜æ¨¡å‹ï¼šMAPPOå¯¹è±¡ç¼ºå°‘å¿…è¦å±æ€§")
            print(f"   å¯ç”¨å±æ€§: {[attr for attr in dir(trained_policy) if not attr.startswith('_')]}")
        
        # è¯„ä¼°æ€§èƒ½
        performance = training_manager.evaluate_training_performance()
        
        # ç”Ÿæˆè®­ç»ƒå›¾è¡¨
        print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒåˆ†æå›¾è¡¨...")
        training_manager.generate_training_plots()
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        training_manager.save_training_report()
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ è®­ç»ƒæ€§èƒ½æ€»ç»“:")
        print(f"   - æ€»è½®æ•°: {performance.get('total_episodes', 0)}")
        print(f"   - è®­ç»ƒæ—¶é—´: {performance.get('training_time', 0):.1f}ç§’")
        print(f"   - å¹³å‡å¥–åŠ±: {performance.get('average_reward', 0):.2f}")
        print(f"   - æœ€ä½³å¥–åŠ±: {performance.get('best_reward', 0):.2f}")
        print(f"   - æ”¶æ•›çŠ¶æ€: {performance.get('convergence_status', 'æœªçŸ¥')}")
        
        # æ˜¾ç¤ºä¼˜åŒ–ç›¸å…³ä¿¡æ¯
        if enable_optimization and performance.get('optimization_enabled', False):
            print(f"\nğŸ”§ è®­ç»ƒä¼˜åŒ–æ€»ç»“:")
            print(f"   - æœ€ç»ˆå­¦ä¹ ç‡: {performance.get('final_learning_rate', config.LEARNING_RATE):.2e}")
            print(f"   - æœ€ç»ˆæ¢ç´¢ç‡: {performance.get('final_exploration_rate', 0.1):.3f}")
            print(f"   - å¥–åŠ±ç¨³å®šæ€§: {performance.get('reward_stability', 0):.3f}")
            
            # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæœ
            if performance.get('reward_stability', 1) < 0.2:
                print(f"   - ä¼˜åŒ–æ•ˆæœ: âœ… å¥–åŠ±ç¨³å®šæ€§è‰¯å¥½")
            elif performance.get('reward_stability', 1) < 0.4:
                print(f"   - ä¼˜åŒ–æ•ˆæœ: âš ï¸ å¥–åŠ±æ³¢åŠ¨é€‚ä¸­")
            else:
                print(f"   - ä¼˜åŒ–æ•ˆæœ: âŒ å¥–åŠ±æ³¢åŠ¨è¾ƒå¤§")
        
        return {
            'status': 'success',
            'performance': performance,
            'model_path': model_path,
            'trained_policy': trained_policy,
            'optimization_enabled': enable_optimization
        }
        
    except Exception as e:
        error_msg = f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return {
            'status': 'error',
            'error_message': error_msg,
            'traceback': traceback.format_exc()
        }


def quick_training_test() -> bool:
    """
    å¿«é€Ÿè®­ç»ƒæµ‹è¯•
    è¿è¡Œå°‘é‡è½®æ•°éªŒè¯è®­ç»ƒæµç¨‹æ˜¯å¦æ­£å¸¸
    
    Returns:
        æµ‹è¯•æ˜¯å¦æˆåŠŸ
    """
    print("ğŸ§ª å¼€å§‹å¿«é€Ÿè®­ç»ƒæµ‹è¯•...")
    
    try:
        # è¿è¡Œ1500è½®è®­ç»ƒæµ‹è¯•ï¼Œè¶³å¤ŸéªŒè¯æ‰€æœ‰åŠŸèƒ½
        result = run_training_session(num_episodes=1500)  # å¿«é€Ÿæµ‹è¯•æ‰€æœ‰åŠŸèƒ½
        
        if result['status'] == 'success':
            print("âœ… å¿«é€Ÿè®­ç»ƒæµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âŒ å¿«é€Ÿè®­ç»ƒæµ‹è¯•å¤±è´¥: {result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
            return False
            
    except Exception as e:
        print(f"âŒ å¿«é€Ÿè®­ç»ƒæµ‹è¯•å¼‚å¸¸: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ¯ å¼€å§‹æ™ºèƒ½å¡è½¦è·¯å¾„è§„åˆ’è®­ç»ƒ")
        print("=" * 50)
        
        # å¿«é€Ÿæµ‹è¯•
        print("ğŸ§ª æ‰§è¡Œå¿«é€ŸåŠŸèƒ½æµ‹è¯•...")
        if not quick_training_test():
            print("âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
            return False
        
        print("âœ… å¿«é€Ÿæµ‹è¯•é€šè¿‡")
        print("\n" + "=" * 50)
        
        # æ­£å¼è®­ç»ƒ - ä½¿ç”¨åŸå§‹ç¯å¢ƒé…ç½®
        print("ğŸš€ å¼€å§‹åŸå§‹ç¯å¢ƒé…ç½®è®­ç»ƒ...")
        results = run_training_session(num_episodes=1000)
        
        if results['status'] == 'success':
            print("ğŸ‰ åŸå§‹ç¯å¢ƒé…ç½®è®­ç»ƒæˆåŠŸå®Œæˆ!")
            performance = results.get('performance', {})
            print(f"ğŸ“ˆ æœ€ç»ˆæ€§èƒ½: {performance.get('recent_average_reward', 0):.4f}")
            print(f"ğŸ“Š æ”¶æ•›çŠ¶æ€: {performance.get('convergence_status', 'æœªçŸ¥')}")
            return True
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        traceback.print_exc()
        return False


def generate_plots_from_report(report_path: str, save_dir: str = "."):
    with open(report_path, "r", encoding="utf-8") as f:
        report = json.load(f)
    training_log = report.get("training_log", [])
    if not training_log:
        print("âš ï¸ æŠ¥å‘Šä¸­æ²¡æœ‰è®­ç»ƒæ—¥å¿—ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
        return
    env = TruckSchedulingEnv(verbose=False)
    optimization_enabled = True
    cfg = report.get("training_config", {})
    if isinstance(cfg, dict) and "optimization_enabled" in cfg:
        optimization_enabled = cfg.get("optimization_enabled", True)
    tm = TrainingManager(env, enable_optimization=optimization_enabled)
    tm.training_log = training_log
    tm.generate_training_plots(save_dir)

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--plots-from-report":
        rp = sys.argv[2] if len(sys.argv) > 2 else "training_report.json"
        sd = sys.argv[3] if len(sys.argv) > 3 else "."
        generate_plots_from_report(rp, sd)
    else:
        result = run_training_session(num_episodes=15000, enable_optimization=True)

        if result['status'] == 'success':
            performance = result['performance']
            print("\nâœ… è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆå¥–åŠ±: {performance.get('best_reward', 0):.2f}")
            print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {performance.get('training_time', 0):.2f}ç§’")
            print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {result['model_path']}")
            
            # æ˜¾ç¤ºä¼˜åŒ–çŠ¶æ€
            if result.get('optimization_enabled', False):
                print(f"ğŸ”§ è®­ç»ƒä¼˜åŒ–: âœ… å·²å¯ç”¨")
            
            sys.exit(0)
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥ï¼")
            print(f"é”™è¯¯ä¿¡æ¯: {result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
            sys.exit(1)
